{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYXxzqGtjO4o",
        "outputId": "a42601f8-afe6-44ef-fd63-b3eedcdea73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ],
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "# from models import *\n",
        "# from utils import progress_bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s0H94RJKjX0D"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P98wVKH1jkFQ",
        "outputId": "1da15d88-6740-43f7-c4ae-24250bd4d931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.3, hue=0.3),\n",
        "    #transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "    #transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "    #transforms.GaussianBlur(3, sigma=(0.1, 2.0)),  # augmentation\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "a575cbbaf1834c81af53830b0f9d4428",
            "20c88bf5087e4b1a84f8cab8500fd690",
            "6cdae7bf52ff4451ab0deee20f688c5a",
            "f2370242142d4340a08a94f0047f4723",
            "3f8a5996c5854f668fbafc516fa07a12",
            "1f77e07c63624bee9c8e2c11757d553d",
            "f7f1385595ce40409097a8f2d31b53c0",
            "c2627013bbea41e0b9cb74e9e7137788",
            "97ae87a2a80f4857835d85c228272199",
            "1989749d38d741d6abb60bc42adaa5ff",
            "d1a3361f94f6432298424783d7efee19"
          ]
        },
        "id": "ET3HGuTnjX5-",
        "outputId": "ddfa745b-c27b-4fad-a22b-fab89e47e603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a575cbbaf1834c81af53830b0f9d4428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2) #128\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2) #100\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BCLozv6sxiG",
        "outputId": "14f4a0d7-525c-48c8-ea0c-847084db3c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391\n"
          ]
        }
      ],
      "source": [
        "print(len(trainloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6PkueVcjshd",
        "outputId": "215a2da7-dc8e-4932-f147-9c0183573087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vZScwCXMjskX"
      },
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2zFlIhMQjsox"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        #self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(1024*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        #out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkevL8y4kKVI",
        "outputId": "64d491c3-d29c-4faa-bede-bb157562df75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "ResNet                                   [128, 10]                 --\n",
            "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,728\n",
            "├─BatchNorm2d: 1-2                       [128, 64, 32, 32]         128\n",
            "├─Sequential: 1-3                        [128, 64, 32, 32]         --\n",
            "│    └─BasicBlock: 2-1                   [128, 64, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-1                  [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-2             [128, 64, 32, 32]         128\n",
            "│    │    └─Conv2d: 3-3                  [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-4             [128, 64, 32, 32]         128\n",
            "│    │    └─Sequential: 3-5              [128, 64, 32, 32]         --\n",
            "│    └─BasicBlock: 2-2                   [128, 64, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-6                  [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-7             [128, 64, 32, 32]         128\n",
            "│    │    └─Conv2d: 3-8                  [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-9             [128, 64, 32, 32]         128\n",
            "│    │    └─Sequential: 3-10             [128, 64, 32, 32]         --\n",
            "│    └─BasicBlock: 2-3                   [128, 64, 32, 32]         --\n",
            "│    │    └─Conv2d: 3-11                 [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-12            [128, 64, 32, 32]         128\n",
            "│    │    └─Conv2d: 3-13                 [128, 64, 32, 32]         36,864\n",
            "│    │    └─BatchNorm2d: 3-14            [128, 64, 32, 32]         128\n",
            "│    │    └─Sequential: 3-15             [128, 64, 32, 32]         --\n",
            "├─Sequential: 1-4                        [128, 128, 16, 16]        --\n",
            "│    └─BasicBlock: 2-4                   [128, 128, 16, 16]        --\n",
            "│    │    └─Conv2d: 3-16                 [128, 128, 16, 16]        73,728\n",
            "│    │    └─BatchNorm2d: 3-17            [128, 128, 16, 16]        256\n",
            "│    │    └─Conv2d: 3-18                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-19            [128, 128, 16, 16]        256\n",
            "│    │    └─Sequential: 3-20             [128, 128, 16, 16]        8,448\n",
            "│    └─BasicBlock: 2-5                   [128, 128, 16, 16]        --\n",
            "│    │    └─Conv2d: 3-21                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-22            [128, 128, 16, 16]        256\n",
            "│    │    └─Conv2d: 3-23                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-24            [128, 128, 16, 16]        256\n",
            "│    │    └─Sequential: 3-25             [128, 128, 16, 16]        --\n",
            "│    └─BasicBlock: 2-6                   [128, 128, 16, 16]        --\n",
            "│    │    └─Conv2d: 3-26                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-27            [128, 128, 16, 16]        256\n",
            "│    │    └─Conv2d: 3-28                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-29            [128, 128, 16, 16]        256\n",
            "│    │    └─Sequential: 3-30             [128, 128, 16, 16]        --\n",
            "│    └─BasicBlock: 2-7                   [128, 128, 16, 16]        --\n",
            "│    │    └─Conv2d: 3-31                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-32            [128, 128, 16, 16]        256\n",
            "│    │    └─Conv2d: 3-33                 [128, 128, 16, 16]        147,456\n",
            "│    │    └─BatchNorm2d: 3-34            [128, 128, 16, 16]        256\n",
            "│    │    └─Sequential: 3-35             [128, 128, 16, 16]        --\n",
            "├─Sequential: 1-5                        [128, 256, 8, 8]          --\n",
            "│    └─BasicBlock: 2-8                   [128, 256, 8, 8]          --\n",
            "│    │    └─Conv2d: 3-36                 [128, 256, 8, 8]          294,912\n",
            "│    │    └─BatchNorm2d: 3-37            [128, 256, 8, 8]          512\n",
            "│    │    └─Conv2d: 3-38                 [128, 256, 8, 8]          589,824\n",
            "│    │    └─BatchNorm2d: 3-39            [128, 256, 8, 8]          512\n",
            "│    │    └─Sequential: 3-40             [128, 256, 8, 8]          33,280\n",
            "│    └─BasicBlock: 2-9                   [128, 256, 8, 8]          --\n",
            "│    │    └─Conv2d: 3-41                 [128, 256, 8, 8]          589,824\n",
            "│    │    └─BatchNorm2d: 3-42            [128, 256, 8, 8]          512\n",
            "│    │    └─Conv2d: 3-43                 [128, 256, 8, 8]          589,824\n",
            "│    │    └─BatchNorm2d: 3-44            [128, 256, 8, 8]          512\n",
            "│    │    └─Sequential: 3-45             [128, 256, 8, 8]          --\n",
            "│    └─BasicBlock: 2-10                  [128, 256, 8, 8]          --\n",
            "│    │    └─Conv2d: 3-46                 [128, 256, 8, 8]          589,824\n",
            "│    │    └─BatchNorm2d: 3-47            [128, 256, 8, 8]          512\n",
            "│    │    └─Conv2d: 3-48                 [128, 256, 8, 8]          589,824\n",
            "│    │    └─BatchNorm2d: 3-49            [128, 256, 8, 8]          512\n",
            "│    │    └─Sequential: 3-50             [128, 256, 8, 8]          --\n",
            "├─Linear: 1-6                            [128, 10]                 10,250\n",
            "==========================================================================================\n",
            "Total params: 4,630,858\n",
            "Trainable params: 4,630,858\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 92.57\n",
            "==========================================================================================\n",
            "Input size (MB): 1.57\n",
            "Forward/backward pass size (MB): 1778.40\n",
            "Params size (MB): 18.52\n",
            "Estimated Total Size (MB): 1798.49\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "net = ResNet(BasicBlock, [3, 4, 3, 3])\n",
        "\n",
        "print(summary(net,input_size=(128,3,32,32)))#input_size = (batch_size, #channel,imgsize)\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wGV6CjKekg_y"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001,weight_decay=6e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "64Oo01YMkhBJ"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Train Epoch: %d | Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (epoch, train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bFiXz9dZkhCt"
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print('Test Epoch: %d | Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                  % (epoch, test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "    return test_loss/(batch_idx+1), acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzse6BCAlB10",
        "outputId": "40c03385-6ddc-4da0-b6ba-f4d086519b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.588% (3114/3400)\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.543% (3204/3500)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.583% (3297/3600)\n",
            "Test Epoch: 139 | Loss: 0.312 | Acc: 91.459% (3384/3700)\n",
            "Test Epoch: 139 | Loss: 0.315 | Acc: 91.342% (3471/3800)\n",
            "Test Epoch: 139 | Loss: 0.311 | Acc: 91.462% (3567/3900)\n",
            "Test Epoch: 139 | Loss: 0.310 | Acc: 91.475% (3659/4000)\n",
            "Test Epoch: 139 | Loss: 0.311 | Acc: 91.415% (3748/4100)\n",
            "Test Epoch: 139 | Loss: 0.310 | Acc: 91.429% (3840/4200)\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.558% (3937/4300)\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.614% (4031/4400)\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.689% (4126/4500)\n",
            "Test Epoch: 139 | Loss: 0.303 | Acc: 91.630% (4215/4600)\n",
            "Test Epoch: 139 | Loss: 0.300 | Acc: 91.723% (4311/4700)\n",
            "Test Epoch: 139 | Loss: 0.305 | Acc: 91.604% (4397/4800)\n",
            "Test Epoch: 139 | Loss: 0.301 | Acc: 91.694% (4493/4900)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.580% (4579/5000)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.627% (4673/5100)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.558% (4761/5200)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.547% (4852/5300)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.537% (4943/5400)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.527% (5034/5500)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.518% (5125/5600)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.474% (5214/5700)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.466% (5305/5800)\n",
            "Test Epoch: 139 | Loss: 0.311 | Acc: 91.424% (5394/5900)\n",
            "Test Epoch: 139 | Loss: 0.311 | Acc: 91.433% (5486/6000)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.459% (5579/6100)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.468% (5671/6200)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.508% (5765/6300)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.531% (5858/6400)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.523% (5949/6500)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.530% (6041/6600)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.582% (6136/6700)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.471% (6220/6800)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.464% (6311/6900)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.457% (6402/7000)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.493% (6496/7100)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.514% (6589/7200)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.548% (6683/7300)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.541% (6774/7400)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.520% (6864/7500)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.500% (6954/7600)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.506% (7046/7700)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.538% (7140/7800)\n",
            "Test Epoch: 139 | Loss: 0.310 | Acc: 91.532% (7231/7900)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.600% (7328/8000)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.654% (7424/8100)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.610% (7512/8200)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.627% (7605/8300)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.631% (7697/8400)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.635% (7789/8500)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.616% (7879/8600)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.655% (7974/8700)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.648% (8065/8800)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.629% (8155/8900)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.622% (8246/9000)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.582% (8334/9100)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.598% (8427/9200)\n",
            "Test Epoch: 139 | Loss: 0.309 | Acc: 91.591% (8518/9300)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.617% (8612/9400)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.632% (8705/9500)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.615% (8795/9600)\n",
            "Test Epoch: 139 | Loss: 0.306 | Acc: 91.660% (8891/9700)\n",
            "Test Epoch: 139 | Loss: 0.304 | Acc: 91.694% (8986/9800)\n",
            "Test Epoch: 139 | Loss: 0.307 | Acc: 91.697% (9078/9900)\n",
            "Test Epoch: 139 | Loss: 0.308 | Acc: 91.690% (9169/10000)\n",
            "\n",
            "Epoch: 140\n",
            "Train Epoch: 140 | Loss: 0.107 | Acc: 96.094% (123/128)\n",
            "Train Epoch: 140 | Loss: 0.090 | Acc: 96.484% (247/256)\n",
            "Train Epoch: 140 | Loss: 0.092 | Acc: 96.094% (369/384)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.289% (493/512)\n",
            "Train Epoch: 140 | Loss: 0.082 | Acc: 96.562% (618/640)\n",
            "Train Epoch: 140 | Loss: 0.083 | Acc: 96.745% (743/768)\n",
            "Train Epoch: 140 | Loss: 0.081 | Acc: 96.875% (868/896)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 96.680% (990/1024)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 96.615% (1113/1152)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 96.719% (1238/1280)\n",
            "Train Epoch: 140 | Loss: 0.092 | Acc: 96.591% (1360/1408)\n",
            "Train Epoch: 140 | Loss: 0.093 | Acc: 96.615% (1484/1536)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.815% (1611/1664)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.763% (1734/1792)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 96.823% (1859/1920)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.729% (1981/2048)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.645% (2103/2176)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.701% (2228/2304)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.793% (2354/2432)\n",
            "Train Epoch: 140 | Loss: 0.090 | Acc: 96.680% (2475/2560)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.726% (2600/2688)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.768% (2725/2816)\n",
            "Train Epoch: 140 | Loss: 0.089 | Acc: 96.739% (2848/2944)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.842% (2975/3072)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 96.844% (3099/3200)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.815% (3222/3328)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.817% (3346/3456)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.763% (3468/3584)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.821% (3594/3712)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 96.901% (3721/3840)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 96.925% (3846/3968)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 96.924% (3970/4096)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 96.993% (4097/4224)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.013% (4222/4352)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 96.987% (4345/4480)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 96.984% (4469/4608)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.044% (4596/4736)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.081% (4722/4864)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.075% (4846/4992)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.051% (4969/5120)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.104% (5096/5248)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.117% (5221/5376)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.111% (5345/5504)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.088% (5468/5632)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.101% (5593/5760)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.113% (5718/5888)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.091% (5841/6016)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.087% (5965/6144)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.114% (6091/6272)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.141% (6217/6400)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.181% (6344/6528)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.145% (6466/6656)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.155% (6591/6784)\n",
            "Train Epoch: 140 | Loss: 0.083 | Acc: 97.179% (6717/6912)\n",
            "Train Epoch: 140 | Loss: 0.083 | Acc: 97.202% (6843/7040)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.154% (6964/7168)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.163% (7089/7296)\n",
            "Train Epoch: 140 | Loss: 0.083 | Acc: 97.198% (7216/7424)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.180% (7339/7552)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.201% (7465/7680)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.157% (7586/7808)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.140% (7709/7936)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.148% (7834/8064)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.168% (7960/8192)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.188% (8086/8320)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.206% (8212/8448)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.201% (8336/8576)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.185% (8459/8704)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.147% (8580/8832)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.143% (8704/8960)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.139% (8828/9088)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.135% (8952/9216)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.132% (9076/9344)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.128% (9200/9472)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.104% (9322/9600)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.081% (9444/9728)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.088% (9569/9856)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.085% (9693/9984)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.083% (9817/10112)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.080% (9941/10240)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.087% (10066/10368)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.094% (10191/10496)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 97.073% (10313/10624)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.080% (10438/10752)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.077% (10562/10880)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.102% (10689/11008)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.099% (10813/11136)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.124% (10940/11264)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.138% (11066/11392)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.109% (11187/11520)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.115% (11312/11648)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.113% (11436/11776)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.110% (11560/11904)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.116% (11685/12032)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.130% (11811/12160)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.135% (11936/12288)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.141% (12061/12416)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.114% (12182/12544)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.128% (12308/12672)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.125% (12432/12800)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.130% (12557/12928)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.128% (12681/13056)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.103% (12802/13184)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.093% (12925/13312)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.091% (13049/13440)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.096% (13174/13568)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.109% (13300/13696)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.114% (13425/13824)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.112% (13549/13952)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.131% (13676/14080)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.128% (13800/14208)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.105% (13921/14336)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.096% (14044/14464)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.108% (14170/14592)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.106% (14294/14720)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.117% (14420/14848)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.102% (14542/14976)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.093% (14665/15104)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.079% (14787/15232)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.070% (14910/15360)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.069% (15034/15488)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.067% (15158/15616)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.046% (15279/15744)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.045% (15403/15872)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.031% (15525/16000)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.024% (15648/16128)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.010% (15770/16256)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.028% (15897/16384)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.032% (16022/16512)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.013% (16143/16640)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.024% (16269/16768)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.035% (16395/16896)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.034% (16519/17024)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.044% (16645/17152)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.043% (16769/17280)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.036% (16892/17408)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.029% (17015/17536)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.045% (17142/17664)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.044% (17266/17792)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.042% (17390/17920)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.036% (17513/18048)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.040% (17638/18176)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.044% (17763/18304)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.016% (17882/18432)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.988% (18001/18560)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.993% (18126/18688)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.008% (18253/18816)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.986% (18373/18944)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.990% (18498/19072)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.979% (18620/19200)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.978% (18744/19328)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.973% (18867/19456)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.982% (18993/19584)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.976% (19116/19712)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.981% (19241/19840)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.980% (19365/19968)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.984% (19490/20096)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.989% (19615/20224)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.993% (19740/20352)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.987% (19863/20480)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.991% (19988/20608)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.986% (20111/20736)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.985% (20235/20864)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.989% (20360/20992)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.965% (20479/21120)\n",
            "Train Epoch: 140 | Loss: 0.088 | Acc: 96.974% (20605/21248)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.983% (20731/21376)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.982% (20855/21504)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.991% (20981/21632)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.990% (21105/21760)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.994% (21230/21888)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.007% (21357/22016)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.006% (21481/22144)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.992% (21602/22272)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.978% (21723/22400)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.982% (21848/22528)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.985% (21973/22656)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.998% (22100/22784)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 96.997% (22224/22912)\n",
            "Train Epoch: 140 | Loss: 0.087 | Acc: 97.010% (22351/23040)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.026% (22479/23168)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.025% (22603/23296)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.020% (22726/23424)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.028% (22852/23552)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.031% (22977/23680)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.022% (23099/23808)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.021% (23223/23936)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.020% (23347/24064)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.016% (23470/24192)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.011% (23593/24320)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.018% (23719/24448)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.017% (23843/24576)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.017% (23967/24704)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.008% (24089/24832)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.023% (24217/24960)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.011% (24338/25088)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.026% (24466/25216)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.037% (24593/25344)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.036% (24717/25472)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.035% (24841/25600)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.038% (24966/25728)\n",
            "Train Epoch: 140 | Loss: 0.086 | Acc: 97.034% (25089/25856)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.040% (25215/25984)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.044% (25340/26112)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.050% (25466/26240)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.046% (25589/26368)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.056% (25716/26496)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.055% (25840/26624)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.058% (25965/26752)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.061% (26090/26880)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.060% (26214/27008)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.063% (26339/27136)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.062% (26463/27264)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.058% (26586/27392)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.068% (26713/27520)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.067% (26837/27648)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.062% (26960/27776)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.054% (27082/27904)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.061% (27208/28032)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.060% (27332/28160)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.066% (27458/28288)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.065% (27582/28416)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.075% (27709/28544)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.077% (27834/28672)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.076% (27958/28800)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.075% (28082/28928)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.071% (28205/29056)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.070% (28329/29184)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.076% (28455/29312)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.069% (28577/29440)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.064% (28700/29568)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.067% (28825/29696)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.063% (28948/29824)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.065% (29073/29952)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.064% (29197/30080)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.077% (29325/30208)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.079% (29450/30336)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.079% (29574/30464)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.084% (29700/30592)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.087% (29825/30720)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.089% (29950/30848)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (30075/30976)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.084% (30197/31104)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.077% (30319/31232)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.076% (30443/31360)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.081% (30569/31488)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.077% (30692/31616)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.070% (30814/31744)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.082% (30942/31872)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.084% (31067/32000)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.084% (31191/32128)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.086% (31316/32256)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (31442/32384)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.090% (31566/32512)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.093% (31691/32640)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.092% (31815/32768)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (31939/32896)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (32065/33024)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.101% (32191/33152)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.094% (32313/33280)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.102% (32440/33408)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.111% (32567/33536)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.113% (32692/33664)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (32814/33792)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.114% (32941/33920)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.107% (33063/34048)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.097% (33184/34176)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (33309/34304)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (33433/34432)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.098% (33557/34560)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.097% (33681/34688)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (33805/34816)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.092% (33928/34944)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.092% (34052/35072)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.097% (34178/35200)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (34303/35328)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (34430/35456)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.105% (34554/35584)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (34675/35712)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.087% (34796/35840)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.086% (34920/35968)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.080% (35042/36096)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.079% (35166/36224)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.073% (35288/36352)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.078% (35414/36480)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.083% (35540/36608)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.082% (35664/36736)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.076% (35786/36864)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.070% (35908/36992)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.074% (36034/37120)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.076% (36159/37248)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.078% (36284/37376)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.078% (36408/37504)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.080% (36533/37632)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.079% (36657/37760)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.084% (36783/37888)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (36910/38016)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.090% (37034/38144)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.089% (37158/38272)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (37283/38400)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.093% (37408/38528)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.087% (37530/38656)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.092% (37656/38784)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.093% (37781/38912)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (37906/39040)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.100% (38032/39168)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.107% (38159/39296)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (38283/39424)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.110% (38409/39552)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.107% (38532/39680)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (38656/39808)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.108% (38781/39936)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.110% (38906/40064)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (39029/40192)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.103% (39152/40320)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.107% (39278/40448)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.102% (39400/40576)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (39522/40704)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (39644/40832)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (39770/40960)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (39896/41088)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.091% (40017/41216)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (40143/41344)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.092% (40266/41472)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.094% (40391/41600)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.098% (40517/41728)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (40640/41856)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.104% (40768/41984)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.110% (40895/42112)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.109% (41019/42240)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.102% (41140/42368)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (41262/42496)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.105% (41390/42624)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.107% (41515/42752)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.104% (41638/42880)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.094% (41758/43008)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.093% (41882/43136)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (42007/43264)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (42132/43392)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.102% (42259/43520)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.106% (42385/43648)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.099% (42506/43776)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.091% (42627/43904)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.093% (42752/44032)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.086% (42873/44160)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.087% (42998/44288)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.084% (43121/44416)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.091% (43248/44544)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.092% (43373/44672)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.094% (43498/44800)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.095% (43623/44928)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.097% (43748/45056)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.094% (43871/45184)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.100% (43998/45312)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.102% (44123/45440)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.101% (44247/45568)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.105% (44373/45696)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.104% (44497/45824)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.103% (44621/45952)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.105% (44746/46080)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.104% (44870/46208)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.104% (44994/46336)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.105% (45119/46464)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.107% (45244/46592)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.102% (45366/46720)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.101% (45490/46848)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.096% (45612/46976)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.092% (45734/47104)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.091% (45858/47232)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.095% (45984/47360)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.092% (46107/47488)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.083% (46227/47616)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.084% (46352/47744)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.082% (46475/47872)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.085% (46601/48000)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.085% (46725/48128)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.082% (46848/48256)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.082% (46972/48384)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.081% (47096/48512)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.087% (47223/48640)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.094% (47351/48768)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (47476/48896)\n",
            "Train Epoch: 140 | Loss: 0.085 | Acc: 97.089% (47597/49024)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (47724/49152)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.096% (47849/49280)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.094% (47972/49408)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.095% (48097/49536)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (48223/49664)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.100% (48348/49792)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.099% (48472/49920)\n",
            "Train Epoch: 140 | Loss: 0.084 | Acc: 97.100% (48550/50000)\n",
            "Test Epoch: 140 | Loss: 0.367 | Acc: 89.000% (89/100)\n",
            "Test Epoch: 140 | Loss: 0.368 | Acc: 88.500% (177/200)\n",
            "Test Epoch: 140 | Loss: 0.373 | Acc: 87.667% (263/300)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 89.750% (359/400)\n",
            "Test Epoch: 140 | Loss: 0.287 | Acc: 90.800% (454/500)\n",
            "Test Epoch: 140 | Loss: 0.265 | Acc: 91.500% (549/600)\n",
            "Test Epoch: 140 | Loss: 0.267 | Acc: 91.571% (641/700)\n",
            "Test Epoch: 140 | Loss: 0.299 | Acc: 90.875% (727/800)\n",
            "Test Epoch: 140 | Loss: 0.302 | Acc: 90.667% (816/900)\n",
            "Test Epoch: 140 | Loss: 0.295 | Acc: 90.600% (906/1000)\n",
            "Test Epoch: 140 | Loss: 0.286 | Acc: 90.818% (999/1100)\n",
            "Test Epoch: 140 | Loss: 0.297 | Acc: 90.500% (1086/1200)\n",
            "Test Epoch: 140 | Loss: 0.290 | Acc: 90.692% (1179/1300)\n",
            "Test Epoch: 140 | Loss: 0.278 | Acc: 91.071% (1275/1400)\n",
            "Test Epoch: 140 | Loss: 0.278 | Acc: 91.133% (1367/1500)\n",
            "Test Epoch: 140 | Loss: 0.273 | Acc: 91.312% (1461/1600)\n",
            "Test Epoch: 140 | Loss: 0.272 | Acc: 91.471% (1555/1700)\n",
            "Test Epoch: 140 | Loss: 0.275 | Acc: 91.444% (1646/1800)\n",
            "Test Epoch: 140 | Loss: 0.283 | Acc: 91.368% (1736/1900)\n",
            "Test Epoch: 140 | Loss: 0.291 | Acc: 91.350% (1827/2000)\n",
            "Test Epoch: 140 | Loss: 0.291 | Acc: 91.238% (1916/2100)\n",
            "Test Epoch: 140 | Loss: 0.299 | Acc: 91.045% (2003/2200)\n",
            "Test Epoch: 140 | Loss: 0.307 | Acc: 90.783% (2088/2300)\n",
            "Test Epoch: 140 | Loss: 0.303 | Acc: 90.875% (2181/2400)\n",
            "Test Epoch: 140 | Loss: 0.312 | Acc: 90.880% (2272/2500)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 90.731% (2359/2600)\n",
            "Test Epoch: 140 | Loss: 0.314 | Acc: 90.815% (2452/2700)\n",
            "Test Epoch: 140 | Loss: 0.316 | Acc: 90.857% (2544/2800)\n",
            "Test Epoch: 140 | Loss: 0.320 | Acc: 90.862% (2635/2900)\n",
            "Test Epoch: 140 | Loss: 0.320 | Acc: 90.867% (2726/3000)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.677% (2811/3100)\n",
            "Test Epoch: 140 | Loss: 0.328 | Acc: 90.656% (2901/3200)\n",
            "Test Epoch: 140 | Loss: 0.328 | Acc: 90.697% (2993/3300)\n",
            "Test Epoch: 140 | Loss: 0.324 | Acc: 90.735% (3085/3400)\n",
            "Test Epoch: 140 | Loss: 0.325 | Acc: 90.657% (3173/3500)\n",
            "Test Epoch: 140 | Loss: 0.323 | Acc: 90.750% (3267/3600)\n",
            "Test Epoch: 140 | Loss: 0.328 | Acc: 90.703% (3356/3700)\n",
            "Test Epoch: 140 | Loss: 0.328 | Acc: 90.684% (3446/3800)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.769% (3540/3900)\n",
            "Test Epoch: 140 | Loss: 0.323 | Acc: 90.825% (3633/4000)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.805% (3723/4100)\n",
            "Test Epoch: 140 | Loss: 0.327 | Acc: 90.810% (3814/4200)\n",
            "Test Epoch: 140 | Loss: 0.321 | Acc: 90.977% (3912/4300)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.045% (4006/4400)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.067% (4098/4500)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.065% (4189/4600)\n",
            "Test Epoch: 140 | Loss: 0.316 | Acc: 91.106% (4282/4700)\n",
            "Test Epoch: 140 | Loss: 0.320 | Acc: 91.083% (4372/4800)\n",
            "Test Epoch: 140 | Loss: 0.315 | Acc: 91.204% (4469/4900)\n",
            "Test Epoch: 140 | Loss: 0.320 | Acc: 91.140% (4557/5000)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.176% (4650/5100)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.115% (4738/5200)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.094% (4828/5300)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.056% (4917/5400)\n",
            "Test Epoch: 140 | Loss: 0.321 | Acc: 91.036% (5007/5500)\n",
            "Test Epoch: 140 | Loss: 0.321 | Acc: 91.036% (5098/5600)\n",
            "Test Epoch: 140 | Loss: 0.325 | Acc: 90.877% (5180/5700)\n",
            "Test Epoch: 140 | Loss: 0.325 | Acc: 90.879% (5271/5800)\n",
            "Test Epoch: 140 | Loss: 0.327 | Acc: 90.814% (5358/5900)\n",
            "Test Epoch: 140 | Loss: 0.327 | Acc: 90.833% (5450/6000)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.885% (5544/6100)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.903% (5636/6200)\n",
            "Test Epoch: 140 | Loss: 0.326 | Acc: 90.937% (5729/6300)\n",
            "Test Epoch: 140 | Loss: 0.324 | Acc: 90.969% (5822/6400)\n",
            "Test Epoch: 140 | Loss: 0.324 | Acc: 90.969% (5913/6500)\n",
            "Test Epoch: 140 | Loss: 0.324 | Acc: 91.015% (6007/6600)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 91.060% (6101/6700)\n",
            "Test Epoch: 140 | Loss: 0.324 | Acc: 91.000% (6188/6800)\n",
            "Test Epoch: 140 | Loss: 0.323 | Acc: 91.029% (6281/6900)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 91.029% (6372/7000)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 91.028% (6463/7100)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 91.028% (6554/7200)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.082% (6649/7300)\n",
            "Test Epoch: 140 | Loss: 0.317 | Acc: 91.149% (6745/7400)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.107% (6833/7500)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.118% (6925/7600)\n",
            "Test Epoch: 140 | Loss: 0.322 | Acc: 91.039% (7010/7700)\n",
            "Test Epoch: 140 | Loss: 0.320 | Acc: 91.090% (7105/7800)\n",
            "Test Epoch: 140 | Loss: 0.321 | Acc: 91.101% (7197/7900)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.138% (7291/8000)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.160% (7384/8100)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.183% (7477/8200)\n",
            "Test Epoch: 140 | Loss: 0.317 | Acc: 91.193% (7569/8300)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.190% (7660/8400)\n",
            "Test Epoch: 140 | Loss: 0.318 | Acc: 91.165% (7749/8500)\n",
            "Test Epoch: 140 | Loss: 0.319 | Acc: 91.140% (7838/8600)\n",
            "Test Epoch: 140 | Loss: 0.317 | Acc: 91.172% (7932/8700)\n",
            "Test Epoch: 140 | Loss: 0.317 | Acc: 91.182% (8024/8800)\n",
            "Test Epoch: 140 | Loss: 0.316 | Acc: 91.191% (8116/8900)\n",
            "Test Epoch: 140 | Loss: 0.316 | Acc: 91.189% (8207/9000)\n",
            "Test Epoch: 140 | Loss: 0.316 | Acc: 91.176% (8297/9100)\n",
            "Test Epoch: 140 | Loss: 0.315 | Acc: 91.217% (8392/9200)\n",
            "Test Epoch: 140 | Loss: 0.315 | Acc: 91.215% (8483/9300)\n",
            "Test Epoch: 140 | Loss: 0.315 | Acc: 91.213% (8574/9400)\n",
            "Test Epoch: 140 | Loss: 0.314 | Acc: 91.221% (8666/9500)\n",
            "Test Epoch: 140 | Loss: 0.314 | Acc: 91.250% (8760/9600)\n",
            "Test Epoch: 140 | Loss: 0.312 | Acc: 91.299% (8856/9700)\n",
            "Test Epoch: 140 | Loss: 0.311 | Acc: 91.337% (8951/9800)\n",
            "Test Epoch: 140 | Loss: 0.314 | Acc: 91.283% (9037/9900)\n",
            "Test Epoch: 140 | Loss: 0.314 | Acc: 91.300% (9130/10000)\n",
            "\n",
            "Epoch: 141\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.656% (125/128)\n",
            "Train Epoch: 141 | Loss: 0.055 | Acc: 98.438% (252/256)\n",
            "Train Epoch: 141 | Loss: 0.053 | Acc: 98.177% (377/384)\n",
            "Train Epoch: 141 | Loss: 0.053 | Acc: 98.242% (503/512)\n",
            "Train Epoch: 141 | Loss: 0.059 | Acc: 98.281% (629/640)\n",
            "Train Epoch: 141 | Loss: 0.063 | Acc: 97.917% (752/768)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.656% (875/896)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.754% (1001/1024)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.743% (1126/1152)\n",
            "Train Epoch: 141 | Loss: 0.064 | Acc: 97.891% (1253/1280)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.798% (1377/1408)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.852% (1503/1536)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.776% (1627/1664)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.879% (1754/1792)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.812% (1878/1920)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.852% (2004/2048)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.886% (2130/2176)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.960% (2257/2304)\n",
            "Train Epoch: 141 | Loss: 0.063 | Acc: 98.026% (2384/2432)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.891% (2506/2560)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.954% (2633/2688)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.834% (2755/2816)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.894% (2882/2944)\n",
            "Train Epoch: 141 | Loss: 0.064 | Acc: 97.982% (3010/3072)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.938% (3134/3200)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.837% (3256/3328)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.772% (3379/3456)\n",
            "Train Epoch: 141 | Loss: 0.065 | Acc: 97.824% (3506/3584)\n",
            "Train Epoch: 141 | Loss: 0.069 | Acc: 97.737% (3628/3712)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.786% (3755/3840)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.757% (3879/3968)\n",
            "Train Epoch: 141 | Loss: 0.069 | Acc: 97.729% (4003/4096)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.775% (4130/4224)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.771% (4255/4352)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.790% (4381/4480)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.786% (4506/4608)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.804% (4632/4736)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.800% (4757/4864)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.837% (4884/4992)\n",
            "Train Epoch: 141 | Loss: 0.066 | Acc: 97.871% (5011/5120)\n",
            "Train Epoch: 141 | Loss: 0.067 | Acc: 97.847% (5135/5248)\n",
            "Train Epoch: 141 | Loss: 0.068 | Acc: 97.786% (5257/5376)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.711% (5378/5504)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.656% (5500/5632)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.604% (5622/5760)\n",
            "Train Epoch: 141 | Loss: 0.072 | Acc: 97.588% (5746/5888)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.590% (5871/6016)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.607% (5997/6144)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.624% (6123/6272)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.562% (6244/6400)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.564% (6369/6528)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.566% (6494/6656)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.538% (6617/6784)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.541% (6742/6912)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.571% (6869/7040)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.600% (6996/7168)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.560% (7118/7296)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.562% (7243/7424)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.590% (7370/7552)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.591% (7495/7680)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.579% (7619/7808)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.593% (7745/7936)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.594% (7870/8064)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.595% (7995/8192)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.584% (8119/8320)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.573% (8243/8448)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.598% (8370/8576)\n",
            "Train Epoch: 141 | Loss: 0.070 | Acc: 97.610% (8496/8704)\n",
            "Train Epoch: 141 | Loss: 0.071 | Acc: 97.588% (8619/8832)\n",
            "Train Epoch: 141 | Loss: 0.072 | Acc: 97.556% (8741/8960)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.535% (8864/9088)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.526% (8988/9216)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.528% (9113/9344)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.530% (9238/9472)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.531% (9363/9600)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.543% (9489/9728)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.534% (9613/9856)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.526% (9737/9984)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.547% (9864/10112)\n",
            "Train Epoch: 141 | Loss: 0.072 | Acc: 97.568% (9991/10240)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.569% (10116/10368)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.542% (10238/10496)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.562% (10365/10624)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.573% (10491/10752)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.555% (10614/10880)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.529% (10736/11008)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.513% (10859/11136)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.532% (10986/11264)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.507% (11108/11392)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.500% (11232/11520)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.510% (11358/11648)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.512% (11483/11776)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.522% (11609/11904)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.532% (11735/12032)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.516% (11858/12160)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.493% (11980/12288)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.471% (12102/12416)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.481% (12228/12544)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.475% (12352/12672)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.469% (12476/12800)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.455% (12599/12928)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.449% (12723/13056)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.451% (12848/13184)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.468% (12975/13312)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.463% (13099/13440)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.487% (13227/13568)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.503% (13354/13696)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.519% (13481/13824)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.520% (13606/13952)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.528% (13732/14080)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.537% (13858/14208)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.538% (13983/14336)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.525% (14106/14464)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.526% (14231/14592)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.541% (14358/14720)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.548% (14484/14848)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.543% (14608/14976)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.544% (14733/15104)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.518% (14854/15232)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.539% (14982/15360)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.540% (15107/15488)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.554% (15234/15616)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.555% (15359/15744)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.549% (15483/15872)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.544% (15607/16000)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.532% (15730/16128)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.527% (15854/16256)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.528% (15979/16384)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.529% (16104/16512)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.530% (16229/16640)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.519% (16352/16768)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.520% (16477/16896)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.521% (16602/17024)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.534% (16729/17152)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.541% (16855/17280)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.541% (16980/17408)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.542% (17105/17536)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.537% (17229/17664)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.544% (17355/17792)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.550% (17481/17920)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.545% (17605/18048)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.546% (17730/18176)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.536% (17853/18304)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.526% (17976/18432)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.538% (18103/18560)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.539% (18228/18688)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.534% (18352/18816)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.530% (18476/18944)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.536% (18602/19072)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.542% (18728/19200)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.542% (18853/19328)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.538% (18977/19456)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.534% (19101/19584)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.540% (19227/19712)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.535% (19351/19840)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.536% (19476/19968)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.532% (19600/20096)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.547% (19728/20224)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.543% (19852/20352)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.544% (19977/20480)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.540% (20101/20608)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.550% (20228/20736)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.551% (20353/20864)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.566% (20481/20992)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.562% (20605/21120)\n",
            "Train Epoch: 141 | Loss: 0.073 | Acc: 97.557% (20729/21248)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.539% (20850/21376)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.521% (20971/21504)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.536% (21099/21632)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.541% (21225/21760)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.547% (21351/21888)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.534% (21473/22016)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.539% (21599/22144)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.548% (21726/22272)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.531% (21847/22400)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.532% (21972/22528)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.524% (22095/22656)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.520% (22219/22784)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.534% (22347/22912)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.522% (22469/23040)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.522% (22594/23168)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.515% (22717/23296)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.520% (22843/23424)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.516% (22967/23552)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.521% (23093/23680)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.522% (23218/23808)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.518% (23342/23936)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.511% (23465/24064)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.507% (23589/24192)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.504% (23713/24320)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.497% (23836/24448)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.493% (23960/24576)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.478% (24081/24704)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.487% (24208/24832)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.488% (24333/24960)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.485% (24457/25088)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.478% (24580/25216)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.479% (24705/25344)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.487% (24832/25472)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.488% (24957/25600)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.481% (25080/25728)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.482% (25205/25856)\n",
            "Train Epoch: 141 | Loss: 0.074 | Acc: 97.475% (25328/25984)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.465% (25450/26112)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.458% (25573/26240)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.459% (25698/26368)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.456% (25822/26496)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.457% (25947/26624)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.458% (26072/26752)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.459% (26197/26880)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.460% (26322/27008)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.461% (26447/27136)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.469% (26574/27264)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.463% (26697/27392)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.453% (26819/27520)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.450% (26943/27648)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.455% (27069/27776)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.438% (27189/27904)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.424% (27310/28032)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.433% (27437/28160)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.434% (27562/28288)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.435% (27687/28416)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.446% (27815/28544)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.443% (27939/28672)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.455% (28067/28800)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.452% (28191/28928)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.464% (28319/29056)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.464% (28444/29184)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.465% (28569/29312)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.463% (28693/29440)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.470% (28820/29568)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.474% (28946/29696)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.472% (29070/29824)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.463% (29192/29952)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.460% (29316/30080)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.454% (29439/30208)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.458% (29565/30336)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.466% (29692/30464)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.460% (29815/30592)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.451% (29937/30720)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.442% (30059/30848)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.440% (30183/30976)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.447% (30310/31104)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.432% (30430/31232)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.430% (30554/31360)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.434% (30680/31488)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.438% (30806/31616)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.436% (30930/31744)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.437% (31055/31872)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.441% (31181/32000)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.441% (31306/32128)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.439% (31430/32256)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.434% (31553/32384)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.422% (31674/32512)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.430% (31801/32640)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.430% (31926/32768)\n",
            "Train Epoch: 141 | Loss: 0.075 | Acc: 97.428% (32050/32896)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.432% (32176/33024)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.430% (32300/33152)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.419% (32421/33280)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.420% (32546/33408)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.421% (32671/33536)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.413% (32793/33664)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.417% (32919/33792)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.417% (33044/33920)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.418% (33169/34048)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.413% (33292/34176)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.408% (33415/34304)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.409% (33540/34432)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.410% (33665/34560)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.411% (33790/34688)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.412% (33915/34816)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.419% (34042/34944)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.414% (34165/35072)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.406% (34287/35200)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.410% (34413/35328)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.408% (34537/35456)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.406% (34661/35584)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.404% (34785/35712)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.402% (34909/35840)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.406% (35035/35968)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.404% (35159/36096)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.408% (35285/36224)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.414% (35412/36352)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.415% (35537/36480)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.416% (35662/36608)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.422% (35789/36736)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.412% (35910/36864)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.410% (36034/36992)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.414% (36160/37120)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.415% (36285/37248)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.405% (36406/37376)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.414% (36534/37504)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.412% (36658/37632)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.413% (36783/37760)\n",
            "Train Epoch: 141 | Loss: 0.076 | Acc: 97.416% (36909/37888)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.406% (37030/38016)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.410% (37156/38144)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.408% (37280/38272)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.398% (37401/38400)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.397% (37525/38528)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.403% (37652/38656)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.404% (37777/38784)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.399% (37900/38912)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.403% (38026/39040)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.396% (38148/39168)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.384% (38268/39296)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.385% (38393/39424)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.386% (38518/39552)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.394% (38646/39680)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.392% (38770/39808)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.391% (38894/39936)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.389% (39018/40064)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.393% (39144/40192)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.386% (39266/40320)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (39389/40448)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.385% (39515/40576)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.379% (39637/40704)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (39763/40832)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.383% (39888/40960)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.386% (40014/41088)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.387% (40139/41216)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (40261/41344)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.384% (40387/41472)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.387% (40513/41600)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.388% (40638/41728)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.391% (40764/41856)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.397% (40891/41984)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.397% (41016/42112)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.401% (41142/42240)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.397% (41265/42368)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.393% (41388/42496)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.393% (41513/42624)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.394% (41638/42752)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.397% (41764/42880)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.391% (41886/43008)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.385% (42008/43136)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.386% (42133/43264)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.387% (42258/43392)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.385% (42382/43520)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.388% (42508/43648)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.389% (42633/43776)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.387% (42757/43904)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.377% (42877/44032)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.378% (43002/44160)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.376% (43126/44288)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.384% (43254/44416)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (43378/44544)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (43502/44672)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (43627/44800)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.385% (43753/44928)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.383% (43877/45056)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (44001/45184)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.383% (44126/45312)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (44250/45440)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.380% (44374/45568)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.383% (44500/45696)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (44624/45824)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.380% (44748/45952)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.383% (44874/46080)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.377% (44996/46208)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.378% (45121/46336)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.372% (45243/46464)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.371% (45367/46592)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.372% (45492/46720)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.374% (45618/46848)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.380% (45745/46976)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.380% (45870/47104)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.379% (45994/47232)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.380% (46119/47360)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (46245/47488)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (46369/47616)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.382% (46494/47744)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.381% (46618/47872)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.373% (46739/48000)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.370% (46862/48128)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.368% (46986/48256)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.367% (47110/48384)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.364% (47233/48512)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.358% (47355/48640)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.361% (47481/48768)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.356% (47603/48896)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.356% (47728/49024)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.359% (47854/49152)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.354% (47976/49280)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.355% (48101/49408)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.355% (48226/49536)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.354% (48350/49664)\n",
            "Train Epoch: 141 | Loss: 0.078 | Acc: 97.349% (48472/49792)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.350% (48597/49920)\n",
            "Train Epoch: 141 | Loss: 0.077 | Acc: 97.352% (48676/50000)\n",
            "Test Epoch: 141 | Loss: 0.318 | Acc: 91.000% (91/100)\n",
            "Test Epoch: 141 | Loss: 0.311 | Acc: 90.500% (181/200)\n",
            "Test Epoch: 141 | Loss: 0.303 | Acc: 90.667% (272/300)\n",
            "Test Epoch: 141 | Loss: 0.269 | Acc: 91.000% (364/400)\n",
            "Test Epoch: 141 | Loss: 0.243 | Acc: 91.400% (457/500)\n",
            "Test Epoch: 141 | Loss: 0.238 | Acc: 91.833% (551/600)\n",
            "Test Epoch: 141 | Loss: 0.230 | Acc: 92.286% (646/700)\n",
            "Test Epoch: 141 | Loss: 0.253 | Acc: 91.500% (732/800)\n",
            "Test Epoch: 141 | Loss: 0.261 | Acc: 91.667% (825/900)\n",
            "Test Epoch: 141 | Loss: 0.258 | Acc: 92.000% (920/1000)\n",
            "Test Epoch: 141 | Loss: 0.254 | Acc: 92.182% (1014/1100)\n",
            "Test Epoch: 141 | Loss: 0.263 | Acc: 91.833% (1102/1200)\n",
            "Test Epoch: 141 | Loss: 0.254 | Acc: 92.077% (1197/1300)\n",
            "Test Epoch: 141 | Loss: 0.242 | Acc: 92.500% (1295/1400)\n",
            "Test Epoch: 141 | Loss: 0.240 | Acc: 92.667% (1390/1500)\n",
            "Test Epoch: 141 | Loss: 0.239 | Acc: 92.750% (1484/1600)\n",
            "Test Epoch: 141 | Loss: 0.243 | Acc: 92.824% (1578/1700)\n",
            "Test Epoch: 141 | Loss: 0.245 | Acc: 92.833% (1671/1800)\n",
            "Test Epoch: 141 | Loss: 0.248 | Acc: 92.789% (1763/1900)\n",
            "Test Epoch: 141 | Loss: 0.259 | Acc: 92.650% (1853/2000)\n",
            "Test Epoch: 141 | Loss: 0.256 | Acc: 92.524% (1943/2100)\n",
            "Test Epoch: 141 | Loss: 0.259 | Acc: 92.455% (2034/2200)\n",
            "Test Epoch: 141 | Loss: 0.268 | Acc: 92.174% (2120/2300)\n",
            "Test Epoch: 141 | Loss: 0.263 | Acc: 92.250% (2214/2400)\n",
            "Test Epoch: 141 | Loss: 0.279 | Acc: 92.040% (2301/2500)\n",
            "Test Epoch: 141 | Loss: 0.292 | Acc: 91.923% (2390/2600)\n",
            "Test Epoch: 141 | Loss: 0.287 | Acc: 91.926% (2482/2700)\n",
            "Test Epoch: 141 | Loss: 0.288 | Acc: 91.893% (2573/2800)\n",
            "Test Epoch: 141 | Loss: 0.291 | Acc: 91.828% (2663/2900)\n",
            "Test Epoch: 141 | Loss: 0.292 | Acc: 91.833% (2755/3000)\n",
            "Test Epoch: 141 | Loss: 0.295 | Acc: 91.677% (2842/3100)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 91.625% (2932/3200)\n",
            "Test Epoch: 141 | Loss: 0.295 | Acc: 91.606% (3023/3300)\n",
            "Test Epoch: 141 | Loss: 0.294 | Acc: 91.618% (3115/3400)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 91.657% (3208/3500)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 91.750% (3303/3600)\n",
            "Test Epoch: 141 | Loss: 0.302 | Acc: 91.676% (3392/3700)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.579% (3480/3800)\n",
            "Test Epoch: 141 | Loss: 0.297 | Acc: 91.667% (3575/3900)\n",
            "Test Epoch: 141 | Loss: 0.294 | Acc: 91.750% (3670/4000)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.634% (3757/4100)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.643% (3849/4200)\n",
            "Test Epoch: 141 | Loss: 0.294 | Acc: 91.814% (3948/4300)\n",
            "Test Epoch: 141 | Loss: 0.295 | Acc: 91.795% (4039/4400)\n",
            "Test Epoch: 141 | Loss: 0.295 | Acc: 91.800% (4131/4500)\n",
            "Test Epoch: 141 | Loss: 0.297 | Acc: 91.783% (4222/4600)\n",
            "Test Epoch: 141 | Loss: 0.294 | Acc: 91.851% (4317/4700)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.771% (4405/4800)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 91.878% (4502/4900)\n",
            "Test Epoch: 141 | Loss: 0.304 | Acc: 91.780% (4589/5000)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.863% (4685/5100)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.827% (4775/5200)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.811% (4866/5300)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.870% (4961/5400)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.891% (5054/5500)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.893% (5146/5600)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.895% (5238/5700)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.897% (5330/5800)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.831% (5418/5900)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.833% (5510/6000)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.803% (5600/6100)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.806% (5692/6200)\n",
            "Test Epoch: 141 | Loss: 0.302 | Acc: 91.841% (5786/6300)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.922% (5883/6400)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.892% (5973/6500)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.924% (6067/6600)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.955% (6161/6700)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.882% (6248/6800)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.913% (6342/6900)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.886% (6432/7000)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.901% (6525/7100)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.903% (6617/7200)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.932% (6711/7300)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 92.000% (6808/7400)\n",
            "Test Epoch: 141 | Loss: 0.298 | Acc: 91.987% (6899/7500)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.974% (6990/7600)\n",
            "Test Epoch: 141 | Loss: 0.302 | Acc: 91.935% (7079/7700)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.962% (7173/7800)\n",
            "Test Epoch: 141 | Loss: 0.302 | Acc: 91.949% (7264/7900)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.963% (7357/8000)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 92.025% (7454/8100)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 92.012% (7545/8200)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.988% (7635/8300)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.976% (7726/8400)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.976% (7818/8500)\n",
            "Test Epoch: 141 | Loss: 0.302 | Acc: 91.930% (7906/8600)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.954% (8000/8700)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.966% (8093/8800)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.955% (8184/8900)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.956% (8276/9000)\n",
            "Test Epoch: 141 | Loss: 0.301 | Acc: 91.934% (8366/9100)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.957% (8460/9200)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.957% (8552/9300)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.979% (8646/9400)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.958% (8736/9500)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 91.969% (8829/9600)\n",
            "Test Epoch: 141 | Loss: 0.297 | Acc: 92.010% (8925/9700)\n",
            "Test Epoch: 141 | Loss: 0.296 | Acc: 92.041% (9020/9800)\n",
            "Test Epoch: 141 | Loss: 0.299 | Acc: 92.000% (9108/9900)\n",
            "Test Epoch: 141 | Loss: 0.300 | Acc: 91.990% (9199/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 142\n",
            "Train Epoch: 142 | Loss: 0.053 | Acc: 96.875% (124/128)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 96.094% (246/256)\n",
            "Train Epoch: 142 | Loss: 0.070 | Acc: 96.615% (371/384)\n",
            "Train Epoch: 142 | Loss: 0.066 | Acc: 97.266% (498/512)\n",
            "Train Epoch: 142 | Loss: 0.068 | Acc: 97.344% (623/640)\n",
            "Train Epoch: 142 | Loss: 0.068 | Acc: 97.526% (749/768)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.098% (870/896)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.070% (994/1024)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.135% (1119/1152)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.188% (1244/1280)\n",
            "Train Epoch: 142 | Loss: 0.071 | Acc: 97.372% (1371/1408)\n",
            "Train Epoch: 142 | Loss: 0.071 | Acc: 97.266% (1494/1536)\n",
            "Train Epoch: 142 | Loss: 0.069 | Acc: 97.416% (1621/1664)\n",
            "Train Epoch: 142 | Loss: 0.071 | Acc: 97.321% (1744/1792)\n",
            "Train Epoch: 142 | Loss: 0.069 | Acc: 97.448% (1871/1920)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.266% (1992/2048)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.197% (2115/2176)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.179% (2239/2304)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.163% (2363/2432)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.188% (2488/2560)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.135% (2611/2688)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.195% (2737/2816)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.249% (2863/2944)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.298% (2989/3072)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.156% (3109/3200)\n",
            "Train Epoch: 142 | Loss: 0.081 | Acc: 97.145% (3233/3328)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.193% (3359/3456)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.210% (3484/3584)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.252% (3610/3712)\n",
            "Train Epoch: 142 | Loss: 0.081 | Acc: 97.109% (3729/3840)\n",
            "Train Epoch: 142 | Loss: 0.081 | Acc: 97.127% (3854/3968)\n",
            "Train Epoch: 142 | Loss: 0.082 | Acc: 97.119% (3978/4096)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.206% (4106/4224)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.220% (4231/4352)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.232% (4356/4480)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.201% (4479/4608)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.213% (4604/4736)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.266% (4731/4864)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.216% (4853/4992)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.227% (4978/5120)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.199% (5101/5248)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.210% (5226/5376)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.275% (5354/5504)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.283% (5479/5632)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.274% (5603/5760)\n",
            "Train Epoch: 142 | Loss: 0.080 | Acc: 97.266% (5727/5888)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.291% (5853/6016)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.331% (5980/6144)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.321% (6104/6272)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.312% (6228/6400)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.319% (6353/6528)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.326% (6478/6656)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.361% (6605/6784)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.410% (6733/6912)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.401% (6857/7040)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.405% (6982/7168)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.423% (7108/7296)\n",
            "Train Epoch: 142 | Loss: 0.079 | Acc: 97.414% (7232/7424)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.444% (7359/7552)\n",
            "Train Epoch: 142 | Loss: 0.078 | Acc: 97.461% (7485/7680)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.490% (7612/7808)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.530% (7740/7936)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.545% (7866/8064)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.522% (7989/8192)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.548% (8116/8320)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.562% (8242/8448)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.575% (8368/8576)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.576% (8493/8704)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.611% (8621/8832)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.600% (8745/8960)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.601% (8870/9088)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (8992/9216)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.603% (9120/9344)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.625% (9247/9472)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.625% (9372/9600)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.625% (9497/9728)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.616% (9621/9856)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.586% (9743/9984)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.587% (9868/10112)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.568% (9991/10240)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.560% (10115/10368)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.561% (10240/10496)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.562% (10365/10624)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.563% (10490/10752)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.555% (10614/10880)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.565% (10740/11008)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.548% (10863/11136)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.550% (10988/11264)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.560% (11114/11392)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.543% (11237/11520)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.553% (11363/11648)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.537% (11486/11776)\n",
            "Train Epoch: 142 | Loss: 0.077 | Acc: 97.555% (11613/11904)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.573% (11740/12032)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.566% (11864/12160)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.583% (11991/12288)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.576% (12115/12416)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.577% (12240/12544)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.577% (12365/12672)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.602% (12493/12800)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.610% (12619/12928)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.633% (12747/13056)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.641% (12873/13184)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.649% (12999/13312)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.656% (13125/13440)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (13250/13568)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (13376/13696)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.649% (13499/13824)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.642% (13623/13952)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.614% (13744/14080)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.586% (13865/14208)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.580% (13989/14336)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.553% (14110/14464)\n",
            "Train Epoch: 142 | Loss: 0.076 | Acc: 97.553% (14235/14592)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.554% (14360/14720)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.575% (14488/14848)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.583% (14614/14976)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.570% (14737/15104)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.564% (14861/15232)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.572% (14987/15360)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.585% (15114/15488)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.586% (15239/15616)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.574% (15362/15744)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.574% (15487/15872)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.581% (15613/16000)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.582% (15738/16128)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.576% (15862/16256)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.552% (15983/16384)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.553% (16108/16512)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.566% (16235/16640)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.555% (16358/16768)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.562% (16484/16896)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.545% (16606/17024)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.557% (16733/17152)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.552% (16857/17280)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.553% (16982/17408)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.565% (17109/17536)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.577% (17236/17664)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.578% (17361/17792)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.589% (17488/17920)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.590% (17613/18048)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.596% (17739/18176)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.585% (17862/18304)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.591% (17988/18432)\n",
            "Train Epoch: 142 | Loss: 0.075 | Acc: 97.581% (18111/18560)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.597% (18239/18688)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.603% (18365/18816)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.593% (18488/18944)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.599% (18614/19072)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.594% (18738/19200)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.589% (18862/19328)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.600% (18989/19456)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.615% (19117/19584)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.621% (19243/19712)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.626% (19369/19840)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.631% (19495/19968)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.626% (19619/20096)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.636% (19746/20224)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.637% (19871/20352)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.642% (19997/20480)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (20125/20608)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.651% (20249/20736)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (20375/20864)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.661% (20501/20992)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.647% (20623/21120)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.647% (20748/21248)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.647% (20873/21376)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.652% (20999/21504)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (21125/21632)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.642% (21247/21760)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.643% (21372/21888)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.638% (21496/22016)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.638% (21621/22144)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.643% (21747/22272)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.652% (21874/22400)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.661% (22001/22528)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.665% (22127/22656)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.665% (22252/22784)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.674% (22379/22912)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (22506/23040)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (22631/23168)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (22756/23296)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (22881/23424)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (23006/23552)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.673% (23129/23680)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.669% (23253/23808)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.669% (23378/23936)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.665% (23502/24064)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.665% (23627/24192)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.677% (23755/24320)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.673% (23879/24448)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (24002/24576)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (24127/24704)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (24252/24832)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.660% (24376/24960)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.668% (24503/25088)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.672% (24629/25216)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (24750/25344)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.660% (24876/25472)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.668% (25003/25600)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (25127/25728)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (25252/25856)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (25375/25984)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.660% (25501/26112)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.664% (25627/26240)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.660% (25751/26368)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.671% (25879/26496)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.679% (26006/26624)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.675% (26130/26752)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.671% (26254/26880)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (26382/27008)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (26507/27136)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.678% (26631/27264)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.682% (26757/27392)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.689% (26884/27520)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.692% (27010/27648)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.689% (27134/27776)\n",
            "Train Epoch: 142 | Loss: 0.072 | Acc: 97.674% (27255/27904)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.663% (27377/28032)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.660% (27501/28160)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (27624/28288)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (27750/28416)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (27875/28544)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (27999/28672)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.649% (28123/28800)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.649% (28248/28928)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.649% (28373/29056)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.656% (28500/29184)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (28624/29312)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (28749/29440)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (28874/29568)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (28999/29696)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.653% (29124/29824)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.646% (29247/29952)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.646% (29372/30080)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.646% (29497/30208)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.650% (29623/30336)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.633% (29743/30464)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.630% (29867/30592)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.630% (29992/30720)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.621% (30114/30848)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.624% (30240/30976)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.627% (30366/31104)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.618% (30488/31232)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.621% (30614/31360)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.624% (30740/31488)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.621% (30864/31616)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.612% (30986/31744)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.612% (31111/31872)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.609% (31235/32000)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.600% (31357/32128)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.604% (31483/32256)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.594% (31605/32384)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.598% (31731/32512)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.595% (31855/32640)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.583% (31976/32768)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.577% (32099/32896)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (32222/33024)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.572% (32347/33152)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (32470/33280)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (32595/33408)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (32721/33536)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.573% (32847/33664)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.582% (32975/33792)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.580% (33099/33920)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.586% (33226/34048)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.592% (33353/34176)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.589% (33477/34304)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.584% (33600/34432)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.581% (33724/34560)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.581% (33849/34688)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.584% (33975/34816)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.585% (34100/34944)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.588% (34226/35072)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.588% (34351/35200)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.585% (34475/35328)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.586% (34600/35456)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.580% (34723/35584)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.578% (34847/35712)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.581% (34973/35840)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (35094/35968)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (35217/36096)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (35340/36224)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (35465/36352)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.555% (35588/36480)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.558% (35714/36608)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.556% (35838/36736)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.553% (35962/36864)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.559% (36089/36992)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.540% (36207/37120)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.546% (36334/37248)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.552% (36461/37376)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.552% (36586/37504)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.553% (36711/37632)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.550% (36835/37760)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.553% (36961/37888)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.546% (37083/38016)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.549% (37209/38144)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.557% (37337/38272)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.552% (37460/38400)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.558% (37587/38528)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.561% (37713/38656)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.561% (37838/38784)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.556% (37961/38912)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.564% (38089/39040)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.564% (38214/39168)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (38338/39296)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (38464/39424)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.568% (38590/39552)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.568% (38715/39680)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (38839/39808)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (38965/39936)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (39089/40064)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (39215/40192)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.567% (39339/40320)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (39465/40448)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (39590/40576)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.573% (39716/40704)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.573% (39841/40832)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (39965/40960)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (40090/41088)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (40213/41216)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.567% (40338/41344)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (40462/41472)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (40586/41600)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.563% (40711/41728)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.568% (40838/41856)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.568% (40963/41984)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.573% (41090/42112)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (41214/42240)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.574% (41340/42368)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (41463/42496)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (41584/42624)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (41709/42752)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.558% (41833/42880)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.552% (41955/43008)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.552% (42080/43136)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.555% (42206/43264)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.557% (42332/43392)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (42458/43520)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.567% (42586/43648)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.563% (42709/43776)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (42835/43904)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (42960/44032)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.561% (43083/44160)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.561% (43208/44288)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.559% (43332/44416)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (43458/44544)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (43583/44672)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (43709/44800)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.569% (43836/44928)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (43961/45056)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (44084/45184)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (44209/45312)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (44336/45440)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (44461/45568)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.564% (44583/45696)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (44710/45824)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (44835/45952)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.567% (44959/46080)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (45085/46208)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.574% (45212/46336)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.570% (45335/46464)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.573% (45461/46592)\n",
            "Train Epoch: 142 | Loss: 0.073 | Acc: 97.577% (45588/46720)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.577% (45713/46848)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.580% (45839/46976)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.578% (45963/47104)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.576% (46087/47232)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.578% (46213/47360)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.578% (46338/47488)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.579% (46463/47616)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.572% (46585/47744)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.571% (46709/47872)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (46833/48000)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.563% (46955/48128)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.569% (47083/48256)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (47206/48384)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (47331/48512)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (47454/48640)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.560% (47578/48768)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.558% (47702/48896)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.558% (47827/49024)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.559% (47952/49152)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.563% (48079/49280)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (48205/49408)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.565% (48330/49536)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.564% (48454/49664)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (48578/49792)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.562% (48703/49920)\n",
            "Train Epoch: 142 | Loss: 0.074 | Acc: 97.566% (48783/50000)\n",
            "Test Epoch: 142 | Loss: 0.259 | Acc: 93.000% (93/100)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 93.000% (186/200)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 92.000% (276/300)\n",
            "Test Epoch: 142 | Loss: 0.293 | Acc: 92.250% (369/400)\n",
            "Test Epoch: 142 | Loss: 0.275 | Acc: 92.600% (463/500)\n",
            "Test Epoch: 142 | Loss: 0.244 | Acc: 93.000% (558/600)\n",
            "Test Epoch: 142 | Loss: 0.254 | Acc: 92.286% (646/700)\n",
            "Test Epoch: 142 | Loss: 0.291 | Acc: 91.625% (733/800)\n",
            "Test Epoch: 142 | Loss: 0.302 | Acc: 91.556% (824/900)\n",
            "Test Epoch: 142 | Loss: 0.300 | Acc: 91.800% (918/1000)\n",
            "Test Epoch: 142 | Loss: 0.305 | Acc: 91.727% (1009/1100)\n",
            "Test Epoch: 142 | Loss: 0.313 | Acc: 91.750% (1101/1200)\n",
            "Test Epoch: 142 | Loss: 0.300 | Acc: 91.923% (1195/1300)\n",
            "Test Epoch: 142 | Loss: 0.287 | Acc: 92.071% (1289/1400)\n",
            "Test Epoch: 142 | Loss: 0.279 | Acc: 92.133% (1382/1500)\n",
            "Test Epoch: 142 | Loss: 0.273 | Acc: 92.375% (1478/1600)\n",
            "Test Epoch: 142 | Loss: 0.272 | Acc: 92.412% (1571/1700)\n",
            "Test Epoch: 142 | Loss: 0.276 | Acc: 92.167% (1659/1800)\n",
            "Test Epoch: 142 | Loss: 0.279 | Acc: 92.158% (1751/1900)\n",
            "Test Epoch: 142 | Loss: 0.287 | Acc: 92.150% (1843/2000)\n",
            "Test Epoch: 142 | Loss: 0.285 | Acc: 91.952% (1931/2100)\n",
            "Test Epoch: 142 | Loss: 0.285 | Acc: 91.864% (2021/2200)\n",
            "Test Epoch: 142 | Loss: 0.296 | Acc: 91.696% (2109/2300)\n",
            "Test Epoch: 142 | Loss: 0.297 | Acc: 91.750% (2202/2400)\n",
            "Test Epoch: 142 | Loss: 0.309 | Acc: 91.680% (2292/2500)\n",
            "Test Epoch: 142 | Loss: 0.320 | Acc: 91.538% (2380/2600)\n",
            "Test Epoch: 142 | Loss: 0.315 | Acc: 91.704% (2476/2700)\n",
            "Test Epoch: 142 | Loss: 0.318 | Acc: 91.714% (2568/2800)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 91.655% (2658/2900)\n",
            "Test Epoch: 142 | Loss: 0.317 | Acc: 91.733% (2752/3000)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.548% (2838/3100)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.531% (2929/3200)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.545% (3021/3300)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.529% (3112/3400)\n",
            "Test Epoch: 142 | Loss: 0.328 | Acc: 91.429% (3200/3500)\n",
            "Test Epoch: 142 | Loss: 0.330 | Acc: 91.444% (3292/3600)\n",
            "Test Epoch: 142 | Loss: 0.334 | Acc: 91.351% (3380/3700)\n",
            "Test Epoch: 142 | Loss: 0.335 | Acc: 91.342% (3471/3800)\n",
            "Test Epoch: 142 | Loss: 0.329 | Acc: 91.462% (3567/3900)\n",
            "Test Epoch: 142 | Loss: 0.327 | Acc: 91.475% (3659/4000)\n",
            "Test Epoch: 142 | Loss: 0.328 | Acc: 91.488% (3751/4100)\n",
            "Test Epoch: 142 | Loss: 0.330 | Acc: 91.452% (3841/4200)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.628% (3940/4300)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.614% (4031/4400)\n",
            "Test Epoch: 142 | Loss: 0.320 | Acc: 91.667% (4125/4500)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.587% (4213/4600)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 91.660% (4308/4700)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.625% (4398/4800)\n",
            "Test Epoch: 142 | Loss: 0.317 | Acc: 91.714% (4494/4900)\n",
            "Test Epoch: 142 | Loss: 0.327 | Acc: 91.620% (4581/5000)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.686% (4676/5100)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.654% (4766/5200)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.623% (4856/5300)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.648% (4949/5400)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.545% (5035/5500)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.554% (5127/5600)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.509% (5216/5700)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.517% (5308/5800)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.475% (5397/5900)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.517% (5491/6000)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.508% (5582/6100)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.500% (5673/6200)\n",
            "Test Epoch: 142 | Loss: 0.326 | Acc: 91.492% (5764/6300)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.531% (5858/6400)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.523% (5949/6500)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.530% (6041/6600)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.552% (6134/6700)\n",
            "Test Epoch: 142 | Loss: 0.326 | Acc: 91.471% (6220/6800)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.493% (6313/6900)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.529% (6407/7000)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.507% (6497/7100)\n",
            "Test Epoch: 142 | Loss: 0.322 | Acc: 91.528% (6590/7200)\n",
            "Test Epoch: 142 | Loss: 0.320 | Acc: 91.589% (6686/7300)\n",
            "Test Epoch: 142 | Loss: 0.317 | Acc: 91.649% (6782/7400)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 91.627% (6872/7500)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.618% (6963/7600)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.636% (7056/7700)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.654% (7149/7800)\n",
            "Test Epoch: 142 | Loss: 0.324 | Acc: 91.646% (7240/7900)\n",
            "Test Epoch: 142 | Loss: 0.323 | Acc: 91.662% (7333/8000)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.716% (7429/8100)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.707% (7520/8200)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.711% (7612/8300)\n",
            "Test Epoch: 142 | Loss: 0.320 | Acc: 91.750% (7707/8400)\n",
            "Test Epoch: 142 | Loss: 0.320 | Acc: 91.741% (7798/8500)\n",
            "Test Epoch: 142 | Loss: 0.321 | Acc: 91.709% (7887/8600)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 91.747% (7982/8700)\n",
            "Test Epoch: 142 | Loss: 0.318 | Acc: 91.773% (8076/8800)\n",
            "Test Epoch: 142 | Loss: 0.318 | Acc: 91.798% (8170/8900)\n",
            "Test Epoch: 142 | Loss: 0.318 | Acc: 91.811% (8263/9000)\n",
            "Test Epoch: 142 | Loss: 0.317 | Acc: 91.835% (8357/9100)\n",
            "Test Epoch: 142 | Loss: 0.316 | Acc: 91.859% (8451/9200)\n",
            "Test Epoch: 142 | Loss: 0.316 | Acc: 91.871% (8544/9300)\n",
            "Test Epoch: 142 | Loss: 0.315 | Acc: 91.915% (8640/9400)\n",
            "Test Epoch: 142 | Loss: 0.316 | Acc: 91.895% (8730/9500)\n",
            "Test Epoch: 142 | Loss: 0.316 | Acc: 91.896% (8822/9600)\n",
            "Test Epoch: 142 | Loss: 0.314 | Acc: 91.959% (8920/9700)\n",
            "Test Epoch: 142 | Loss: 0.314 | Acc: 91.980% (9014/9800)\n",
            "Test Epoch: 142 | Loss: 0.316 | Acc: 91.939% (9102/9900)\n",
            "Test Epoch: 142 | Loss: 0.319 | Acc: 91.940% (9194/10000)\n",
            "\n",
            "Epoch: 143\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 96.875% (124/128)\n",
            "Train Epoch: 143 | Loss: 0.084 | Acc: 97.266% (249/256)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.656% (375/384)\n",
            "Train Epoch: 143 | Loss: 0.060 | Acc: 98.047% (502/512)\n",
            "Train Epoch: 143 | Loss: 0.065 | Acc: 97.812% (626/640)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.656% (750/768)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.321% (872/896)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.363% (997/1024)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.569% (1124/1152)\n",
            "Train Epoch: 143 | Loss: 0.075 | Acc: 97.422% (1247/1280)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.372% (1371/1408)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.461% (1497/1536)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.416% (1621/1664)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.377% (1745/1792)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.448% (1871/1920)\n",
            "Train Epoch: 143 | Loss: 0.075 | Acc: 97.412% (1995/2048)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.472% (2121/2176)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.439% (2245/2304)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.533% (2372/2432)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.539% (2497/2560)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.582% (2623/2688)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.656% (2750/2816)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.656% (2875/2944)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.624% (2999/3072)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.688% (3126/3200)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.716% (3252/3328)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.656% (3375/3456)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.712% (3502/3584)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.764% (3629/3712)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.812% (3756/3840)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.807% (3881/3968)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.827% (4007/4096)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.727% (4128/4224)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.771% (4255/4352)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.723% (4378/4480)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.765% (4505/4608)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.804% (4632/4736)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.841% (4759/4864)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.817% (4883/4992)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.832% (5009/5120)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.866% (5136/5248)\n",
            "Train Epoch: 143 | Loss: 0.067 | Acc: 97.879% (5262/5376)\n",
            "Train Epoch: 143 | Loss: 0.067 | Acc: 97.892% (5388/5504)\n",
            "Train Epoch: 143 | Loss: 0.067 | Acc: 97.869% (5512/5632)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.847% (5636/5760)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.877% (5763/5888)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.872% (5888/6016)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.819% (6010/6144)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.864% (6138/6272)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.844% (6262/6400)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.825% (6386/6528)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.852% (6513/6656)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.848% (6638/6784)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.888% (6766/6912)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.898% (6892/7040)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.879% (7016/7168)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.848% (7139/7296)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.845% (7264/7424)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.842% (7389/7552)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.839% (7514/7680)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.861% (7641/7808)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.833% (7764/7936)\n",
            "Train Epoch: 143 | Loss: 0.068 | Acc: 97.830% (7889/8064)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.815% (8013/8192)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.800% (8137/8320)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.786% (8261/8448)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.796% (8387/8576)\n",
            "Train Epoch: 143 | Loss: 0.069 | Acc: 97.794% (8512/8704)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.769% (8635/8832)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.757% (8759/8960)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.755% (8884/9088)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.754% (9009/9216)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.774% (9136/9344)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.783% (9262/9472)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.792% (9388/9600)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.800% (9514/9728)\n",
            "Train Epoch: 143 | Loss: 0.070 | Acc: 97.788% (9638/9856)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.756% (9760/9984)\n",
            "Train Epoch: 143 | Loss: 0.071 | Acc: 97.765% (9886/10112)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.754% (10010/10240)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.743% (10134/10368)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.742% (10259/10496)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.732% (10383/10624)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.740% (10509/10752)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.748% (10635/10880)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.747% (10760/11008)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.710% (10881/11136)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.701% (11005/11264)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.709% (11131/11392)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.691% (11254/11520)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.699% (11380/11648)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.699% (11505/11776)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.707% (11631/11904)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.706% (11756/12032)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.697% (11880/12160)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.697% (12005/12288)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.705% (12131/12416)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.712% (12257/12544)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.696% (12380/12672)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.703% (12506/12800)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.703% (12631/12928)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.702% (12756/13056)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.694% (12880/13184)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.716% (13008/13312)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.716% (13133/13440)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.723% (13259/13568)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.729% (13385/13696)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.714% (13508/13824)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.714% (13633/13952)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.699% (13756/14080)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.670% (13877/14208)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.670% (14002/14336)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.663% (14126/14464)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.677% (14253/14592)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.670% (14377/14720)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.676% (14503/14848)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.663% (14626/14976)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.663% (14751/15104)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.676% (14878/15232)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.676% (15003/15360)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.682% (15129/15488)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.682% (15254/15616)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.682% (15379/15744)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.700% (15507/15872)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.700% (15632/16000)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.706% (15758/16128)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.699% (15882/16256)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.687% (16005/16384)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.687% (16130/16512)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.698% (16257/16640)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.680% (16379/16768)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.668% (16502/16896)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.680% (16629/17024)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.674% (16753/17152)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.674% (16878/17280)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (17000/17408)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (17125/17536)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.662% (17251/17664)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.651% (17374/17792)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.667% (17502/17920)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.662% (17626/18048)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.673% (17753/18176)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.673% (17878/18304)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.673% (18003/18432)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.672% (18128/18560)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.672% (18253/18688)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.662% (18376/18816)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (18500/18944)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.667% (18627/19072)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.672% (18753/19200)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.682% (18880/19328)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.682% (19005/19456)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.682% (19130/19584)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.666% (19252/19712)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.671% (19378/19840)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.661% (19501/19968)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.661% (19626/20096)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.676% (19754/20224)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.681% (19880/20352)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.681% (20005/20480)\n",
            "Train Epoch: 143 | Loss: 0.072 | Acc: 97.685% (20131/20608)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.671% (20253/20736)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.671% (20378/20864)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (20500/20992)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (20625/21120)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.652% (20749/21248)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.652% (20874/21376)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.656% (21000/21504)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.647% (21123/21632)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.633% (21245/21760)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.629% (21369/21888)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.624% (21493/22016)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.638% (21621/22144)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.634% (21745/22272)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.616% (21866/22400)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.621% (21992/22528)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.634% (22120/22656)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.643% (22247/22784)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (22373/22912)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.652% (22499/23040)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.643% (22622/23168)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.639% (22746/23296)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.643% (22872/23424)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (22998/23552)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.644% (23122/23680)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.639% (23246/23808)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.631% (23369/23936)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.631% (23494/24064)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.636% (23620/24192)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.640% (23746/24320)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.636% (23870/24448)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.636% (23995/24576)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (24123/24704)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (24248/24832)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (24373/24960)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.648% (24498/25088)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.636% (24620/25216)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.644% (24747/25344)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.637% (24870/25472)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.637% (24995/25600)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.633% (25119/25728)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.621% (25241/25856)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.629% (25368/25984)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.637% (25495/26112)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.622% (25616/26240)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.618% (25740/26368)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.619% (25865/26496)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.619% (25990/26624)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.611% (26113/26752)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.615% (26239/26880)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.616% (26364/27008)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.619% (26490/27136)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.616% (26614/27264)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.612% (26738/27392)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.613% (26863/27520)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.620% (26990/27648)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.620% (27115/27776)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.631% (27243/27904)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.631% (27368/28032)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.635% (27494/28160)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.642% (27621/28288)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.639% (27745/28416)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.642% (27871/28544)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.646% (27997/28672)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.642% (28121/28800)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.642% (28246/28928)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.642% (28371/29056)\n",
            "Train Epoch: 143 | Loss: 0.073 | Acc: 97.632% (28493/29184)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.619% (28614/29312)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.615% (28738/29440)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (28859/29568)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.599% (28983/29696)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (29110/29824)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.616% (29238/29952)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.620% (29364/30080)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.610% (29486/30208)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.600% (29608/30336)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.604% (29734/30464)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (29860/30592)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (29985/30720)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.611% (30111/30848)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.618% (30238/30976)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.614% (30362/31104)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.615% (30487/31232)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.615% (30612/31360)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.612% (30736/31488)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (30858/31616)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (30984/31744)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.609% (31110/31872)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.612% (31236/32000)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (31359/32128)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (31484/32256)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.616% (31612/32384)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.613% (31736/32512)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.616% (31862/32640)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.617% (31987/32768)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.614% (32111/32896)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (32234/33024)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (32357/33152)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.599% (32481/33280)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (32607/33408)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (32732/33536)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (32857/33664)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (32982/33792)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (33108/33920)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (33232/34048)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.604% (33357/34176)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (33481/34304)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (33606/34432)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.604% (33732/34560)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.599% (33855/34688)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (33981/34816)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (34106/34944)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (34233/35072)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (34358/35200)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.605% (34482/35328)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.597% (34604/35456)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.597% (34729/35584)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.600% (34855/35712)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.592% (34977/35840)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.590% (35101/35968)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (35224/36096)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.587% (35350/36224)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.585% (35474/36352)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.579% (35597/36480)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.580% (35722/36608)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.588% (35850/36736)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.586% (35974/36864)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.578% (36096/36992)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (36223/37120)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (36348/37248)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.587% (36474/37376)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.582% (36597/37504)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.582% (36722/37632)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.585% (36848/37760)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.585% (36973/37888)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.590% (37100/38016)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.593% (37226/38144)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.586% (37348/38272)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.583% (37472/38400)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.591% (37600/38528)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.594% (37726/38656)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.597% (37852/38784)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.600% (37978/38912)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.600% (38103/39040)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.600% (38228/39168)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (38354/39296)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (38481/39424)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (38606/39552)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (38730/39680)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (38853/39808)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (38978/39936)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (39105/40064)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (39230/40192)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (39355/40320)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.609% (39481/40448)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (39605/40576)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.612% (39732/40704)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.610% (39856/40832)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.610% (39981/40960)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.610% (40106/41088)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.610% (40231/41216)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.613% (40357/41344)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (40479/41472)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (40604/41600)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.606% (40729/41728)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.608% (40855/41856)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.611% (40981/41984)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.609% (41105/42112)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.604% (41228/42240)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.602% (41352/42368)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.604% (41478/42496)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.609% (41605/42624)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.612% (41731/42752)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.607% (41854/42880)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (41977/43008)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (42102/43136)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (42227/43264)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.603% (42352/43392)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.599% (42475/43520)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (42601/43648)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.601% (42726/43776)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.599% (42850/43904)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.595% (42973/44032)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.595% (43098/44160)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.593% (43222/44288)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.595% (43348/44416)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.598% (43474/44544)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.591% (43596/44672)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.594% (43722/44800)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.594% (43847/44928)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.590% (43970/45056)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.588% (44094/45184)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.586% (44218/45312)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (44342/45440)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.586% (44468/45568)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.588% (44594/45696)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.593% (44721/45824)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.595% (44847/45952)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.595% (44972/46080)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.591% (45095/46208)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.587% (45218/46336)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.583% (45341/46464)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.583% (45466/46592)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.577% (45588/46720)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.575% (45712/46848)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.573% (45836/46976)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.580% (45964/47104)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.582% (46090/47232)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.580% (46214/47360)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.576% (46337/47488)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.572% (46460/47616)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.577% (46587/47744)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.581% (46714/47872)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.579% (46838/48000)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.586% (46966/48128)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.590% (47093/48256)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.588% (47217/48384)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.588% (47342/48512)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (47465/48640)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.584% (47590/48768)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.587% (47716/48896)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.583% (47839/49024)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.585% (47965/49152)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.589% (48092/49280)\n",
            "Train Epoch: 143 | Loss: 0.075 | Acc: 97.581% (48213/49408)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.582% (48338/49536)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.582% (48463/49664)\n",
            "Train Epoch: 143 | Loss: 0.074 | Acc: 97.580% (48587/49792)\n",
            "Train Epoch: 143 | Loss: 0.075 | Acc: 97.578% (48711/49920)\n",
            "Train Epoch: 143 | Loss: 0.075 | Acc: 97.578% (48789/50000)\n",
            "Test Epoch: 143 | Loss: 0.250 | Acc: 94.000% (94/100)\n",
            "Test Epoch: 143 | Loss: 0.319 | Acc: 93.000% (186/200)\n",
            "Test Epoch: 143 | Loss: 0.333 | Acc: 92.333% (277/300)\n",
            "Test Epoch: 143 | Loss: 0.311 | Acc: 92.750% (371/400)\n",
            "Test Epoch: 143 | Loss: 0.272 | Acc: 93.200% (466/500)\n",
            "Test Epoch: 143 | Loss: 0.254 | Acc: 93.167% (559/600)\n",
            "Test Epoch: 143 | Loss: 0.270 | Acc: 92.714% (649/700)\n",
            "Test Epoch: 143 | Loss: 0.299 | Acc: 91.500% (732/800)\n",
            "Test Epoch: 143 | Loss: 0.299 | Acc: 91.556% (824/900)\n",
            "Test Epoch: 143 | Loss: 0.297 | Acc: 91.700% (917/1000)\n",
            "Test Epoch: 143 | Loss: 0.295 | Acc: 91.727% (1009/1100)\n",
            "Test Epoch: 143 | Loss: 0.313 | Acc: 91.417% (1097/1200)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 91.462% (1189/1300)\n",
            "Test Epoch: 143 | Loss: 0.289 | Acc: 91.786% (1285/1400)\n",
            "Test Epoch: 143 | Loss: 0.281 | Acc: 91.933% (1379/1500)\n",
            "Test Epoch: 143 | Loss: 0.274 | Acc: 92.188% (1475/1600)\n",
            "Test Epoch: 143 | Loss: 0.273 | Acc: 92.353% (1570/1700)\n",
            "Test Epoch: 143 | Loss: 0.271 | Acc: 92.444% (1664/1800)\n",
            "Test Epoch: 143 | Loss: 0.275 | Acc: 92.421% (1756/1900)\n",
            "Test Epoch: 143 | Loss: 0.280 | Acc: 92.500% (1850/2000)\n",
            "Test Epoch: 143 | Loss: 0.278 | Acc: 92.476% (1942/2100)\n",
            "Test Epoch: 143 | Loss: 0.279 | Acc: 92.364% (2032/2200)\n",
            "Test Epoch: 143 | Loss: 0.287 | Acc: 92.130% (2119/2300)\n",
            "Test Epoch: 143 | Loss: 0.287 | Acc: 92.208% (2213/2400)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.040% (2301/2500)\n",
            "Test Epoch: 143 | Loss: 0.313 | Acc: 91.846% (2388/2600)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.000% (2484/2700)\n",
            "Test Epoch: 143 | Loss: 0.307 | Acc: 92.000% (2576/2800)\n",
            "Test Epoch: 143 | Loss: 0.308 | Acc: 92.000% (2668/2900)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.100% (2763/3000)\n",
            "Test Epoch: 143 | Loss: 0.310 | Acc: 91.903% (2849/3100)\n",
            "Test Epoch: 143 | Loss: 0.309 | Acc: 91.781% (2937/3200)\n",
            "Test Epoch: 143 | Loss: 0.307 | Acc: 91.818% (3030/3300)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 91.794% (3121/3400)\n",
            "Test Epoch: 143 | Loss: 0.307 | Acc: 91.800% (3213/3500)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 91.889% (3308/3600)\n",
            "Test Epoch: 143 | Loss: 0.311 | Acc: 91.811% (3397/3700)\n",
            "Test Epoch: 143 | Loss: 0.312 | Acc: 91.789% (3488/3800)\n",
            "Test Epoch: 143 | Loss: 0.310 | Acc: 91.872% (3583/3900)\n",
            "Test Epoch: 143 | Loss: 0.307 | Acc: 91.925% (3677/4000)\n",
            "Test Epoch: 143 | Loss: 0.309 | Acc: 91.951% (3770/4100)\n",
            "Test Epoch: 143 | Loss: 0.311 | Acc: 91.952% (3862/4200)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.093% (3960/4300)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.114% (4053/4400)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.111% (4145/4500)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.043% (4234/4600)\n",
            "Test Epoch: 143 | Loss: 0.299 | Acc: 92.170% (4332/4700)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.104% (4421/4800)\n",
            "Test Epoch: 143 | Loss: 0.298 | Acc: 92.163% (4516/4900)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.100% (4605/5000)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.196% (4702/5100)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.135% (4791/5200)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.113% (4882/5300)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.111% (4974/5400)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.055% (5063/5500)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.089% (5157/5600)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.088% (5249/5700)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.086% (5341/5800)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.034% (5430/5900)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.033% (5522/6000)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.066% (5616/6100)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.048% (5707/6200)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.079% (5801/6300)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.109% (5895/6400)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.108% (5987/6500)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.136% (6081/6600)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.149% (6174/6700)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.074% (6261/6800)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.116% (6356/6900)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.086% (6446/7000)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.070% (6537/7100)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.069% (6629/7200)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.082% (6722/7300)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.122% (6817/7400)\n",
            "Test Epoch: 143 | Loss: 0.302 | Acc: 92.093% (6907/7500)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.053% (6996/7600)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 92.026% (7086/7700)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.090% (7183/7800)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 92.063% (7273/7900)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 92.013% (7361/8000)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.049% (7456/8100)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.049% (7548/8200)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.036% (7639/8300)\n",
            "Test Epoch: 143 | Loss: 0.305 | Acc: 92.036% (7731/8400)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.024% (7822/8500)\n",
            "Test Epoch: 143 | Loss: 0.306 | Acc: 91.942% (7907/8600)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 91.966% (8001/8700)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 91.977% (8094/8800)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 91.989% (8187/8900)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.000% (8280/9000)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.011% (8373/9100)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.033% (8467/9200)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.022% (8558/9300)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.043% (8652/9400)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.021% (8742/9500)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.000% (8832/9600)\n",
            "Test Epoch: 143 | Loss: 0.301 | Acc: 92.041% (8928/9700)\n",
            "Test Epoch: 143 | Loss: 0.300 | Acc: 92.082% (9024/9800)\n",
            "Test Epoch: 143 | Loss: 0.303 | Acc: 92.040% (9112/9900)\n",
            "Test Epoch: 143 | Loss: 0.304 | Acc: 92.020% (9202/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 144\n",
            "Train Epoch: 144 | Loss: 0.096 | Acc: 96.875% (124/128)\n",
            "Train Epoch: 144 | Loss: 0.067 | Acc: 98.047% (251/256)\n",
            "Train Epoch: 144 | Loss: 0.065 | Acc: 98.177% (377/384)\n",
            "Train Epoch: 144 | Loss: 0.061 | Acc: 98.438% (504/512)\n",
            "Train Epoch: 144 | Loss: 0.063 | Acc: 98.281% (629/640)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.917% (752/768)\n",
            "Train Epoch: 144 | Loss: 0.067 | Acc: 97.879% (877/896)\n",
            "Train Epoch: 144 | Loss: 0.063 | Acc: 98.047% (1004/1024)\n",
            "Train Epoch: 144 | Loss: 0.064 | Acc: 97.917% (1128/1152)\n",
            "Train Epoch: 144 | Loss: 0.064 | Acc: 97.891% (1253/1280)\n",
            "Train Epoch: 144 | Loss: 0.064 | Acc: 97.798% (1377/1408)\n",
            "Train Epoch: 144 | Loss: 0.064 | Acc: 97.786% (1502/1536)\n",
            "Train Epoch: 144 | Loss: 0.067 | Acc: 97.716% (1626/1664)\n",
            "Train Epoch: 144 | Loss: 0.065 | Acc: 97.712% (1751/1792)\n",
            "Train Epoch: 144 | Loss: 0.066 | Acc: 97.760% (1877/1920)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.607% (1999/2048)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.518% (2122/2176)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.439% (2245/2304)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.451% (2370/2432)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.422% (2494/2560)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.284% (2615/2688)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.372% (2742/2816)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.351% (2866/2944)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.396% (2992/3072)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.406% (3117/3200)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.476% (3244/3328)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.454% (3368/3456)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.461% (3493/3584)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.495% (3619/3712)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.526% (3745/3840)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.530% (3870/3968)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.534% (3995/4096)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.443% (4116/4224)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.403% (4239/4352)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.411% (4364/4480)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.396% (4488/4608)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.403% (4613/4736)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.410% (4738/4864)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.416% (4863/4992)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.363% (4985/5120)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.351% (5109/5248)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.321% (5232/5376)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.366% (5359/5504)\n",
            "Train Epoch: 144 | Loss: 0.075 | Acc: 97.337% (5482/5632)\n",
            "Train Epoch: 144 | Loss: 0.075 | Acc: 97.344% (5607/5760)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.368% (5733/5888)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.374% (5858/6016)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.396% (5984/6144)\n",
            "Train Epoch: 144 | Loss: 0.075 | Acc: 97.337% (6105/6272)\n",
            "Train Epoch: 144 | Loss: 0.075 | Acc: 97.312% (6228/6400)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.319% (6353/6528)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.341% (6479/6656)\n",
            "Train Epoch: 144 | Loss: 0.075 | Acc: 97.317% (6602/6784)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.367% (6730/6912)\n",
            "Train Epoch: 144 | Loss: 0.074 | Acc: 97.358% (6854/7040)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.363% (6979/7168)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.382% (7105/7296)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.414% (7232/7424)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.405% (7356/7552)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.370% (7478/7680)\n",
            "Train Epoch: 144 | Loss: 0.073 | Acc: 97.387% (7604/7808)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.417% (7731/7936)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.445% (7858/8064)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.437% (7982/8192)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.428% (8106/8320)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.443% (8232/8448)\n",
            "Train Epoch: 144 | Loss: 0.072 | Acc: 97.470% (8359/8576)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.495% (8486/8704)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.509% (8612/8832)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.545% (8740/8960)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.535% (8864/9088)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.537% (8989/9216)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.517% (9112/9344)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.540% (9239/9472)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.562% (9366/9600)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.574% (9492/9728)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.585% (9618/9856)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.586% (9743/9984)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.577% (9867/10112)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.607% (9995/10240)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.618% (10121/10368)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.628% (10247/10496)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.647% (10374/10624)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.628% (10497/10752)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.610% (10620/10880)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.629% (10747/11008)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.656% (10875/11136)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.630% (10997/11264)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.630% (11122/11392)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.639% (11248/11520)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.656% (11375/11648)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.673% (11502/11776)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.681% (11628/11904)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.681% (11753/12032)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.681% (11878/12160)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.673% (12002/12288)\n",
            "Train Epoch: 144 | Loss: 0.068 | Acc: 97.648% (12124/12416)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.640% (12248/12544)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.593% (12367/12672)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.586% (12491/12800)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.587% (12616/12928)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.595% (12742/13056)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.580% (12865/13184)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.574% (12989/13312)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.560% (13112/13440)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.553% (13236/13568)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.554% (13361/13696)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.577% (13489/13824)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.585% (13615/13952)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.607% (13743/14080)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.614% (13869/14208)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.573% (13988/14336)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.587% (14115/14464)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.588% (14240/14592)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.582% (14364/14720)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.555% (14485/14848)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.556% (14610/14976)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.530% (14731/15104)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.545% (14858/15232)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.526% (14980/15360)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.514% (15103/15488)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.528% (15230/15616)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.529% (15355/15744)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.530% (15480/15872)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.506% (15601/16000)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.501% (15725/16128)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.509% (15851/16256)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.504% (15975/16384)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.505% (16100/16512)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.512% (16226/16640)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.525% (16353/16768)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.538% (16480/16896)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.539% (16605/17024)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.551% (16732/17152)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.541% (16855/17280)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.547% (16981/17408)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.554% (17107/17536)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.560% (17233/17664)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.572% (17360/17792)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.584% (17487/17920)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.568% (17609/18048)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.579% (17736/18176)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.580% (17861/18304)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.580% (17986/18432)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.586% (18112/18560)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.597% (18239/18688)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.577% (18360/18816)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.588% (18487/18944)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.583% (18611/19072)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.583% (18736/19200)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.584% (18861/19328)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.589% (18987/19456)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.595% (19113/19584)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.606% (19240/19712)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.616% (19367/19840)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.616% (19492/19968)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.621% (19618/20096)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.622% (19743/20224)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.622% (19868/20352)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.607% (19990/20480)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.613% (20116/20608)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.618% (20242/20736)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.613% (20366/20864)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.613% (20491/20992)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.623% (20618/21120)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.614% (20741/21248)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.600% (20863/21376)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.596% (20987/21504)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.601% (21113/21632)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.615% (21241/21760)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.611% (21365/21888)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.620% (21492/22016)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.634% (21620/22144)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.634% (21745/22272)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.621% (21867/22400)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.621% (21992/22528)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.608% (22114/22656)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.608% (22239/22784)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.608% (22364/22912)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.617% (22491/23040)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.613% (22615/23168)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.622% (22742/23296)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.631% (22869/23424)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.639% (22996/23552)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.648% (23123/23680)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.652% (23249/23808)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.644% (23372/23936)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.652% (23499/24064)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.644% (23622/24192)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.648% (23748/24320)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.656% (23875/24448)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.660% (24001/24576)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.668% (24128/24704)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.676% (24255/24832)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.672% (24379/24960)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.676% (24505/25088)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.672% (24629/25216)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.672% (24754/25344)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.672% (24879/25472)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.684% (25007/25600)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.687% (25133/25728)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (25259/25856)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (25384/25984)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.683% (25507/26112)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.687% (25633/26240)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.690% (25759/26368)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (25886/26496)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (26011/26624)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.701% (26137/26752)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.701% (26262/26880)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.701% (26387/27008)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.708% (26514/27136)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.700% (26637/27264)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.689% (26759/27392)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.685% (26883/27520)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.685% (27008/27648)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.692% (27135/27776)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.685% (27258/27904)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.660% (27376/28032)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.670% (27504/28160)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.660% (27626/28288)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.670% (27754/28416)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.674% (27880/28544)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.670% (28004/28672)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.667% (28128/28800)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.674% (28255/28928)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.673% (28380/29056)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.684% (28508/29184)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.684% (28633/29312)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.680% (28757/29440)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.677% (28881/29568)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.687% (29009/29696)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.693% (29136/29824)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.693% (29261/29952)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.703% (29389/30080)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.709% (29516/30208)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.709% (29641/30336)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.705% (29765/30464)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.702% (29889/30592)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.695% (30012/30720)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (30138/30848)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (30263/30976)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.695% (30387/31104)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (30511/31232)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.695% (30637/31360)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (30761/31488)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.697% (30888/31616)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (31011/31744)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.688% (31135/31872)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (31261/32000)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.681% (31383/32128)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.675% (31506/32256)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.669% (31629/32384)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.675% (31756/32512)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.669% (31879/32640)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.668% (32004/32768)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.659% (32126/32896)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (32248/33024)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (32375/33152)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.662% (32502/33280)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (32625/33408)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.665% (32753/33536)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.671% (32880/33664)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.668% (33004/33792)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.671% (33130/33920)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.665% (33253/34048)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (33375/34176)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (33498/34304)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.653% (33624/34432)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.653% (33749/34560)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.653% (33874/34688)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (34000/34816)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.662% (34127/34944)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.668% (34254/35072)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.673% (34381/35200)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.676% (34507/35328)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.682% (34634/35456)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.682% (34759/35584)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (34884/35712)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (35009/35840)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.684% (35135/35968)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (35259/36096)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.678% (35383/36224)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (35509/36352)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.675% (35632/36480)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.673% (35756/36608)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.678% (35883/36736)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.675% (36007/36864)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.678% (36133/36992)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.686% (36261/37120)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (36388/37248)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (36513/37376)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.691% (36638/37504)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.688% (36762/37632)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.688% (36887/37760)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.693% (37014/37888)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.685% (37136/38016)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.688% (37262/38144)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.685% (37386/38272)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.685% (37511/38400)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.685% (37636/38528)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.685% (37761/38656)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.692% (37889/38784)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.700% (38017/38912)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.700% (38142/39040)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.700% (38267/39168)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.700% (38392/39296)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.704% (38519/39424)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.699% (38642/39552)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.699% (38767/39680)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.701% (38893/39808)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.701% (39018/39936)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.701% (39143/40064)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.694% (39265/40192)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.696% (39391/40320)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.696% (39516/40448)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.696% (39641/40576)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (39767/40704)\n",
            "Train Epoch: 144 | Loss: 0.069 | Acc: 97.698% (39892/40832)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.693% (40015/40960)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.688% (40138/41088)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.683% (40261/41216)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.690% (40389/41344)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.690% (40514/41472)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.690% (40639/41600)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.692% (40765/41728)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.690% (40889/41856)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.690% (41014/41984)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.689% (41139/42112)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.689% (41264/42240)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.687% (41388/42368)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.684% (41512/42496)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.684% (41637/42624)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.689% (41764/42752)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.691% (41890/42880)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.691% (42015/43008)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.686% (42138/43136)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.684% (42262/43264)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.686% (42388/43392)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.691% (42515/43520)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.684% (42637/43648)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.686% (42763/43776)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.688% (42889/43904)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (43011/44032)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (43136/44160)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.681% (43261/44288)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.683% (43387/44416)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.688% (43514/44544)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.676% (43634/44672)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.676% (43759/44800)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.679% (43885/44928)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.683% (44012/45056)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.678% (44135/45184)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.678% (44260/45312)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.672% (44382/45440)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.674% (44508/45568)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.674% (44633/45696)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.674% (44758/45824)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.658% (44876/45952)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (45000/46080)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (45122/46208)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (45248/46336)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.658% (45376/46464)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (45500/46592)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (45622/46720)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.643% (45744/46848)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.646% (45870/46976)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (45997/47104)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (46123/47232)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (46250/47360)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.654% (46374/47488)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.658% (46501/47616)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (46623/47744)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (46747/47872)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.656% (46875/48000)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (46998/48128)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (47123/48256)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.654% (47249/48384)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.654% (47374/48512)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.652% (47498/48640)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.646% (47620/48768)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.648% (47746/48896)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.650% (47872/49024)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.644% (47994/49152)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.644% (48119/49280)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.634% (48239/49408)\n",
            "Train Epoch: 144 | Loss: 0.071 | Acc: 97.632% (48363/49536)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.638% (48491/49664)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.636% (48615/49792)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.640% (48742/49920)\n",
            "Train Epoch: 144 | Loss: 0.070 | Acc: 97.638% (48819/50000)\n",
            "Test Epoch: 144 | Loss: 0.197 | Acc: 96.000% (96/100)\n",
            "Test Epoch: 144 | Loss: 0.273 | Acc: 94.000% (188/200)\n",
            "Test Epoch: 144 | Loss: 0.309 | Acc: 92.000% (276/300)\n",
            "Test Epoch: 144 | Loss: 0.289 | Acc: 91.750% (367/400)\n",
            "Test Epoch: 144 | Loss: 0.255 | Acc: 92.600% (463/500)\n",
            "Test Epoch: 144 | Loss: 0.240 | Acc: 92.833% (557/600)\n",
            "Test Epoch: 144 | Loss: 0.263 | Acc: 92.429% (647/700)\n",
            "Test Epoch: 144 | Loss: 0.291 | Acc: 91.875% (735/800)\n",
            "Test Epoch: 144 | Loss: 0.299 | Acc: 91.778% (826/900)\n",
            "Test Epoch: 144 | Loss: 0.299 | Acc: 91.900% (919/1000)\n",
            "Test Epoch: 144 | Loss: 0.309 | Acc: 91.455% (1006/1100)\n",
            "Test Epoch: 144 | Loss: 0.324 | Acc: 91.083% (1093/1200)\n",
            "Test Epoch: 144 | Loss: 0.320 | Acc: 91.154% (1185/1300)\n",
            "Test Epoch: 144 | Loss: 0.314 | Acc: 91.286% (1278/1400)\n",
            "Test Epoch: 144 | Loss: 0.306 | Acc: 91.533% (1373/1500)\n",
            "Test Epoch: 144 | Loss: 0.299 | Acc: 91.812% (1469/1600)\n",
            "Test Epoch: 144 | Loss: 0.299 | Acc: 91.882% (1562/1700)\n",
            "Test Epoch: 144 | Loss: 0.302 | Acc: 91.722% (1651/1800)\n",
            "Test Epoch: 144 | Loss: 0.302 | Acc: 91.684% (1742/1900)\n",
            "Test Epoch: 144 | Loss: 0.308 | Acc: 91.700% (1834/2000)\n",
            "Test Epoch: 144 | Loss: 0.303 | Acc: 91.810% (1928/2100)\n",
            "Test Epoch: 144 | Loss: 0.299 | Acc: 91.909% (2022/2200)\n",
            "Test Epoch: 144 | Loss: 0.308 | Acc: 91.739% (2110/2300)\n",
            "Test Epoch: 144 | Loss: 0.306 | Acc: 91.667% (2200/2400)\n",
            "Test Epoch: 144 | Loss: 0.321 | Acc: 91.560% (2289/2500)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.423% (2377/2600)\n",
            "Test Epoch: 144 | Loss: 0.325 | Acc: 91.556% (2472/2700)\n",
            "Test Epoch: 144 | Loss: 0.324 | Acc: 91.643% (2566/2800)\n",
            "Test Epoch: 144 | Loss: 0.326 | Acc: 91.621% (2657/2900)\n",
            "Test Epoch: 144 | Loss: 0.328 | Acc: 91.567% (2747/3000)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.387% (2833/3100)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.250% (2920/3200)\n",
            "Test Epoch: 144 | Loss: 0.340 | Acc: 91.182% (3009/3300)\n",
            "Test Epoch: 144 | Loss: 0.341 | Acc: 91.088% (3097/3400)\n",
            "Test Epoch: 144 | Loss: 0.342 | Acc: 91.029% (3186/3500)\n",
            "Test Epoch: 144 | Loss: 0.340 | Acc: 91.167% (3282/3600)\n",
            "Test Epoch: 144 | Loss: 0.344 | Acc: 91.108% (3371/3700)\n",
            "Test Epoch: 144 | Loss: 0.347 | Acc: 91.053% (3460/3800)\n",
            "Test Epoch: 144 | Loss: 0.343 | Acc: 91.154% (3555/3900)\n",
            "Test Epoch: 144 | Loss: 0.341 | Acc: 91.225% (3649/4000)\n",
            "Test Epoch: 144 | Loss: 0.347 | Acc: 91.171% (3738/4100)\n",
            "Test Epoch: 144 | Loss: 0.349 | Acc: 91.167% (3829/4200)\n",
            "Test Epoch: 144 | Loss: 0.344 | Acc: 91.279% (3925/4300)\n",
            "Test Epoch: 144 | Loss: 0.342 | Acc: 91.364% (4020/4400)\n",
            "Test Epoch: 144 | Loss: 0.339 | Acc: 91.378% (4112/4500)\n",
            "Test Epoch: 144 | Loss: 0.342 | Acc: 91.348% (4202/4600)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.426% (4297/4700)\n",
            "Test Epoch: 144 | Loss: 0.341 | Acc: 91.333% (4384/4800)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.429% (4480/4900)\n",
            "Test Epoch: 144 | Loss: 0.343 | Acc: 91.320% (4566/5000)\n",
            "Test Epoch: 144 | Loss: 0.339 | Acc: 91.412% (4662/5100)\n",
            "Test Epoch: 144 | Loss: 0.341 | Acc: 91.346% (4750/5200)\n",
            "Test Epoch: 144 | Loss: 0.339 | Acc: 91.377% (4843/5300)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.407% (4936/5400)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.455% (5030/5500)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.464% (5122/5600)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.456% (5213/5700)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.448% (5304/5800)\n",
            "Test Epoch: 144 | Loss: 0.339 | Acc: 91.390% (5392/5900)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.417% (5485/6000)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.410% (5576/6100)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.452% (5670/6200)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.492% (5764/6300)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.594% (5862/6400)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.569% (5952/6500)\n",
            "Test Epoch: 144 | Loss: 0.334 | Acc: 91.576% (6044/6600)\n",
            "Test Epoch: 144 | Loss: 0.334 | Acc: 91.567% (6135/6700)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.485% (6221/6800)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.464% (6311/6900)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.429% (6400/7000)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.408% (6490/7100)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.431% (6583/7200)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.466% (6677/7300)\n",
            "Test Epoch: 144 | Loss: 0.332 | Acc: 91.500% (6771/7400)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.480% (6861/7500)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.461% (6951/7600)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.403% (7038/7700)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.436% (7132/7800)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.392% (7220/7900)\n",
            "Test Epoch: 144 | Loss: 0.338 | Acc: 91.350% (7308/8000)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.407% (7404/8100)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.427% (7497/8200)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.398% (7586/8300)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.405% (7678/8400)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.412% (7770/8500)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.337% (7855/8600)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.379% (7950/8700)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.364% (8040/8800)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.337% (8129/8900)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.300% (8217/9000)\n",
            "Test Epoch: 144 | Loss: 0.337 | Acc: 91.308% (8309/9100)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.315% (8401/9200)\n",
            "Test Epoch: 144 | Loss: 0.335 | Acc: 91.355% (8496/9300)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.404% (8592/9400)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.400% (8683/9500)\n",
            "Test Epoch: 144 | Loss: 0.333 | Acc: 91.406% (8775/9600)\n",
            "Test Epoch: 144 | Loss: 0.332 | Acc: 91.454% (8871/9700)\n",
            "Test Epoch: 144 | Loss: 0.331 | Acc: 91.480% (8965/9800)\n",
            "Test Epoch: 144 | Loss: 0.334 | Acc: 91.434% (9052/9900)\n",
            "Test Epoch: 144 | Loss: 0.336 | Acc: 91.400% (9140/10000)\n",
            "\n",
            "Epoch: 145\n",
            "Train Epoch: 145 | Loss: 0.112 | Acc: 96.875% (124/128)\n",
            "Train Epoch: 145 | Loss: 0.083 | Acc: 98.047% (251/256)\n",
            "Train Epoch: 145 | Loss: 0.083 | Acc: 97.917% (376/384)\n",
            "Train Epoch: 145 | Loss: 0.080 | Acc: 97.852% (501/512)\n",
            "Train Epoch: 145 | Loss: 0.078 | Acc: 97.812% (626/640)\n",
            "Train Epoch: 145 | Loss: 0.082 | Acc: 97.656% (750/768)\n",
            "Train Epoch: 145 | Loss: 0.084 | Acc: 97.545% (874/896)\n",
            "Train Epoch: 145 | Loss: 0.076 | Acc: 97.852% (1002/1024)\n",
            "Train Epoch: 145 | Loss: 0.075 | Acc: 97.917% (1128/1152)\n",
            "Train Epoch: 145 | Loss: 0.074 | Acc: 97.969% (1254/1280)\n",
            "Train Epoch: 145 | Loss: 0.073 | Acc: 97.869% (1378/1408)\n",
            "Train Epoch: 145 | Loss: 0.074 | Acc: 97.917% (1504/1536)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.837% (1628/1664)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.935% (1755/1792)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.917% (1880/1920)\n",
            "Train Epoch: 145 | Loss: 0.067 | Acc: 97.998% (2007/2048)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.932% (2131/2176)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.960% (2257/2304)\n",
            "Train Epoch: 145 | Loss: 0.072 | Acc: 97.903% (2381/2432)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.930% (2507/2560)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.917% (2632/2688)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.940% (2758/2816)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.996% (2885/2944)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.982% (3010/3072)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.938% (3134/3200)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.957% (3260/3328)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.888% (3383/3456)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.852% (3507/3584)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.845% (3632/3712)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.839% (3757/3840)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.833% (3882/3968)\n",
            "Train Epoch: 145 | Loss: 0.067 | Acc: 97.852% (4008/4096)\n",
            "Train Epoch: 145 | Loss: 0.067 | Acc: 97.846% (4133/4224)\n",
            "Train Epoch: 145 | Loss: 0.066 | Acc: 97.886% (4260/4352)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.723% (4378/4480)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.700% (4502/4608)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.741% (4629/4736)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.759% (4755/4864)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.796% (4882/4992)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.793% (5007/5120)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.828% (5134/5248)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.824% (5259/5376)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.783% (5382/5504)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.781% (5507/5632)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.812% (5634/5760)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.826% (5760/5888)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.806% (5884/6016)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.803% (6009/6144)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.800% (6134/6272)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.781% (6258/6400)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.809% (6385/6528)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.837% (6512/6656)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.833% (6637/6784)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.844% (6763/6912)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.798% (6885/7040)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.782% (7009/7168)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.738% (7131/7296)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.710% (7254/7424)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.669% (7376/7552)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.669% (7501/7680)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.656% (7625/7808)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.694% (7753/7936)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.693% (7878/8064)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.681% (8002/8192)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.680% (8127/8320)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.692% (8253/8448)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.691% (8378/8576)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.702% (8504/8704)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.736% (8632/8832)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.723% (8756/8960)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.711% (8880/9088)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.732% (9007/9216)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.720% (9131/9344)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.720% (9256/9472)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.708% (9380/9600)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.718% (9506/9728)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.707% (9630/9856)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.696% (9754/9984)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.716% (9881/10112)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.705% (10005/10240)\n",
            "Train Epoch: 145 | Loss: 0.068 | Acc: 97.704% (10130/10368)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.675% (10252/10496)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.619% (10371/10624)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.610% (10495/10752)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.601% (10619/10880)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.593% (10743/11008)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.593% (10868/11136)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.576% (10991/11264)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.595% (11118/11392)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.604% (11244/11520)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.605% (11369/11648)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.605% (11494/11776)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.606% (11619/11904)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.598% (11743/12032)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.599% (11868/12160)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.591% (11992/12288)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.608% (12119/12416)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.592% (12242/12544)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.601% (12368/12672)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.609% (12494/12800)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.618% (12620/12928)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.641% (12748/13056)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.626% (12871/13184)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.619% (12995/13312)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.641% (13123/13440)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.642% (13248/13568)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.612% (13369/13696)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.613% (13494/13824)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.599% (13617/13952)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.599% (13742/14080)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.600% (13867/14208)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.600% (13992/14336)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.594% (14116/14464)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.615% (14244/14592)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.629% (14371/14720)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.623% (14495/14848)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.616% (14619/14976)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.617% (14744/15104)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.623% (14870/15232)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.630% (14996/15360)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.630% (15121/15488)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.650% (15249/15616)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.624% (15370/15744)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.625% (15495/15872)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.631% (15621/16000)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.631% (15746/16128)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.625% (15870/16256)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.620% (15994/16384)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.614% (16118/16512)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.620% (16244/16640)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.626% (16370/16768)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.603% (16491/16896)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.603% (16616/17024)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.610% (16742/17152)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.616% (16868/17280)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.628% (16995/17408)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.633% (17121/17536)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.645% (17248/17664)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.628% (17370/17792)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.617% (17493/17920)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.623% (17619/18048)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.623% (17744/18176)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.629% (17870/18304)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.629% (17995/18432)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.640% (18122/18560)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.640% (18247/18688)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.630% (18370/18816)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.625% (18494/18944)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.630% (18620/19072)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.635% (18746/19200)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.630% (18870/19328)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.620% (18993/19456)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.621% (19118/19584)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.621% (19243/19712)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.616% (19367/19840)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.616% (19492/19968)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.621% (19618/20096)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.636% (19746/20224)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.642% (19872/20352)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.627% (19994/20480)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.622% (20118/20608)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.618% (20242/20736)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.608% (20365/20864)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.618% (20492/20992)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.628% (20619/21120)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.628% (20744/21248)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.638% (20871/21376)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.619% (20992/21504)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.629% (21119/21632)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.642% (21247/21760)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.647% (21373/21888)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.652% (21499/22016)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.656% (21625/22144)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.656% (21750/22272)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.661% (21876/22400)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.656% (22000/22528)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.656% (22125/22656)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.661% (22251/22784)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.652% (22374/22912)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.656% (22500/23040)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.661% (22626/23168)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.669% (22753/23296)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.661% (22876/23424)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23001/23552)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23126/23680)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23251/23808)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.665% (23377/23936)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23501/24064)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23626/24192)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (23751/24320)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.652% (23874/24448)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (24001/24576)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (24127/24704)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (24252/24832)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (24377/24960)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (24502/25088)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (24627/25216)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.676% (24755/25344)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.676% (24880/25472)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.676% (25005/25600)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.652% (25124/25728)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.652% (25249/25856)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.656% (25375/25984)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (25502/26112)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.671% (25629/26240)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.671% (25754/26368)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.675% (25880/26496)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.675% (26005/26624)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.675% (26130/26752)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.671% (26254/26880)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.671% (26379/27008)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.664% (26502/27136)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (26626/27264)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.660% (26751/27392)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (26874/27520)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.649% (26998/27648)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (27124/27776)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (27249/27904)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.656% (27375/28032)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.656% (27500/28160)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.649% (27623/28288)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (27747/28416)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.649% (27873/28544)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.635% (27994/28672)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (28122/28800)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (28247/28928)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.639% (28370/29056)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.643% (28496/29184)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.643% (28621/29312)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.639% (28745/29440)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (28872/29568)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.636% (28994/29696)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.639% (29120/29824)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.643% (29246/29952)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.640% (29370/30080)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.640% (29495/30208)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (29622/30336)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (29749/30464)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (29872/30592)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.646% (29997/30720)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (30124/30848)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.659% (30251/30976)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.663% (30377/31104)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.666% (30503/31232)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.669% (30629/31360)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.669% (30754/31488)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.672% (30880/31616)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.672% (31005/31744)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.666% (31128/31872)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.659% (31251/32000)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.666% (31378/32128)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.672% (31505/32256)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.672% (31630/32384)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.675% (31756/32512)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.675% (31881/32640)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.662% (32002/32768)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.671% (32130/32896)\n",
            "Train Epoch: 145 | Loss: 0.069 | Acc: 97.674% (32256/33024)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.668% (32379/33152)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.671% (32505/33280)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.665% (32628/33408)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.665% (32753/33536)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.662% (32877/33664)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.662% (33002/33792)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.662% (33127/33920)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.659% (33251/34048)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.653% (33374/34176)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.648% (33497/34304)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.639% (33619/34432)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.633% (33742/34560)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.633% (33867/34688)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.636% (33993/34816)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.633% (34117/34944)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.631% (34241/35072)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.634% (34367/35200)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.639% (34494/35328)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.645% (34621/35456)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.645% (34746/35584)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.651% (34873/35712)\n",
            "Train Epoch: 145 | Loss: 0.070 | Acc: 97.645% (34996/35840)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.637% (35118/35968)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.637% (35243/36096)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.640% (35369/36224)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.648% (35497/36352)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (35623/36480)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (35748/36608)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (35874/36736)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (35998/36864)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (36123/36992)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (36248/37120)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (36373/37248)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (36500/37376)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (36621/37504)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (36749/37632)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (36875/37760)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.662% (37002/37888)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (37126/38016)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (37251/38144)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (37376/38272)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.661% (37502/38400)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (37626/38528)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (37749/38656)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (37874/38784)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38000/38912)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38125/39040)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (38249/39168)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (38374/39296)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38500/39424)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38625/39552)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38750/39680)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (38875/39808)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.644% (38995/39936)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.644% (39120/40064)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.636% (39242/40192)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.639% (39368/40320)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.632% (39490/40448)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.637% (39617/40576)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.642% (39744/40704)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.649% (39872/40832)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (39998/40960)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.649% (40122/41088)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (40246/41216)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.651% (40373/41344)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (40499/41472)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (40626/41600)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (40751/41728)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.659% (40876/41856)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (41000/41984)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (41125/42112)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.661% (41252/42240)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.661% (41377/42368)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.661% (41502/42496)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (41625/42624)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (41750/42752)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.652% (41873/42880)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (41996/43008)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.649% (42122/43136)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (42249/43264)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.656% (42375/43392)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (42499/43520)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (42624/43648)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.652% (42748/43776)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.649% (42872/43904)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (42996/44032)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (43121/44160)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.649% (43247/44288)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.652% (43373/44416)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.643% (43494/44544)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.643% (43619/44672)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.638% (43742/44800)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.643% (43869/44928)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.645% (43995/45056)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.643% (44119/45184)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (44246/45312)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (44371/45440)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.650% (44497/45568)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.647% (44621/45696)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.641% (44743/45824)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.637% (44866/45952)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.641% (44993/46080)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.641% (45118/46208)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.645% (45245/46336)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.645% (45370/46464)\n",
            "Train Epoch: 145 | Loss: 0.072 | Acc: 97.641% (45493/46592)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.643% (45619/46720)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (45745/46848)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (45870/46976)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.650% (45997/47104)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.652% (46123/47232)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.650% (46247/47360)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.654% (46374/47488)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (46495/47616)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.644% (46619/47744)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.644% (46744/47872)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (46870/48000)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.642% (46993/48128)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.646% (47120/48256)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.640% (47242/48384)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.640% (47367/48512)\n",
            "Train Epoch: 145 | Loss: 0.072 | Acc: 97.627% (47486/48640)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.628% (47611/48768)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.628% (47736/48896)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.628% (47861/49024)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.630% (47987/49152)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.626% (48110/49280)\n",
            "Train Epoch: 145 | Loss: 0.072 | Acc: 97.624% (48234/49408)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.628% (48361/49536)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.630% (48487/49664)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.630% (48612/49792)\n",
            "Train Epoch: 145 | Loss: 0.071 | Acc: 97.634% (48739/49920)\n",
            "Train Epoch: 145 | Loss: 0.072 | Acc: 97.624% (48812/50000)\n",
            "Test Epoch: 145 | Loss: 0.304 | Acc: 95.000% (95/100)\n",
            "Test Epoch: 145 | Loss: 0.365 | Acc: 92.500% (185/200)\n",
            "Test Epoch: 145 | Loss: 0.367 | Acc: 91.667% (275/300)\n",
            "Test Epoch: 145 | Loss: 0.321 | Acc: 92.250% (369/400)\n",
            "Test Epoch: 145 | Loss: 0.307 | Acc: 92.400% (462/500)\n",
            "Test Epoch: 145 | Loss: 0.271 | Acc: 93.000% (558/600)\n",
            "Test Epoch: 145 | Loss: 0.301 | Acc: 93.000% (651/700)\n",
            "Test Epoch: 145 | Loss: 0.328 | Acc: 92.125% (737/800)\n",
            "Test Epoch: 145 | Loss: 0.333 | Acc: 91.889% (827/900)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.900% (919/1000)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.636% (1008/1100)\n",
            "Test Epoch: 145 | Loss: 0.352 | Acc: 91.417% (1097/1200)\n",
            "Test Epoch: 145 | Loss: 0.351 | Acc: 91.231% (1186/1300)\n",
            "Test Epoch: 145 | Loss: 0.333 | Acc: 91.643% (1283/1400)\n",
            "Test Epoch: 145 | Loss: 0.323 | Acc: 91.667% (1375/1500)\n",
            "Test Epoch: 145 | Loss: 0.312 | Acc: 91.938% (1471/1600)\n",
            "Test Epoch: 145 | Loss: 0.308 | Acc: 92.118% (1566/1700)\n",
            "Test Epoch: 145 | Loss: 0.304 | Acc: 92.056% (1657/1800)\n",
            "Test Epoch: 145 | Loss: 0.309 | Acc: 92.000% (1748/1900)\n",
            "Test Epoch: 145 | Loss: 0.319 | Acc: 91.900% (1838/2000)\n",
            "Test Epoch: 145 | Loss: 0.318 | Acc: 91.857% (1929/2100)\n",
            "Test Epoch: 145 | Loss: 0.321 | Acc: 91.682% (2017/2200)\n",
            "Test Epoch: 145 | Loss: 0.325 | Acc: 91.652% (2108/2300)\n",
            "Test Epoch: 145 | Loss: 0.322 | Acc: 91.708% (2201/2400)\n",
            "Test Epoch: 145 | Loss: 0.330 | Acc: 91.640% (2291/2500)\n",
            "Test Epoch: 145 | Loss: 0.344 | Acc: 91.538% (2380/2600)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.667% (2475/2700)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.750% (2569/2800)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.655% (2658/2900)\n",
            "Test Epoch: 145 | Loss: 0.342 | Acc: 91.633% (2749/3000)\n",
            "Test Epoch: 145 | Loss: 0.349 | Acc: 91.419% (2834/3100)\n",
            "Test Epoch: 145 | Loss: 0.349 | Acc: 91.375% (2924/3200)\n",
            "Test Epoch: 145 | Loss: 0.346 | Acc: 91.364% (3015/3300)\n",
            "Test Epoch: 145 | Loss: 0.347 | Acc: 91.265% (3103/3400)\n",
            "Test Epoch: 145 | Loss: 0.348 | Acc: 91.257% (3194/3500)\n",
            "Test Epoch: 145 | Loss: 0.347 | Acc: 91.389% (3290/3600)\n",
            "Test Epoch: 145 | Loss: 0.354 | Acc: 91.270% (3377/3700)\n",
            "Test Epoch: 145 | Loss: 0.356 | Acc: 91.263% (3468/3800)\n",
            "Test Epoch: 145 | Loss: 0.353 | Acc: 91.333% (3562/3900)\n",
            "Test Epoch: 145 | Loss: 0.349 | Acc: 91.425% (3657/4000)\n",
            "Test Epoch: 145 | Loss: 0.355 | Acc: 91.293% (3743/4100)\n",
            "Test Epoch: 145 | Loss: 0.355 | Acc: 91.310% (3835/4200)\n",
            "Test Epoch: 145 | Loss: 0.350 | Acc: 91.419% (3931/4300)\n",
            "Test Epoch: 145 | Loss: 0.350 | Acc: 91.455% (4024/4400)\n",
            "Test Epoch: 145 | Loss: 0.347 | Acc: 91.489% (4117/4500)\n",
            "Test Epoch: 145 | Loss: 0.348 | Acc: 91.478% (4208/4600)\n",
            "Test Epoch: 145 | Loss: 0.344 | Acc: 91.532% (4302/4700)\n",
            "Test Epoch: 145 | Loss: 0.349 | Acc: 91.396% (4387/4800)\n",
            "Test Epoch: 145 | Loss: 0.344 | Acc: 91.551% (4486/4900)\n",
            "Test Epoch: 145 | Loss: 0.348 | Acc: 91.500% (4575/5000)\n",
            "Test Epoch: 145 | Loss: 0.343 | Acc: 91.627% (4673/5100)\n",
            "Test Epoch: 145 | Loss: 0.342 | Acc: 91.673% (4767/5200)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.717% (4861/5300)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.704% (4952/5400)\n",
            "Test Epoch: 145 | Loss: 0.338 | Acc: 91.709% (5044/5500)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.732% (5137/5600)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.737% (5229/5700)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.707% (5319/5800)\n",
            "Test Epoch: 145 | Loss: 0.341 | Acc: 91.644% (5407/5900)\n",
            "Test Epoch: 145 | Loss: 0.341 | Acc: 91.600% (5496/6000)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.639% (5590/6100)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.661% (5683/6200)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.651% (5774/6300)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.719% (5870/6400)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.692% (5960/6500)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.712% (6053/6600)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.701% (6144/6700)\n",
            "Test Epoch: 145 | Loss: 0.341 | Acc: 91.632% (6231/6800)\n",
            "Test Epoch: 145 | Loss: 0.338 | Acc: 91.681% (6326/6900)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.657% (6416/7000)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.592% (6503/7100)\n",
            "Test Epoch: 145 | Loss: 0.341 | Acc: 91.583% (6594/7200)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.603% (6687/7300)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.649% (6782/7400)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.640% (6873/7500)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.592% (6961/7600)\n",
            "Test Epoch: 145 | Loss: 0.340 | Acc: 91.558% (7050/7700)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.577% (7143/7800)\n",
            "Test Epoch: 145 | Loss: 0.339 | Acc: 91.582% (7235/7900)\n",
            "Test Epoch: 145 | Loss: 0.338 | Acc: 91.575% (7326/8000)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.654% (7424/8100)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.622% (7513/8200)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.614% (7604/8300)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.631% (7697/8400)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.624% (7788/8500)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.535% (7872/8600)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.552% (7965/8700)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.568% (8058/8800)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.573% (8150/8900)\n",
            "Test Epoch: 145 | Loss: 0.336 | Acc: 91.544% (8239/9000)\n",
            "Test Epoch: 145 | Loss: 0.337 | Acc: 91.538% (8330/9100)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.554% (8423/9200)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.581% (8517/9300)\n",
            "Test Epoch: 145 | Loss: 0.334 | Acc: 91.574% (8608/9400)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.516% (8694/9500)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.510% (8785/9600)\n",
            "Test Epoch: 145 | Loss: 0.334 | Acc: 91.546% (8880/9700)\n",
            "Test Epoch: 145 | Loss: 0.333 | Acc: 91.592% (8976/9800)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.545% (9063/9900)\n",
            "Test Epoch: 145 | Loss: 0.335 | Acc: 91.530% (9153/10000)\n",
            "\n",
            "Epoch: 146\n",
            "Train Epoch: 146 | Loss: 0.086 | Acc: 98.438% (126/128)\n",
            "Train Epoch: 146 | Loss: 0.052 | Acc: 99.219% (254/256)\n",
            "Train Epoch: 146 | Loss: 0.074 | Acc: 98.177% (377/384)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 98.438% (504/512)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 98.281% (629/640)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 98.177% (754/768)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 98.214% (880/896)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.340% (1007/1024)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.351% (1133/1152)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.438% (1260/1280)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.295% (1384/1408)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.307% (1510/1536)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.257% (1635/1664)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.326% (1762/1792)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.229% (1886/1920)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.242% (2012/2048)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.254% (2138/2176)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.307% (2265/2304)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.314% (2391/2432)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.242% (2515/2560)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.289% (2642/2688)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.260% (2767/2816)\n",
            "Train Epoch: 146 | Loss: 0.054 | Acc: 98.302% (2894/2944)\n",
            "Train Epoch: 146 | Loss: 0.053 | Acc: 98.340% (3021/3072)\n",
            "Train Epoch: 146 | Loss: 0.053 | Acc: 98.312% (3146/3200)\n",
            "Train Epoch: 146 | Loss: 0.052 | Acc: 98.347% (3273/3328)\n",
            "Train Epoch: 146 | Loss: 0.054 | Acc: 98.293% (3397/3456)\n",
            "Train Epoch: 146 | Loss: 0.054 | Acc: 98.270% (3522/3584)\n",
            "Train Epoch: 146 | Loss: 0.054 | Acc: 98.303% (3649/3712)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.307% (3775/3840)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.286% (3900/3968)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.267% (4025/4096)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.248% (4150/4224)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.208% (4274/4352)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.192% (4399/4480)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.242% (4527/4608)\n",
            "Train Epoch: 146 | Loss: 0.054 | Acc: 98.247% (4653/4736)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.191% (4776/4864)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.197% (4902/4992)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.164% (5026/5120)\n",
            "Train Epoch: 146 | Loss: 0.055 | Acc: 98.171% (5152/5248)\n",
            "Train Epoch: 146 | Loss: 0.056 | Acc: 98.158% (5277/5376)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.129% (5401/5504)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.136% (5527/5632)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.142% (5653/5760)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.183% (5781/5888)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.138% (5904/6016)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.079% (6026/6144)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.087% (6152/6272)\n",
            "Train Epoch: 146 | Loss: 0.057 | Acc: 98.094% (6278/6400)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.070% (6402/6528)\n",
            "Train Epoch: 146 | Loss: 0.058 | Acc: 98.077% (6528/6656)\n",
            "Train Epoch: 146 | Loss: 0.059 | Acc: 98.069% (6653/6784)\n",
            "Train Epoch: 146 | Loss: 0.059 | Acc: 98.076% (6779/6912)\n",
            "Train Epoch: 146 | Loss: 0.059 | Acc: 98.054% (6903/7040)\n",
            "Train Epoch: 146 | Loss: 0.059 | Acc: 98.061% (7029/7168)\n",
            "Train Epoch: 146 | Loss: 0.059 | Acc: 98.067% (7155/7296)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.047% (7279/7424)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.067% (7406/7552)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.034% (7529/7680)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.053% (7656/7808)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.072% (7783/7936)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.078% (7909/8064)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.083% (8035/8192)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.065% (8159/8320)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.082% (8286/8448)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.053% (8409/8576)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.070% (8536/8704)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.087% (8663/8832)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.103% (8790/8960)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.052% (8911/9088)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.058% (9037/9216)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.052% (9162/9344)\n",
            "Train Epoch: 146 | Loss: 0.060 | Acc: 98.057% (9288/9472)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.052% (9413/9600)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.037% (9537/9728)\n",
            "Train Epoch: 146 | Loss: 0.062 | Acc: 98.022% (9661/9856)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.037% (9788/9984)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.062% (9916/10112)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.066% (10042/10240)\n",
            "Train Epoch: 146 | Loss: 0.061 | Acc: 98.042% (10165/10368)\n",
            "Train Epoch: 146 | Loss: 0.062 | Acc: 98.028% (10289/10496)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.967% (10408/10624)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.972% (10534/10752)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.987% (10661/10880)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.983% (10786/11008)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.997% (10913/11136)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 98.011% (11040/11264)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 98.016% (11166/11392)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 98.003% (11290/11520)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.948% (11409/11648)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.936% (11533/11776)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.950% (11660/11904)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.955% (11786/12032)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.952% (11911/12160)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.933% (12034/12288)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.938% (12160/12416)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.919% (12283/12544)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.940% (12411/12672)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.953% (12538/12800)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.958% (12664/12928)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.970% (12791/13056)\n",
            "Train Epoch: 146 | Loss: 0.063 | Acc: 97.944% (12913/13184)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.919% (13035/13312)\n",
            "Train Epoch: 146 | Loss: 0.064 | Acc: 97.902% (13158/13440)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.870% (13279/13568)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.883% (13406/13696)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.859% (13528/13824)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.843% (13651/13952)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.848% (13777/14080)\n",
            "Train Epoch: 146 | Loss: 0.066 | Acc: 97.853% (13903/14208)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.859% (14029/14336)\n",
            "Train Epoch: 146 | Loss: 0.065 | Acc: 97.857% (14154/14464)\n",
            "Train Epoch: 146 | Loss: 0.066 | Acc: 97.855% (14279/14592)\n",
            "Train Epoch: 146 | Loss: 0.066 | Acc: 97.840% (14402/14720)\n",
            "Train Epoch: 146 | Loss: 0.066 | Acc: 97.825% (14525/14848)\n",
            "Train Epoch: 146 | Loss: 0.066 | Acc: 97.823% (14650/14976)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.795% (14771/15104)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.781% (14894/15232)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.760% (15016/15360)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.779% (15144/15488)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.759% (15266/15616)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.764% (15392/15744)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.763% (15517/15872)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.756% (15641/16000)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.737% (15763/16128)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.755% (15891/16256)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.754% (16016/16384)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.759% (16142/16512)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.758% (16267/16640)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.758% (16392/16768)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.757% (16517/16896)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.750% (16641/17024)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.761% (16768/17152)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.766% (16894/17280)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.771% (17020/17408)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.776% (17146/17536)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.786% (17273/17664)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.769% (17395/17792)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.779% (17522/17920)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.773% (17646/18048)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.788% (17774/18176)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.793% (17900/18304)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.803% (18027/18432)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.807% (18153/18560)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.785% (18274/18688)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.800% (18402/18816)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.793% (18526/18944)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.787% (18650/19072)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.776% (18773/19200)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.780% (18899/19328)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.790% (19026/19456)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.799% (19153/19584)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.798% (19278/19712)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.787% (19401/19840)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.786% (19526/19968)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.791% (19652/20096)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.780% (19775/20224)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.774% (19899/20352)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.773% (20024/20480)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.782% (20151/20608)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.772% (20274/20736)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.771% (20399/20864)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.771% (20524/20992)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.770% (20649/21120)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.760% (20772/21248)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.759% (20897/21376)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.773% (21025/21504)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.767% (21149/21632)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.762% (21273/21760)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.757% (21397/21888)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.747% (21520/22016)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.756% (21647/22144)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.764% (21774/22272)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.768% (21900/22400)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.763% (22024/22528)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.762% (22149/22656)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.753% (22272/22784)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.757% (22398/22912)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.752% (22522/23040)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.738% (22644/23168)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.742% (22770/23296)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.750% (22897/23424)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.745% (23021/23552)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.736% (23144/23680)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.736% (23269/23808)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.736% (23394/23936)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.739% (23520/24064)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.743% (23646/24192)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.747% (23772/24320)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.734% (23894/24448)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.729% (24018/24576)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.741% (24146/24704)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.733% (24269/24832)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.728% (24393/24960)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.732% (24519/25088)\n",
            "Train Epoch: 146 | Loss: 0.067 | Acc: 97.728% (24643/25216)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.711% (24764/25344)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.719% (24891/25472)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.707% (25013/25600)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.699% (25136/25728)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.695% (25260/25856)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.702% (25387/25984)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.695% (25510/26112)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.691% (25634/26240)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.687% (25758/26368)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.683% (25882/26496)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.686% (26008/26624)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.690% (26134/26752)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.697% (26261/26880)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.697% (26386/27008)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.689% (26509/27136)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.693% (26635/27264)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.696% (26761/27392)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.703% (26888/27520)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.700% (27012/27648)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.699% (27137/27776)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.696% (27261/27904)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.695% (27386/28032)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.692% (27510/28160)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.695% (27636/28288)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.698% (27762/28416)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.698% (27887/28544)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.702% (28013/28672)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.691% (28135/28800)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.687% (28259/28928)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.694% (28386/29056)\n",
            "Train Epoch: 146 | Loss: 0.068 | Acc: 97.691% (28510/29184)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.680% (28632/29312)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.666% (28753/29440)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.666% (28878/29568)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.673% (29005/29696)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.670% (29129/29824)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.680% (29257/29952)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.673% (29380/30080)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.679% (29507/30208)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.676% (29631/30336)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.679% (29757/30464)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.676% (29881/30592)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.679% (30007/30720)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.679% (30132/30848)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.676% (30256/30976)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.682% (30383/31104)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.679% (30507/31232)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.669% (30629/31360)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (30751/31488)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.653% (30874/31616)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (31000/31744)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (31126/31872)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (31252/32000)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (31377/32128)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.666% (31503/32256)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.669% (31629/32384)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.678% (31757/32512)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.669% (31879/32640)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (32004/32768)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.674% (32131/32896)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.671% (32255/33024)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (32379/33152)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.665% (32503/33280)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.671% (32630/33408)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.671% (32755/33536)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (32879/33664)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (33004/33792)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (33127/33920)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (33254/34048)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (33377/34176)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (33502/34304)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (33629/34432)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (33754/34560)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.676% (33882/34688)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.676% (34007/34816)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.682% (34134/34944)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.682% (34259/35072)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.668% (34379/35200)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.673% (34506/35328)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.670% (34630/35456)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.667% (34754/35584)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.673% (34881/35712)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.667% (35004/35840)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (35127/35968)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (35250/36096)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (35375/36224)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.653% (35499/36352)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (35626/36480)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (35751/36608)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.664% (35878/36736)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (36002/36864)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.664% (36128/36992)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (36251/37120)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (36377/37248)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.670% (36505/37376)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.662% (36627/37504)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (36749/37632)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (36874/37760)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (36999/37888)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.648% (37122/38016)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (37249/38144)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.648% (37372/38272)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (37499/38400)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (37626/38528)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (37749/38656)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (37873/38784)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (37998/38912)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (38122/39040)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (38249/39168)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (38375/39296)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.646% (38496/39424)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (38622/39552)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (38749/39680)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (38875/39808)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (39001/39936)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (39124/40064)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (39248/40192)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (39376/40320)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (39499/40448)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.646% (39621/40576)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (39748/40704)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (39873/40832)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (39997/40960)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (40122/41088)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.651% (40248/41216)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (40372/41344)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (40497/41472)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (40624/41600)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (40751/41728)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.659% (40876/41856)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.663% (41003/41984)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.663% (41128/42112)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.661% (41252/42240)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.663% (41378/42368)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.663% (41503/42496)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.656% (41625/42624)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.652% (41748/42752)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.649% (41872/42880)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.647% (41996/43008)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.645% (42120/43136)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.647% (42246/43264)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.642% (42369/43392)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.642% (42494/43520)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (42618/43648)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (42743/43776)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.636% (42866/43904)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.643% (42994/44032)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.638% (43117/44160)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.643% (43244/44288)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.636% (43366/44416)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.638% (43492/44544)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.636% (43616/44672)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.632% (43739/44800)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.625% (43861/44928)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.630% (43988/45056)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.627% (44112/45184)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.632% (44239/45312)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.634% (44365/45440)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.630% (44488/45568)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.634% (44615/45696)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.630% (44738/45824)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.632% (44864/45952)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.632% (44989/46080)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.639% (45117/46208)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.645% (45245/46336)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.645% (45370/46464)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.650% (45497/46592)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (45624/46720)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.652% (45748/46848)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (45874/46976)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (45999/47104)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.650% (46122/47232)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.654% (46249/47360)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.650% (46372/47488)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.652% (46498/47616)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.646% (46620/47744)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.644% (46744/47872)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.642% (46868/48000)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (46992/48128)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (47117/48256)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.644% (47244/48384)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.642% (47368/48512)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (47492/48640)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.636% (47615/48768)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.638% (47741/48896)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.636% (47865/49024)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.634% (47989/49152)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.636% (48115/49280)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.640% (48242/49408)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.644% (48369/49536)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.642% (48493/49664)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.646% (48620/49792)\n",
            "Train Epoch: 146 | Loss: 0.069 | Acc: 97.644% (48744/49920)\n",
            "Train Epoch: 146 | Loss: 0.070 | Acc: 97.642% (48821/50000)\n",
            "Test Epoch: 146 | Loss: 0.173 | Acc: 97.000% (97/100)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 94.000% (188/200)\n",
            "Test Epoch: 146 | Loss: 0.387 | Acc: 92.333% (277/300)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 92.500% (370/400)\n",
            "Test Epoch: 146 | Loss: 0.292 | Acc: 92.800% (464/500)\n",
            "Test Epoch: 146 | Loss: 0.262 | Acc: 93.333% (560/600)\n",
            "Test Epoch: 146 | Loss: 0.285 | Acc: 93.143% (652/700)\n",
            "Test Epoch: 146 | Loss: 0.312 | Acc: 92.125% (737/800)\n",
            "Test Epoch: 146 | Loss: 0.319 | Acc: 92.111% (829/900)\n",
            "Test Epoch: 146 | Loss: 0.316 | Acc: 92.200% (922/1000)\n",
            "Test Epoch: 146 | Loss: 0.316 | Acc: 92.000% (1012/1100)\n",
            "Test Epoch: 146 | Loss: 0.320 | Acc: 91.750% (1101/1200)\n",
            "Test Epoch: 146 | Loss: 0.311 | Acc: 91.615% (1191/1300)\n",
            "Test Epoch: 146 | Loss: 0.298 | Acc: 91.786% (1285/1400)\n",
            "Test Epoch: 146 | Loss: 0.290 | Acc: 92.000% (1380/1500)\n",
            "Test Epoch: 146 | Loss: 0.280 | Acc: 92.250% (1476/1600)\n",
            "Test Epoch: 146 | Loss: 0.281 | Acc: 92.235% (1568/1700)\n",
            "Test Epoch: 146 | Loss: 0.277 | Acc: 92.167% (1659/1800)\n",
            "Test Epoch: 146 | Loss: 0.280 | Acc: 92.053% (1749/1900)\n",
            "Test Epoch: 146 | Loss: 0.289 | Acc: 92.000% (1840/2000)\n",
            "Test Epoch: 146 | Loss: 0.286 | Acc: 91.905% (1930/2100)\n",
            "Test Epoch: 146 | Loss: 0.286 | Acc: 91.955% (2023/2200)\n",
            "Test Epoch: 146 | Loss: 0.297 | Acc: 91.696% (2109/2300)\n",
            "Test Epoch: 146 | Loss: 0.296 | Acc: 91.792% (2203/2400)\n",
            "Test Epoch: 146 | Loss: 0.302 | Acc: 91.840% (2296/2500)\n",
            "Test Epoch: 146 | Loss: 0.315 | Acc: 91.654% (2383/2600)\n",
            "Test Epoch: 146 | Loss: 0.310 | Acc: 91.667% (2475/2700)\n",
            "Test Epoch: 146 | Loss: 0.310 | Acc: 91.750% (2569/2800)\n",
            "Test Epoch: 146 | Loss: 0.313 | Acc: 91.690% (2659/2900)\n",
            "Test Epoch: 146 | Loss: 0.316 | Acc: 91.700% (2751/3000)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.516% (2837/3100)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.438% (2926/3200)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.424% (3017/3300)\n",
            "Test Epoch: 146 | Loss: 0.331 | Acc: 91.353% (3106/3400)\n",
            "Test Epoch: 146 | Loss: 0.337 | Acc: 91.257% (3194/3500)\n",
            "Test Epoch: 146 | Loss: 0.336 | Acc: 91.333% (3288/3600)\n",
            "Test Epoch: 146 | Loss: 0.343 | Acc: 91.189% (3374/3700)\n",
            "Test Epoch: 146 | Loss: 0.345 | Acc: 91.132% (3463/3800)\n",
            "Test Epoch: 146 | Loss: 0.340 | Acc: 91.256% (3559/3900)\n",
            "Test Epoch: 146 | Loss: 0.339 | Acc: 91.300% (3652/4000)\n",
            "Test Epoch: 146 | Loss: 0.345 | Acc: 91.220% (3740/4100)\n",
            "Test Epoch: 146 | Loss: 0.344 | Acc: 91.190% (3830/4200)\n",
            "Test Epoch: 146 | Loss: 0.339 | Acc: 91.326% (3927/4300)\n",
            "Test Epoch: 146 | Loss: 0.338 | Acc: 91.364% (4020/4400)\n",
            "Test Epoch: 146 | Loss: 0.335 | Acc: 91.422% (4114/4500)\n",
            "Test Epoch: 146 | Loss: 0.336 | Acc: 91.413% (4205/4600)\n",
            "Test Epoch: 146 | Loss: 0.333 | Acc: 91.447% (4298/4700)\n",
            "Test Epoch: 146 | Loss: 0.336 | Acc: 91.396% (4387/4800)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.531% (4485/4900)\n",
            "Test Epoch: 146 | Loss: 0.335 | Acc: 91.480% (4574/5000)\n",
            "Test Epoch: 146 | Loss: 0.331 | Acc: 91.588% (4671/5100)\n",
            "Test Epoch: 146 | Loss: 0.332 | Acc: 91.577% (4762/5200)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.566% (4853/5300)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.574% (4945/5400)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.545% (5035/5500)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.589% (5129/5600)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.579% (5220/5700)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.586% (5312/5800)\n",
            "Test Epoch: 146 | Loss: 0.331 | Acc: 91.525% (5400/5900)\n",
            "Test Epoch: 146 | Loss: 0.330 | Acc: 91.583% (5495/6000)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.607% (5588/6100)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.629% (5681/6200)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.667% (5775/6300)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.703% (5869/6400)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.692% (5960/6500)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.742% (6055/6600)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.776% (6149/6700)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.706% (6236/6800)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.681% (6326/6900)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.671% (6417/7000)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.662% (6508/7100)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.694% (6602/7200)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.726% (6696/7300)\n",
            "Test Epoch: 146 | Loss: 0.323 | Acc: 91.716% (6787/7400)\n",
            "Test Epoch: 146 | Loss: 0.324 | Acc: 91.653% (6874/7500)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.658% (6966/7600)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.597% (7053/7700)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.641% (7148/7800)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.633% (7239/7900)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.625% (7330/8000)\n",
            "Test Epoch: 146 | Loss: 0.324 | Acc: 91.679% (7426/8100)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.671% (7517/8200)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.651% (7607/8300)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.655% (7699/8400)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.635% (7789/8500)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.547% (7873/8600)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.575% (7967/8700)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.557% (8057/8800)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.573% (8150/8900)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.567% (8241/9000)\n",
            "Test Epoch: 146 | Loss: 0.329 | Acc: 91.593% (8335/9100)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.609% (8428/9200)\n",
            "Test Epoch: 146 | Loss: 0.328 | Acc: 91.602% (8519/9300)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.628% (8613/9400)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.642% (8706/9500)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.635% (8797/9600)\n",
            "Test Epoch: 146 | Loss: 0.325 | Acc: 91.680% (8893/9700)\n",
            "Test Epoch: 146 | Loss: 0.324 | Acc: 91.704% (8987/9800)\n",
            "Test Epoch: 146 | Loss: 0.326 | Acc: 91.687% (9077/9900)\n",
            "Test Epoch: 146 | Loss: 0.327 | Acc: 91.700% (9170/10000)\n",
            "\n",
            "Epoch: 147\n",
            "Train Epoch: 147 | Loss: 0.101 | Acc: 96.094% (123/128)\n",
            "Train Epoch: 147 | Loss: 0.078 | Acc: 97.266% (249/256)\n",
            "Train Epoch: 147 | Loss: 0.062 | Acc: 98.177% (377/384)\n",
            "Train Epoch: 147 | Loss: 0.059 | Acc: 98.047% (502/512)\n",
            "Train Epoch: 147 | Loss: 0.071 | Acc: 97.812% (626/640)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 98.047% (753/768)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.768% (876/896)\n",
            "Train Epoch: 147 | Loss: 0.070 | Acc: 97.656% (1000/1024)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.656% (1125/1152)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.812% (1252/1280)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.869% (1378/1408)\n",
            "Train Epoch: 147 | Loss: 0.061 | Acc: 98.047% (1506/1536)\n",
            "Train Epoch: 147 | Loss: 0.060 | Acc: 98.077% (1632/1664)\n",
            "Train Epoch: 147 | Loss: 0.059 | Acc: 98.047% (1757/1792)\n",
            "Train Epoch: 147 | Loss: 0.060 | Acc: 97.969% (1881/1920)\n",
            "Train Epoch: 147 | Loss: 0.061 | Acc: 97.949% (2006/2048)\n",
            "Train Epoch: 147 | Loss: 0.060 | Acc: 97.932% (2131/2176)\n",
            "Train Epoch: 147 | Loss: 0.060 | Acc: 97.960% (2257/2304)\n",
            "Train Epoch: 147 | Loss: 0.060 | Acc: 97.985% (2383/2432)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.734% (2502/2560)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.545% (2622/2688)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.585% (2748/2816)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.554% (2872/2944)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.624% (2999/3072)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.594% (3123/3200)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.626% (3249/3328)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.569% (3372/3456)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.628% (3499/3584)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.629% (3624/3712)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.656% (3750/3840)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.732% (3878/3968)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.705% (4002/4096)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.775% (4130/4224)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.794% (4256/4352)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.768% (4380/4480)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.765% (4505/4608)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.741% (4629/4736)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.759% (4755/4864)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.796% (4882/4992)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.773% (5006/5120)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.790% (5132/5248)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.805% (5258/5376)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.838% (5385/5504)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.852% (5511/5632)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.812% (5634/5760)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.843% (5761/5888)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.806% (5884/6016)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.786% (6008/6144)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.800% (6134/6272)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.812% (6260/6400)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.840% (6387/6528)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.867% (6514/6656)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.892% (6641/6784)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.917% (6768/6912)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.955% (6896/7040)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.949% (7021/7168)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.971% (7148/7296)\n",
            "Train Epoch: 147 | Loss: 0.062 | Acc: 97.980% (7274/7424)\n",
            "Train Epoch: 147 | Loss: 0.062 | Acc: 97.987% (7400/7552)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.930% (7521/7680)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.938% (7647/7808)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.921% (7771/7936)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.904% (7895/8064)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.888% (8019/8192)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.873% (8143/8320)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.893% (8270/8448)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.854% (8392/8576)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.875% (8519/8704)\n",
            "Train Epoch: 147 | Loss: 0.063 | Acc: 97.871% (8644/8832)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.868% (8769/8960)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.843% (8892/9088)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.841% (9017/9216)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.860% (9144/9344)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.867% (9270/9472)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.844% (9393/9600)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.862% (9520/9728)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.859% (9645/9856)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.857% (9770/9984)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.824% (9892/10112)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.822% (10017/10240)\n",
            "Train Epoch: 147 | Loss: 0.064 | Acc: 97.840% (10144/10368)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.799% (10265/10496)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.807% (10391/10624)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.796% (10515/10752)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.785% (10639/10880)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.765% (10762/11008)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.746% (10885/11136)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.754% (11011/11264)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.770% (11138/11392)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.795% (11266/11520)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.794% (11391/11648)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.775% (11514/11776)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.774% (11639/11904)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.781% (11765/12032)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.796% (11892/12160)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.811% (12019/12288)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.801% (12143/12416)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.800% (12268/12544)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.783% (12391/12672)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.781% (12516/12800)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.765% (12639/12928)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.763% (12764/13056)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.762% (12889/13184)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.754% (13013/13312)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.768% (13140/13440)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.767% (13265/13568)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.788% (13393/13696)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.794% (13519/13824)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.785% (13643/13952)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.791% (13769/14080)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.783% (13893/14208)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.775% (14017/14336)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.767% (14141/14464)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.773% (14267/14592)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.765% (14391/14720)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.764% (14516/14848)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.770% (14642/14976)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.762% (14766/15104)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.755% (14890/15232)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.773% (15018/15360)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.779% (15144/15488)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.784% (15270/15616)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.783% (15395/15744)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.770% (15518/15872)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.769% (15643/16000)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.780% (15770/16128)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.773% (15894/16256)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.778% (16020/16384)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.789% (16147/16512)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.788% (16272/16640)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.793% (16398/16768)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.792% (16523/16896)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.797% (16649/17024)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.796% (16774/17152)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.789% (16898/17280)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.800% (17025/17408)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.805% (17151/17536)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.798% (17275/17664)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.814% (17403/17792)\n",
            "Train Epoch: 147 | Loss: 0.065 | Acc: 97.807% (17527/17920)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.806% (17652/18048)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.799% (17776/18176)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.804% (17902/18304)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.797% (18026/18432)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.807% (18153/18560)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.801% (18277/18688)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.800% (18402/18816)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.804% (18528/18944)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.803% (18653/19072)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.802% (18778/19200)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.806% (18904/19328)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.821% (19032/19456)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.825% (19158/19584)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.834% (19285/19712)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.833% (19410/19840)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.817% (19532/19968)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.820% (19658/20096)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.824% (19784/20224)\n",
            "Train Epoch: 147 | Loss: 0.066 | Acc: 97.809% (19906/20352)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.798% (20029/20480)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.782% (20151/20608)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.777% (20275/20736)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.766% (20398/20864)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.766% (20523/20992)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (20648/21120)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (20772/21248)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.769% (20899/21376)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (21021/21504)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (21147/21632)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (21272/21760)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (21398/21888)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (21523/22016)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (21649/22144)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (21773/22272)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.759% (21898/22400)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (22021/22528)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (22146/22656)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.753% (22272/22784)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (22399/22912)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (22525/23040)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.751% (22647/23168)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (22773/23296)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.767% (22901/23424)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.767% (23026/23552)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.774% (23153/23680)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.778% (23279/23808)\n",
            "Train Epoch: 147 | Loss: 0.067 | Acc: 97.786% (23406/23936)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.785% (23531/24064)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.768% (23652/24192)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (23774/24320)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (23899/24448)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (24022/24576)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (24148/24704)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (24275/24832)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.748% (24398/24960)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.744% (24522/25088)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.740% (24646/25216)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.739% (24771/25344)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.743% (24897/25472)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.742% (25022/25600)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.742% (25147/25728)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (25273/25856)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (25398/25984)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.744% (25523/26112)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.736% (25646/26240)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.740% (25772/26368)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (25898/26496)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (26023/26624)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.750% (26150/26752)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.753% (26276/26880)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.745% (26399/27008)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.752% (26526/27136)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.759% (26653/27264)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (26778/27392)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.751% (26901/27520)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (27027/27648)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (27154/27776)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (27279/27904)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.763% (27405/28032)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.759% (27529/28160)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (27655/28288)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (27779/28416)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.744% (27900/28544)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.740% (28024/28672)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (28150/28800)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (28275/28928)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.742% (28400/29056)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (28526/29184)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.731% (28647/29312)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.731% (28772/29440)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.741% (28900/29568)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.734% (29023/29696)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.730% (29147/29824)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.740% (29275/29952)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.736% (29399/30080)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.732% (29523/30208)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.739% (29650/30336)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.738% (29775/30464)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.728% (29897/30592)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.728% (30022/30720)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.724% (30146/30848)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.727% (30272/30976)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.727% (30397/31104)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.727% (30522/31232)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.730% (30648/31360)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.723% (30771/31488)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.726% (30897/31616)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.732% (31024/31744)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.732% (31149/31872)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.728% (31273/32000)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.737% (31401/32128)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.737% (31526/32256)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (31653/32384)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (31780/32512)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.751% (31906/32640)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (32029/32768)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.744% (32154/32896)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.741% (32278/33024)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.744% (32404/33152)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.743% (32529/33280)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (32656/33408)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.752% (32782/33536)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (32909/33664)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (33035/33792)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (33161/33920)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.768% (33288/34048)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.770% (33414/34176)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.770% (33539/34304)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.772% (33665/34432)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.778% (33792/34560)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.777% (33917/34688)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.780% (34043/34816)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.782% (34169/34944)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.787% (34296/35072)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.790% (34422/35200)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.786% (34546/35328)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.786% (34671/35456)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.777% (34793/35584)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.774% (34917/35712)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.771% (35041/35840)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.770% (35166/35968)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (35289/36096)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.772% (35417/36224)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.775% (35543/36352)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.777% (35669/36480)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.779% (35795/36608)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.787% (35923/36736)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.786% (36048/36864)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.775% (36169/36992)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (36290/37120)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.756% (36412/37248)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.753% (36536/37376)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.752% (36661/37504)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (36788/37632)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (36915/37760)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.767% (37042/37888)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.767% (37167/38016)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (37291/38144)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (37415/38272)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (37540/38400)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (37667/38528)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (37791/38656)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (37917/38784)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.772% (38045/38912)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.777% (38172/39040)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.774% (38296/39168)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.766% (38418/39296)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (38543/39424)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (38668/39552)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (38791/39680)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.759% (38916/39808)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (39039/39936)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (39164/40064)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (39292/40192)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (39416/40320)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.763% (39543/40448)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (39669/40576)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (39794/40704)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.759% (39917/40832)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (40044/40960)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.763% (40169/41088)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (40295/41216)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.768% (40421/41344)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.767% (40546/41472)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (40669/41600)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.766% (40796/41728)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.766% (40921/41856)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.766% (41046/41984)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.765% (41171/42112)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (41293/42240)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (41419/42368)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (41543/42496)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.757% (41668/42624)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (41795/42752)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.764% (41921/42880)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (42045/43008)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (42167/43136)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.753% (42292/43264)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (42418/43392)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (42543/43520)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.762% (42671/43648)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (42796/43776)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.761% (42921/43904)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.763% (43047/44032)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.760% (43171/44160)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (43295/44288)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.758% (43420/44416)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (43544/44544)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.755% (43669/44672)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.752% (43793/44800)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.743% (43914/44928)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.741% (44038/45056)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (44165/45184)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (44290/45312)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.749% (44417/45440)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (44541/45568)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (44666/45696)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (44791/45824)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (44916/45952)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.752% (45044/46080)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.754% (45170/46208)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.751% (45294/46336)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.742% (45415/46464)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.744% (45541/46592)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (45667/46720)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.748% (45793/46848)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.739% (45914/46976)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.741% (46040/47104)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.737% (46163/47232)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.739% (46289/47360)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.738% (46414/47488)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.740% (46540/47616)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.734% (46662/47744)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.736% (46788/47872)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.737% (46914/48000)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.733% (47037/48128)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.735% (47163/48256)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.735% (47288/48384)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.739% (47415/48512)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.738% (47540/48640)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.738% (47665/48768)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.740% (47791/48896)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.740% (47916/49024)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (48044/49152)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.746% (48169/49280)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.747% (48295/49408)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.747% (48420/49536)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.745% (48544/49664)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.747% (48670/49792)\n",
            "Train Epoch: 147 | Loss: 0.068 | Acc: 97.744% (48794/49920)\n",
            "Train Epoch: 147 | Loss: 0.069 | Acc: 97.740% (48870/50000)\n",
            "Test Epoch: 147 | Loss: 0.228 | Acc: 94.000% (94/100)\n",
            "Test Epoch: 147 | Loss: 0.358 | Acc: 91.500% (183/200)\n",
            "Test Epoch: 147 | Loss: 0.341 | Acc: 91.333% (274/300)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.500% (366/400)\n",
            "Test Epoch: 147 | Loss: 0.278 | Acc: 92.200% (461/500)\n",
            "Test Epoch: 147 | Loss: 0.241 | Acc: 93.167% (559/600)\n",
            "Test Epoch: 147 | Loss: 0.248 | Acc: 93.286% (653/700)\n",
            "Test Epoch: 147 | Loss: 0.262 | Acc: 93.000% (744/800)\n",
            "Test Epoch: 147 | Loss: 0.269 | Acc: 92.778% (835/900)\n",
            "Test Epoch: 147 | Loss: 0.273 | Acc: 92.600% (926/1000)\n",
            "Test Epoch: 147 | Loss: 0.277 | Acc: 92.455% (1017/1100)\n",
            "Test Epoch: 147 | Loss: 0.295 | Acc: 92.083% (1105/1200)\n",
            "Test Epoch: 147 | Loss: 0.293 | Acc: 92.000% (1196/1300)\n",
            "Test Epoch: 147 | Loss: 0.284 | Acc: 92.071% (1289/1400)\n",
            "Test Epoch: 147 | Loss: 0.277 | Acc: 92.267% (1384/1500)\n",
            "Test Epoch: 147 | Loss: 0.272 | Acc: 92.438% (1479/1600)\n",
            "Test Epoch: 147 | Loss: 0.272 | Acc: 92.471% (1572/1700)\n",
            "Test Epoch: 147 | Loss: 0.273 | Acc: 92.500% (1665/1800)\n",
            "Test Epoch: 147 | Loss: 0.280 | Acc: 92.526% (1758/1900)\n",
            "Test Epoch: 147 | Loss: 0.288 | Acc: 92.450% (1849/2000)\n",
            "Test Epoch: 147 | Loss: 0.285 | Acc: 92.429% (1941/2100)\n",
            "Test Epoch: 147 | Loss: 0.285 | Acc: 92.364% (2032/2200)\n",
            "Test Epoch: 147 | Loss: 0.296 | Acc: 92.217% (2121/2300)\n",
            "Test Epoch: 147 | Loss: 0.297 | Acc: 92.208% (2213/2400)\n",
            "Test Epoch: 147 | Loss: 0.305 | Acc: 92.160% (2304/2500)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 92.077% (2394/2600)\n",
            "Test Epoch: 147 | Loss: 0.310 | Acc: 92.222% (2490/2700)\n",
            "Test Epoch: 147 | Loss: 0.311 | Acc: 92.286% (2584/2800)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 92.172% (2673/2900)\n",
            "Test Epoch: 147 | Loss: 0.311 | Acc: 92.200% (2766/3000)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.968% (2851/3100)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.938% (2942/3200)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 91.939% (3034/3300)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 91.912% (3125/3400)\n",
            "Test Epoch: 147 | Loss: 0.321 | Acc: 91.743% (3211/3500)\n",
            "Test Epoch: 147 | Loss: 0.323 | Acc: 91.806% (3305/3600)\n",
            "Test Epoch: 147 | Loss: 0.331 | Acc: 91.649% (3391/3700)\n",
            "Test Epoch: 147 | Loss: 0.334 | Acc: 91.605% (3481/3800)\n",
            "Test Epoch: 147 | Loss: 0.329 | Acc: 91.744% (3578/3900)\n",
            "Test Epoch: 147 | Loss: 0.328 | Acc: 91.750% (3670/4000)\n",
            "Test Epoch: 147 | Loss: 0.330 | Acc: 91.732% (3761/4100)\n",
            "Test Epoch: 147 | Loss: 0.330 | Acc: 91.786% (3855/4200)\n",
            "Test Epoch: 147 | Loss: 0.324 | Acc: 91.907% (3952/4300)\n",
            "Test Epoch: 147 | Loss: 0.323 | Acc: 91.909% (4044/4400)\n",
            "Test Epoch: 147 | Loss: 0.321 | Acc: 91.956% (4138/4500)\n",
            "Test Epoch: 147 | Loss: 0.321 | Acc: 91.913% (4228/4600)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 92.021% (4325/4700)\n",
            "Test Epoch: 147 | Loss: 0.320 | Acc: 91.917% (4412/4800)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 92.000% (4508/4900)\n",
            "Test Epoch: 147 | Loss: 0.321 | Acc: 91.920% (4596/5000)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.980% (4691/5100)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.981% (4783/5200)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.962% (4874/5300)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.926% (4964/5400)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.927% (5056/5500)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.946% (5149/5600)\n",
            "Test Epoch: 147 | Loss: 0.319 | Acc: 91.877% (5237/5700)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.914% (5331/5800)\n",
            "Test Epoch: 147 | Loss: 0.319 | Acc: 91.831% (5418/5900)\n",
            "Test Epoch: 147 | Loss: 0.318 | Acc: 91.883% (5513/6000)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.934% (5608/6100)\n",
            "Test Epoch: 147 | Loss: 0.318 | Acc: 91.935% (5700/6200)\n",
            "Test Epoch: 147 | Loss: 0.318 | Acc: 91.937% (5792/6300)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 92.000% (5888/6400)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.954% (5977/6500)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.970% (6070/6600)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 92.000% (6164/6700)\n",
            "Test Epoch: 147 | Loss: 0.318 | Acc: 91.912% (6250/6800)\n",
            "Test Epoch: 147 | Loss: 0.318 | Acc: 91.870% (6339/6900)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.900% (6433/7000)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.887% (6524/7100)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.917% (6618/7200)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.932% (6711/7300)\n",
            "Test Epoch: 147 | Loss: 0.312 | Acc: 91.986% (6807/7400)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 91.973% (6898/7500)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.974% (6990/7600)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.922% (7078/7700)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.936% (7171/7800)\n",
            "Test Epoch: 147 | Loss: 0.317 | Acc: 91.937% (7263/7900)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.912% (7353/8000)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.963% (7449/8100)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.939% (7539/8200)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.964% (7633/8300)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.964% (7725/8400)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.894% (7811/8500)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.884% (7902/8600)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.920% (7997/8700)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 91.920% (8089/8800)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.921% (8181/8900)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.889% (8270/9000)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.912% (8364/9100)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.913% (8456/9200)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.914% (8548/9300)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.915% (8640/9400)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.895% (8730/9500)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.896% (8822/9600)\n",
            "Test Epoch: 147 | Loss: 0.314 | Acc: 91.928% (8917/9700)\n",
            "Test Epoch: 147 | Loss: 0.313 | Acc: 91.949% (9011/9800)\n",
            "Test Epoch: 147 | Loss: 0.315 | Acc: 91.909% (9099/9900)\n",
            "Test Epoch: 147 | Loss: 0.316 | Acc: 91.900% (9190/10000)\n",
            "\n",
            "Epoch: 148\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.656% (125/128)\n",
            "Train Epoch: 148 | Loss: 0.099 | Acc: 96.094% (246/256)\n",
            "Train Epoch: 148 | Loss: 0.087 | Acc: 96.615% (371/384)\n",
            "Train Epoch: 148 | Loss: 0.089 | Acc: 96.680% (495/512)\n",
            "Train Epoch: 148 | Loss: 0.076 | Acc: 97.344% (623/640)\n",
            "Train Epoch: 148 | Loss: 0.079 | Acc: 97.135% (746/768)\n",
            "Train Epoch: 148 | Loss: 0.076 | Acc: 97.321% (872/896)\n",
            "Train Epoch: 148 | Loss: 0.079 | Acc: 97.168% (995/1024)\n",
            "Train Epoch: 148 | Loss: 0.071 | Acc: 97.483% (1123/1152)\n",
            "Train Epoch: 148 | Loss: 0.071 | Acc: 97.578% (1249/1280)\n",
            "Train Epoch: 148 | Loss: 0.071 | Acc: 97.514% (1373/1408)\n",
            "Train Epoch: 148 | Loss: 0.070 | Acc: 97.396% (1496/1536)\n",
            "Train Epoch: 148 | Loss: 0.069 | Acc: 97.416% (1621/1664)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.545% (1748/1792)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.604% (1874/1920)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.607% (1999/2048)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.702% (2126/2176)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.830% (2254/2304)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.780% (2378/2432)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.695% (2501/2560)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.731% (2627/2688)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.798% (2754/2816)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.656% (2875/2944)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.656% (3000/3072)\n",
            "Train Epoch: 148 | Loss: 0.068 | Acc: 97.531% (3121/3200)\n",
            "Train Epoch: 148 | Loss: 0.068 | Acc: 97.566% (3247/3328)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.656% (3375/3456)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.628% (3499/3584)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.683% (3626/3712)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.656% (3750/3840)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.656% (3875/3968)\n",
            "Train Epoch: 148 | Loss: 0.069 | Acc: 97.607% (3998/4096)\n",
            "Train Epoch: 148 | Loss: 0.070 | Acc: 97.562% (4121/4224)\n",
            "Train Epoch: 148 | Loss: 0.069 | Acc: 97.587% (4247/4352)\n",
            "Train Epoch: 148 | Loss: 0.068 | Acc: 97.634% (4374/4480)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.656% (4500/4608)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.677% (4626/4736)\n",
            "Train Epoch: 148 | Loss: 0.067 | Acc: 97.656% (4750/4864)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.696% (4877/4992)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.734% (5004/5120)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.732% (5129/5248)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.768% (5256/5376)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.802% (5383/5504)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.834% (5510/5632)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.865% (5637/5760)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.894% (5764/5888)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.889% (5889/6016)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.900% (6015/6144)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.927% (6142/6272)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.953% (6269/6400)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.932% (6393/6528)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.912% (6517/6656)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.892% (6641/6784)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.902% (6767/6912)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.869% (6890/7040)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.824% (7012/7168)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.848% (7139/7296)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.818% (7262/7424)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.789% (7385/7552)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.786% (7510/7680)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.784% (7635/7808)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.782% (7760/7936)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.805% (7887/8064)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.803% (8012/8192)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.825% (8139/8320)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.846% (8266/8448)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.854% (8392/8576)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.875% (8519/8704)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.894% (8646/8832)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.913% (8773/8960)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.909% (8898/9088)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.938% (9026/9216)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.945% (9152/9344)\n",
            "Train Epoch: 148 | Loss: 0.060 | Acc: 97.962% (9279/9472)\n",
            "Train Epoch: 148 | Loss: 0.060 | Acc: 97.938% (9402/9600)\n",
            "Train Epoch: 148 | Loss: 0.060 | Acc: 97.944% (9528/9728)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.930% (9652/9856)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.907% (9775/9984)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.903% (9900/10112)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.910% (10026/10240)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.888% (10149/10368)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.894% (10275/10496)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.882% (10399/10624)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.889% (10525/10752)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.877% (10649/10880)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.902% (10777/11008)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.908% (10903/11136)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.931% (11031/11264)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.928% (11156/11392)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.925% (11281/11520)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.905% (11404/11648)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.886% (11527/11776)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.883% (11652/11904)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.889% (11778/12032)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.895% (11904/12160)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.909% (12031/12288)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.914% (12157/12416)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.911% (12282/12544)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.917% (12408/12672)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.922% (12534/12800)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.919% (12659/12928)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.917% (12784/13056)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.929% (12911/13184)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.912% (13034/13312)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.909% (13159/13440)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.922% (13286/13568)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.904% (13409/13696)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.902% (13534/13824)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.900% (13659/13952)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.898% (13784/14080)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.917% (13912/14208)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.928% (14039/14336)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.947% (14167/14464)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.944% (14292/14592)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.955% (14419/14720)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.946% (14543/14848)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.943% (14668/14976)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.961% (14796/15104)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.945% (14919/15232)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.930% (15042/15360)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.927% (15167/15488)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.919% (15291/15616)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.923% (15417/15744)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.915% (15541/15872)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.919% (15667/16000)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.917% (15792/16128)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.921% (15918/16256)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.913% (16042/16384)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.923% (16169/16512)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.939% (16297/16640)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.937% (16422/16768)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.929% (16546/16896)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.932% (16672/17024)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.948% (16800/17152)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.951% (16926/17280)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.938% (17049/17408)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.924% (17172/17536)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.922% (17297/17664)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.932% (17424/17792)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.935% (17550/17920)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.928% (17674/18048)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.926% (17799/18176)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.913% (17922/18304)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.917% (18048/18432)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.909% (18172/18560)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.913% (18298/18688)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.901% (18421/18816)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.899% (18546/18944)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.908% (18673/19072)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.896% (18796/19200)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.905% (18923/19328)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.913% (19050/19456)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.922% (19177/19584)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.915% (19301/19712)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.903% (19424/19840)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.907% (19550/19968)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.905% (19675/20096)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.913% (19802/20224)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.912% (19927/20352)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.920% (20054/20480)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.913% (20178/20608)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.907% (20302/20736)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.905% (20427/20864)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.904% (20552/20992)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.902% (20677/21120)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.901% (20802/21248)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.909% (20929/21376)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.912% (21055/21504)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.911% (21180/21632)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.918% (21307/21760)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.908% (21430/21888)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.915% (21557/22016)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.927% (21685/22144)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.930% (21811/22272)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.933% (21937/22400)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.940% (22064/22528)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.948% (22191/22656)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.955% (22318/22784)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.940% (22440/22912)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.947% (22567/23040)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.932% (22689/23168)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.922% (22812/23296)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.929% (22939/23424)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.932% (23065/23552)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.935% (23191/23680)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.933% (23316/23808)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.936% (23442/23936)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.943% (23569/24064)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.941% (23694/24192)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.928% (23816/24320)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.934% (23943/24448)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.937% (24069/24576)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.940% (24195/24704)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.938% (24320/24832)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.941% (24446/24960)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.931% (24569/25088)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.942% (24697/25216)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.936% (24821/25344)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.939% (24947/25472)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.949% (25075/25600)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.956% (25202/25728)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.942% (25324/25856)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.937% (25448/25984)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.932% (25572/26112)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.934% (25698/26240)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.941% (25825/26368)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.939% (25950/26496)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.945% (26077/26624)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.952% (26204/26752)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.954% (26330/26880)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.960% (26457/27008)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.969% (26585/27136)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.975% (26712/27264)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.978% (26838/27392)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.969% (26961/27520)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.975% (27088/27648)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.977% (27214/27776)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.979% (27340/27904)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.977% (27465/28032)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.979% (27591/28160)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.964% (27712/28288)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.966% (27838/28416)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.961% (27962/28544)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.963% (28088/28672)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.965% (28214/28800)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.964% (28339/28928)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.969% (28466/29056)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.968% (28591/29184)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.970% (28717/29312)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.979% (28845/29440)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.988% (28973/29568)\n",
            "Train Epoch: 148 | Loss: 0.061 | Acc: 97.980% (29096/29696)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.965% (29217/29824)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.963% (29342/29952)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.969% (29469/30080)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.961% (29592/30208)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.956% (29716/30336)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.962% (29843/30464)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.957% (29967/30592)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.956% (30092/30720)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.958% (30218/30848)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.947% (30340/30976)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.946% (30465/31104)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.951% (30592/31232)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.950% (30717/31360)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.948% (30842/31488)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.950% (30968/31616)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.956% (31095/31744)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.948% (31218/31872)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.953% (31345/32000)\n",
            "Train Epoch: 148 | Loss: 0.062 | Acc: 97.946% (31468/32128)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.932% (31589/32256)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.934% (31715/32384)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.927% (31838/32512)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.923% (31962/32640)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.906% (32082/32768)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.909% (32208/32896)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.905% (32332/33024)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.910% (32459/33152)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.903% (32582/33280)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.911% (32710/33408)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.904% (32833/33536)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.891% (32954/33664)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.890% (33079/33792)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.889% (33204/33920)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.897% (33332/34048)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.890% (33455/34176)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.895% (33582/34304)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.900% (33709/34432)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.896% (33833/34560)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.896% (33958/34688)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.892% (34082/34816)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.891% (34207/34944)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.887% (34331/35072)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.872% (34451/35200)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.880% (34579/35328)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.882% (34705/35456)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.887% (34832/35584)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.889% (34958/35712)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.896% (35086/35840)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.895% (35211/35968)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.897% (35337/36096)\n",
            "Train Epoch: 148 | Loss: 0.063 | Acc: 97.902% (35464/36224)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.896% (35587/36352)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.897% (35713/36480)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.886% (35834/36608)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.882% (35958/36736)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.879% (36082/36864)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.878% (36207/36992)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.874% (36331/37120)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.871% (36455/37248)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.868% (36579/37376)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.872% (36706/37504)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.871% (36831/37632)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.865% (36954/37760)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.867% (37080/37888)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.867% (37205/38016)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.866% (37330/38144)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.860% (37453/38272)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.865% (37580/38400)\n",
            "Train Epoch: 148 | Loss: 0.064 | Acc: 97.866% (37706/38528)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.863% (37830/38656)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.860% (37954/38784)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.857% (38078/38912)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.846% (38199/39040)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.840% (38322/39168)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.839% (38447/39296)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.836% (38571/39424)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.843% (38699/39552)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.850% (38827/39680)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.847% (38951/39808)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.844% (39075/39936)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.841% (39199/40064)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.840% (39324/40192)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.840% (39449/40320)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.842% (39575/40448)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.836% (39698/40576)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.836% (39823/40704)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.833% (39947/40832)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.834% (40073/40960)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.836% (40199/41088)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.838% (40325/41216)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.835% (40449/41344)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.832% (40573/41472)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.837% (40700/41600)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.831% (40823/41728)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.833% (40949/41856)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.830% (41073/41984)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.825% (41196/42112)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.824% (41321/42240)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.824% (41446/42368)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.828% (41573/42496)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.828% (41698/42624)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.822% (41821/42752)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.822% (41946/42880)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.821% (42071/43008)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.816% (42194/43136)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.820% (42321/43264)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.818% (42445/43392)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.822% (42572/43520)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.819% (42696/43648)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.823% (42823/43776)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.820% (42947/43904)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.817% (43071/44032)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.819% (43197/44160)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.814% (43320/44288)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.814% (43445/44416)\n",
            "Train Epoch: 148 | Loss: 0.065 | Acc: 97.813% (43570/44544)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.813% (43695/44672)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.810% (43819/44800)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.808% (43943/44928)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.807% (44068/45056)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.802% (44191/45184)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.800% (44315/45312)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.801% (44441/45440)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.801% (44566/45568)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.798% (44690/45696)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.796% (44814/45824)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.796% (44939/45952)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.799% (45066/46080)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.797% (45190/46208)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.799% (45316/46336)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.792% (45438/46464)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.787% (45561/46592)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.791% (45688/46720)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.793% (45814/46848)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.797% (45941/46976)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.796% (46066/47104)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.798% (46192/47232)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.802% (46319/47360)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.804% (46445/47488)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.805% (46571/47616)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.805% (46696/47744)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.809% (46823/47872)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.808% (46948/48000)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.810% (47074/48128)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.812% (47200/48256)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.809% (47324/48384)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.805% (47447/48512)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.806% (47573/48640)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.808% (47699/48768)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.812% (47826/48896)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.807% (47949/49024)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.807% (48074/49152)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.808% (48200/49280)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.804% (48323/49408)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.806% (48449/49536)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.805% (48574/49664)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.805% (48699/49792)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.800% (48822/49920)\n",
            "Train Epoch: 148 | Loss: 0.066 | Acc: 97.800% (48900/50000)\n",
            "Test Epoch: 148 | Loss: 0.228 | Acc: 94.000% (94/100)\n",
            "Test Epoch: 148 | Loss: 0.323 | Acc: 92.500% (185/200)\n",
            "Test Epoch: 148 | Loss: 0.341 | Acc: 92.000% (276/300)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.500% (370/400)\n",
            "Test Epoch: 148 | Loss: 0.276 | Acc: 92.800% (464/500)\n",
            "Test Epoch: 148 | Loss: 0.252 | Acc: 93.167% (559/600)\n",
            "Test Epoch: 148 | Loss: 0.271 | Acc: 93.000% (651/700)\n",
            "Test Epoch: 148 | Loss: 0.291 | Acc: 92.750% (742/800)\n",
            "Test Epoch: 148 | Loss: 0.294 | Acc: 92.444% (832/900)\n",
            "Test Epoch: 148 | Loss: 0.294 | Acc: 92.500% (925/1000)\n",
            "Test Epoch: 148 | Loss: 0.296 | Acc: 92.545% (1018/1100)\n",
            "Test Epoch: 148 | Loss: 0.305 | Acc: 92.333% (1108/1200)\n",
            "Test Epoch: 148 | Loss: 0.298 | Acc: 92.462% (1202/1300)\n",
            "Test Epoch: 148 | Loss: 0.286 | Acc: 92.643% (1297/1400)\n",
            "Test Epoch: 148 | Loss: 0.278 | Acc: 92.800% (1392/1500)\n",
            "Test Epoch: 148 | Loss: 0.273 | Acc: 92.812% (1485/1600)\n",
            "Test Epoch: 148 | Loss: 0.273 | Acc: 92.941% (1580/1700)\n",
            "Test Epoch: 148 | Loss: 0.269 | Acc: 92.944% (1673/1800)\n",
            "Test Epoch: 148 | Loss: 0.273 | Acc: 92.842% (1764/1900)\n",
            "Test Epoch: 148 | Loss: 0.283 | Acc: 92.800% (1856/2000)\n",
            "Test Epoch: 148 | Loss: 0.280 | Acc: 92.714% (1947/2100)\n",
            "Test Epoch: 148 | Loss: 0.279 | Acc: 92.682% (2039/2200)\n",
            "Test Epoch: 148 | Loss: 0.291 | Acc: 92.435% (2126/2300)\n",
            "Test Epoch: 148 | Loss: 0.290 | Acc: 92.500% (2220/2400)\n",
            "Test Epoch: 148 | Loss: 0.297 | Acc: 92.520% (2313/2500)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.346% (2401/2600)\n",
            "Test Epoch: 148 | Loss: 0.306 | Acc: 92.481% (2497/2700)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.393% (2587/2800)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.276% (2676/2900)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.233% (2767/3000)\n",
            "Test Epoch: 148 | Loss: 0.319 | Acc: 92.065% (2854/3100)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 92.062% (2946/3200)\n",
            "Test Epoch: 148 | Loss: 0.316 | Acc: 92.061% (3038/3300)\n",
            "Test Epoch: 148 | Loss: 0.316 | Acc: 92.029% (3129/3400)\n",
            "Test Epoch: 148 | Loss: 0.321 | Acc: 91.943% (3218/3500)\n",
            "Test Epoch: 148 | Loss: 0.321 | Acc: 91.972% (3311/3600)\n",
            "Test Epoch: 148 | Loss: 0.329 | Acc: 91.811% (3397/3700)\n",
            "Test Epoch: 148 | Loss: 0.328 | Acc: 91.763% (3487/3800)\n",
            "Test Epoch: 148 | Loss: 0.324 | Acc: 91.872% (3583/3900)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 91.950% (3678/4000)\n",
            "Test Epoch: 148 | Loss: 0.323 | Acc: 91.951% (3770/4100)\n",
            "Test Epoch: 148 | Loss: 0.326 | Acc: 91.952% (3862/4200)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 92.047% (3958/4300)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 92.045% (4050/4400)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 92.022% (4141/4500)\n",
            "Test Epoch: 148 | Loss: 0.321 | Acc: 92.000% (4232/4600)\n",
            "Test Epoch: 148 | Loss: 0.318 | Acc: 92.043% (4326/4700)\n",
            "Test Epoch: 148 | Loss: 0.321 | Acc: 91.979% (4415/4800)\n",
            "Test Epoch: 148 | Loss: 0.317 | Acc: 92.061% (4511/4900)\n",
            "Test Epoch: 148 | Loss: 0.323 | Acc: 91.960% (4598/5000)\n",
            "Test Epoch: 148 | Loss: 0.320 | Acc: 92.039% (4694/5100)\n",
            "Test Epoch: 148 | Loss: 0.319 | Acc: 92.038% (4786/5200)\n",
            "Test Epoch: 148 | Loss: 0.318 | Acc: 92.000% (4876/5300)\n",
            "Test Epoch: 148 | Loss: 0.317 | Acc: 92.019% (4969/5400)\n",
            "Test Epoch: 148 | Loss: 0.316 | Acc: 92.018% (5061/5500)\n",
            "Test Epoch: 148 | Loss: 0.315 | Acc: 92.054% (5155/5600)\n",
            "Test Epoch: 148 | Loss: 0.317 | Acc: 92.035% (5246/5700)\n",
            "Test Epoch: 148 | Loss: 0.315 | Acc: 92.017% (5337/5800)\n",
            "Test Epoch: 148 | Loss: 0.317 | Acc: 91.966% (5426/5900)\n",
            "Test Epoch: 148 | Loss: 0.315 | Acc: 92.000% (5520/6000)\n",
            "Test Epoch: 148 | Loss: 0.315 | Acc: 92.000% (5612/6100)\n",
            "Test Epoch: 148 | Loss: 0.316 | Acc: 91.968% (5702/6200)\n",
            "Test Epoch: 148 | Loss: 0.315 | Acc: 92.016% (5797/6300)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.078% (5893/6400)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.031% (5982/6500)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.061% (6076/6600)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.090% (6170/6700)\n",
            "Test Epoch: 148 | Loss: 0.314 | Acc: 92.044% (6259/6800)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.029% (6350/6900)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.000% (6440/7000)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 91.972% (6530/7100)\n",
            "Test Epoch: 148 | Loss: 0.314 | Acc: 91.972% (6622/7200)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.000% (6716/7300)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.054% (6812/7400)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.053% (6904/7500)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.026% (6994/7600)\n",
            "Test Epoch: 148 | Loss: 0.314 | Acc: 92.013% (7085/7700)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.077% (7182/7800)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.051% (7272/7900)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.025% (7362/8000)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.086% (7459/8100)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.061% (7549/8200)\n",
            "Test Epoch: 148 | Loss: 0.310 | Acc: 92.084% (7643/8300)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.083% (7735/8400)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.035% (7823/8500)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 91.977% (7910/8600)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.023% (8006/8700)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.045% (8100/8800)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.034% (8191/8900)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.011% (8281/9000)\n",
            "Test Epoch: 148 | Loss: 0.314 | Acc: 92.033% (8375/9100)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.043% (8468/9200)\n",
            "Test Epoch: 148 | Loss: 0.313 | Acc: 92.022% (8558/9300)\n",
            "Test Epoch: 148 | Loss: 0.312 | Acc: 92.043% (8652/9400)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.053% (8745/9500)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.052% (8837/9600)\n",
            "Test Epoch: 148 | Loss: 0.309 | Acc: 92.082% (8932/9700)\n",
            "Test Epoch: 148 | Loss: 0.307 | Acc: 92.122% (9028/9800)\n",
            "Test Epoch: 148 | Loss: 0.309 | Acc: 92.111% (9119/9900)\n",
            "Test Epoch: 148 | Loss: 0.311 | Acc: 92.110% (9211/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 149\n",
            "Train Epoch: 149 | Loss: 0.131 | Acc: 96.094% (123/128)\n",
            "Train Epoch: 149 | Loss: 0.098 | Acc: 96.875% (248/256)\n",
            "Train Epoch: 149 | Loss: 0.079 | Acc: 97.656% (375/384)\n",
            "Train Epoch: 149 | Loss: 0.077 | Acc: 97.852% (501/512)\n",
            "Train Epoch: 149 | Loss: 0.076 | Acc: 97.812% (626/640)\n",
            "Train Epoch: 149 | Loss: 0.074 | Acc: 97.786% (751/768)\n",
            "Train Epoch: 149 | Loss: 0.082 | Acc: 97.321% (872/896)\n",
            "Train Epoch: 149 | Loss: 0.081 | Acc: 97.363% (997/1024)\n",
            "Train Epoch: 149 | Loss: 0.076 | Acc: 97.569% (1124/1152)\n",
            "Train Epoch: 149 | Loss: 0.072 | Acc: 97.656% (1250/1280)\n",
            "Train Epoch: 149 | Loss: 0.068 | Acc: 97.798% (1377/1408)\n",
            "Train Epoch: 149 | Loss: 0.065 | Acc: 97.982% (1505/1536)\n",
            "Train Epoch: 149 | Loss: 0.065 | Acc: 98.017% (1631/1664)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 98.103% (1758/1792)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 98.073% (1883/1920)\n",
            "Train Epoch: 149 | Loss: 0.066 | Acc: 97.949% (2006/2048)\n",
            "Train Epoch: 149 | Loss: 0.065 | Acc: 97.978% (2132/2176)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.917% (2256/2304)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.944% (2382/2432)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.969% (2508/2560)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.991% (2634/2688)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 98.047% (2761/2816)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 98.064% (2887/2944)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.079% (3013/3072)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.094% (3139/3200)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 98.017% (3262/3328)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 98.003% (3387/3456)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.991% (3512/3584)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.980% (3637/3712)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.969% (3762/3840)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.984% (3888/3968)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.949% (4012/4096)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (4138/4224)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.909% (4261/4352)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (4388/4480)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (4514/4608)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.973% (4640/4736)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.006% (4767/4864)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.937% (4889/4992)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.949% (5015/5120)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.923% (5139/5248)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.935% (5265/5376)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.892% (5388/5504)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 97.905% (5514/5632)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 97.882% (5638/5760)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.843% (5761/5888)\n",
            "Train Epoch: 149 | Loss: 0.065 | Acc: 97.806% (5884/6016)\n",
            "Train Epoch: 149 | Loss: 0.065 | Acc: 97.819% (6010/6144)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.848% (6137/6272)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.859% (6263/6400)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.855% (6388/6528)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.837% (6512/6656)\n",
            "Train Epoch: 149 | Loss: 0.064 | Acc: 97.848% (6638/6784)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 97.888% (6766/6912)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 97.884% (6891/7040)\n",
            "Train Epoch: 149 | Loss: 0.063 | Acc: 97.893% (7017/7168)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.917% (7144/7296)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.939% (7271/7424)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.934% (7396/7552)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.904% (7519/7680)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.874% (7642/7808)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.883% (7768/7936)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.879% (7893/8064)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.913% (8021/8192)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.921% (8147/8320)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.952% (8275/8448)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.971% (8402/8576)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.978% (8528/8704)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.985% (8654/8832)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.980% (8779/8960)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (8903/9088)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.971% (9029/9216)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.988% (9156/9344)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.015% (9284/9472)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.021% (9410/9600)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.026% (9536/9728)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.032% (9662/9856)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.037% (9788/9984)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.052% (9915/10112)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.066% (10042/10240)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.071% (10168/10368)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.075% (10294/10496)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.080% (10420/10624)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.075% (10545/10752)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.033% (10666/10880)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.029% (10791/11008)\n",
            "Train Epoch: 149 | Loss: 0.059 | Acc: 98.042% (10918/11136)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.029% (11042/11264)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.999% (11164/11392)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.986% (11288/11520)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.982% (11413/11648)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.979% (11538/11776)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (11663/11904)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (11787/12032)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.969% (11913/12160)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.974% (12039/12288)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.962% (12163/12416)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.959% (12288/12544)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (12415/12672)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.945% (12537/12800)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.958% (12664/12928)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.963% (12790/13056)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.952% (12914/13184)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.972% (13042/13312)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.954% (13165/13440)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.973% (13293/13568)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.985% (13420/13696)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.982% (13545/13824)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.986% (13671/13952)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.976% (13795/14080)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.973% (13920/14208)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (14045/14336)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.954% (14168/14464)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.965% (14295/14592)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.969% (14421/14720)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.966% (14546/14848)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (14670/14976)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (14796/15104)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.978% (14924/15232)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.995% (15052/15360)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.986% (15176/15488)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (15299/15616)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.967% (15424/15744)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.971% (15550/15872)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (15676/16000)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (15801/16128)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.952% (15923/16256)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.943% (16047/16384)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.953% (16174/16512)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (16300/16640)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (16426/16768)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (16553/16896)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.938% (16673/17024)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.948% (16800/17152)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.963% (16928/17280)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (17055/17408)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.987% (17183/17536)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.985% (17308/17664)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.988% (17434/17792)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.980% (17558/17920)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.983% (17684/18048)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (17808/18176)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.979% (17934/18304)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.976% (18059/18432)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.974% (18184/18560)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.956% (18306/18688)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.954% (18431/18816)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.936% (18553/18944)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.929% (18677/19072)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.938% (18804/19200)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.941% (18930/19328)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.939% (19055/19456)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.927% (19178/19584)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.935% (19305/19712)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.944% (19432/19840)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.942% (19557/19968)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.950% (19684/20096)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.953% (19810/20224)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (19934/20352)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.944% (20059/20480)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.952% (20186/20608)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.950% (20311/20736)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.944% (20435/20864)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (20561/20992)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.945% (20686/21120)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (20814/21248)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (20940/21376)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.963% (21066/21504)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.938% (21186/21632)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.941% (21312/21760)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.940% (21437/21888)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.942% (21563/22016)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.945% (21689/22144)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.953% (21816/22272)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.955% (21942/22400)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.954% (22067/22528)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.956% (22193/22656)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.963% (22320/22784)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (22447/22912)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.969% (22572/23040)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.971% (22698/23168)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.974% (22824/23296)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.985% (22952/23424)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.979% (23076/23552)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.981% (23202/23680)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (23326/23808)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.982% (23453/23936)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.985% (23579/24064)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.983% (23704/24192)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.989% (23831/24320)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.988% (23956/24448)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.982% (24080/24576)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.992% (24208/24704)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.995% (24334/24832)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.977% (24455/24960)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.979% (24581/25088)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.989% (24709/25216)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.000% (24837/25344)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.010% (24965/25472)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.012% (25091/25600)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.014% (25217/25728)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.016% (25343/25856)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.018% (25469/25984)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.009% (25592/26112)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.003% (25716/26240)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.001% (25841/26368)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.003% (25967/26496)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.998% (26091/26624)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.004% (26218/26752)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.999% (26342/26880)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.004% (26469/27008)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.006% (26595/27136)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 98.008% (26721/27264)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.996% (26843/27392)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.994% (26968/27520)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.993% (27093/27648)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.991% (27218/27776)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.990% (27343/27904)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.995% (27470/28032)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.990% (27594/28160)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.981% (27717/28288)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.976% (27841/28416)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.986% (27969/28544)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.984% (28094/28672)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.990% (28221/28800)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.988% (28346/28928)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.990% (28472/29056)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.995% (28599/29184)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.997% (28725/29312)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.989% (28848/29440)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.988% (28973/29568)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.993% (29100/29696)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.975% (29220/29824)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.980% (29347/29952)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.975% (29471/30080)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.974% (29596/30208)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.963% (29718/30336)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.955% (29841/30464)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (29967/30592)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.956% (30092/30720)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.942% (30213/30848)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.944% (30339/30976)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.942% (30464/31104)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.948% (30591/31232)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.956% (30719/31360)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.955% (30844/31488)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (30967/31616)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (31092/31744)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.939% (31215/31872)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (31343/32000)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.936% (31465/32128)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.935% (31590/32256)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.937% (31716/32384)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.936% (31841/32512)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.929% (31964/32640)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.925% (32088/32768)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.924% (32213/32896)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.926% (32339/33024)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.925% (32464/33152)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.924% (32589/33280)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.914% (32711/33408)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.919% (32838/33536)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.918% (32963/33664)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.926% (33091/33792)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.922% (33215/33920)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.921% (33340/34048)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.923% (33466/34176)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.930% (33594/34304)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.938% (33722/34432)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (33850/34560)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (33976/34688)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.949% (34102/34816)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.948% (34227/34944)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (34352/35072)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (34477/35200)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.953% (34605/35328)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.952% (34730/35456)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (34857/35584)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.953% (34981/35712)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (35104/35840)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.954% (35232/35968)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.955% (35358/36096)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.957% (35484/36224)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.956% (35609/36352)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.958% (35735/36480)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.954% (35859/36608)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.958% (35986/36736)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.963% (36113/36864)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.962% (36238/36992)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.963% (36364/37120)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.962% (36489/37248)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.961% (36614/37376)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.958% (36738/37504)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.959% (36864/37632)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.958% (36989/37760)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.962% (37116/37888)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.959% (37240/38016)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.963% (37367/38144)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.959% (37491/38272)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.951% (37613/38400)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.950% (37738/38528)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.951% (37864/38656)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.955% (37991/38784)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (38117/38912)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (38244/39040)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (38369/39168)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.967% (38497/39296)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (38620/39424)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (38745/39552)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (38872/39680)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.968% (38999/39808)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (39123/39936)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (39247/40064)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.955% (39370/40192)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.959% (39497/40320)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.963% (39624/40448)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (39750/40576)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.971% (39878/40704)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.967% (40002/40832)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.971% (40129/40960)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (40254/41088)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (40380/41216)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.973% (40506/41344)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (40631/41472)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.974% (40757/41600)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (40881/41728)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.972% (41007/41856)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.975% (41134/41984)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.970% (41257/42112)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.976% (41385/42240)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.977% (41511/42368)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.976% (41636/42496)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (41761/42624)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.974% (41886/42752)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.976% (42012/42880)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (42137/43008)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.976% (42263/43136)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.975% (42388/43264)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.979% (42515/43392)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.980% (42641/43520)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.982% (42767/43648)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.983% (42893/43776)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.982% (43018/43904)\n",
            "Train Epoch: 149 | Loss: 0.060 | Acc: 97.976% (43141/44032)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (43261/44160)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (43385/44288)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.962% (43511/44416)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (43637/44544)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.967% (43764/44672)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.964% (43888/44800)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (44012/44928)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.960% (44137/45056)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.962% (44263/45184)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.961% (44388/45312)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.962% (44514/45440)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.959% (44638/45568)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.958% (44763/45696)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.957% (44888/45824)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.950% (45010/45952)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.951% (45136/46080)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.946% (45259/46208)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.941% (45382/46336)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (45510/46464)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.950% (45637/46592)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.945% (45760/46720)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.947% (45886/46848)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.948% (46012/46976)\n",
            "Train Epoch: 149 | Loss: 0.061 | Acc: 97.951% (46139/47104)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.942% (46260/47232)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.941% (46385/47360)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.941% (46510/47488)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.940% (46635/47616)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.941% (46761/47744)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.936% (46884/47872)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.940% (47011/48000)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.943% (47138/48128)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.940% (47262/48256)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.935% (47385/48384)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.932% (47509/48512)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.938% (47637/48640)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.941% (47764/48768)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.938% (47888/48896)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.936% (48012/49024)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.931% (48135/49152)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.930% (48260/49280)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.929% (48385/49408)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.931% (48511/49536)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.928% (48635/49664)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.925% (48759/49792)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.923% (48883/49920)\n",
            "Train Epoch: 149 | Loss: 0.062 | Acc: 97.922% (48961/50000)\n",
            "Test Epoch: 149 | Loss: 0.222 | Acc: 93.000% (93/100)\n",
            "Test Epoch: 149 | Loss: 0.279 | Acc: 93.500% (187/200)\n",
            "Test Epoch: 149 | Loss: 0.303 | Acc: 91.667% (275/300)\n",
            "Test Epoch: 149 | Loss: 0.292 | Acc: 91.750% (367/400)\n",
            "Test Epoch: 149 | Loss: 0.256 | Acc: 92.400% (462/500)\n",
            "Test Epoch: 149 | Loss: 0.229 | Acc: 93.167% (559/600)\n",
            "Test Epoch: 149 | Loss: 0.253 | Acc: 93.000% (651/700)\n",
            "Test Epoch: 149 | Loss: 0.275 | Acc: 92.625% (741/800)\n",
            "Test Epoch: 149 | Loss: 0.284 | Acc: 92.333% (831/900)\n",
            "Test Epoch: 149 | Loss: 0.276 | Acc: 92.600% (926/1000)\n",
            "Test Epoch: 149 | Loss: 0.287 | Acc: 92.273% (1015/1100)\n",
            "Test Epoch: 149 | Loss: 0.294 | Acc: 92.167% (1106/1200)\n",
            "Test Epoch: 149 | Loss: 0.289 | Acc: 92.077% (1197/1300)\n",
            "Test Epoch: 149 | Loss: 0.280 | Acc: 92.143% (1290/1400)\n",
            "Test Epoch: 149 | Loss: 0.275 | Acc: 92.133% (1382/1500)\n",
            "Test Epoch: 149 | Loss: 0.266 | Acc: 92.500% (1480/1600)\n",
            "Test Epoch: 149 | Loss: 0.269 | Acc: 92.588% (1574/1700)\n",
            "Test Epoch: 149 | Loss: 0.273 | Acc: 92.611% (1667/1800)\n",
            "Test Epoch: 149 | Loss: 0.280 | Acc: 92.474% (1757/1900)\n",
            "Test Epoch: 149 | Loss: 0.283 | Acc: 92.500% (1850/2000)\n",
            "Test Epoch: 149 | Loss: 0.286 | Acc: 92.381% (1940/2100)\n",
            "Test Epoch: 149 | Loss: 0.287 | Acc: 92.136% (2027/2200)\n",
            "Test Epoch: 149 | Loss: 0.302 | Acc: 92.043% (2117/2300)\n",
            "Test Epoch: 149 | Loss: 0.299 | Acc: 92.083% (2210/2400)\n",
            "Test Epoch: 149 | Loss: 0.313 | Acc: 91.880% (2297/2500)\n",
            "Test Epoch: 149 | Loss: 0.328 | Acc: 91.731% (2385/2600)\n",
            "Test Epoch: 149 | Loss: 0.321 | Acc: 91.815% (2479/2700)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.857% (2572/2800)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.759% (2661/2900)\n",
            "Test Epoch: 149 | Loss: 0.324 | Acc: 91.800% (2754/3000)\n",
            "Test Epoch: 149 | Loss: 0.326 | Acc: 91.710% (2843/3100)\n",
            "Test Epoch: 149 | Loss: 0.326 | Acc: 91.625% (2932/3200)\n",
            "Test Epoch: 149 | Loss: 0.325 | Acc: 91.636% (3024/3300)\n",
            "Test Epoch: 149 | Loss: 0.322 | Acc: 91.676% (3117/3400)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.629% (3207/3500)\n",
            "Test Epoch: 149 | Loss: 0.324 | Acc: 91.694% (3301/3600)\n",
            "Test Epoch: 149 | Loss: 0.330 | Acc: 91.595% (3389/3700)\n",
            "Test Epoch: 149 | Loss: 0.329 | Acc: 91.632% (3482/3800)\n",
            "Test Epoch: 149 | Loss: 0.324 | Acc: 91.744% (3578/3900)\n",
            "Test Epoch: 149 | Loss: 0.322 | Acc: 91.750% (3670/4000)\n",
            "Test Epoch: 149 | Loss: 0.325 | Acc: 91.732% (3761/4100)\n",
            "Test Epoch: 149 | Loss: 0.329 | Acc: 91.690% (3851/4200)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.837% (3949/4300)\n",
            "Test Epoch: 149 | Loss: 0.324 | Acc: 91.841% (4041/4400)\n",
            "Test Epoch: 149 | Loss: 0.322 | Acc: 91.844% (4133/4500)\n",
            "Test Epoch: 149 | Loss: 0.324 | Acc: 91.761% (4221/4600)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.872% (4318/4700)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.812% (4407/4800)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.898% (4503/4900)\n",
            "Test Epoch: 149 | Loss: 0.325 | Acc: 91.840% (4592/5000)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.902% (4687/5100)\n",
            "Test Epoch: 149 | Loss: 0.323 | Acc: 91.923% (4780/5200)\n",
            "Test Epoch: 149 | Loss: 0.321 | Acc: 91.925% (4872/5300)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.926% (4964/5400)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.927% (5056/5500)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.929% (5148/5600)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.895% (5238/5700)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.914% (5331/5800)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.881% (5421/5900)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.900% (5514/6000)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.885% (5605/6100)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.855% (5695/6200)\n",
            "Test Epoch: 149 | Loss: 0.322 | Acc: 91.857% (5787/6300)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.969% (5886/6400)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.892% (5973/6500)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.924% (6067/6600)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.940% (6160/6700)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.941% (6252/6800)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.957% (6345/6900)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.900% (6433/7000)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.944% (6528/7100)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.944% (6620/7200)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.986% (6715/7300)\n",
            "Test Epoch: 149 | Loss: 0.315 | Acc: 92.014% (6809/7400)\n",
            "Test Epoch: 149 | Loss: 0.316 | Acc: 92.013% (6901/7500)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.987% (6991/7600)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.974% (7082/7700)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 92.026% (7178/7800)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.975% (7266/7900)\n",
            "Test Epoch: 149 | Loss: 0.319 | Acc: 91.963% (7357/8000)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 92.000% (7452/8100)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 92.000% (7544/8200)\n",
            "Test Epoch: 149 | Loss: 0.316 | Acc: 92.012% (7637/8300)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 92.000% (7728/8400)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.965% (7817/8500)\n",
            "Test Epoch: 149 | Loss: 0.320 | Acc: 91.919% (7905/8600)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.943% (7999/8700)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.977% (8094/8800)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.989% (8187/8900)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.989% (8279/9000)\n",
            "Test Epoch: 149 | Loss: 0.318 | Acc: 91.967% (8369/9100)\n",
            "Test Epoch: 149 | Loss: 0.316 | Acc: 92.000% (8464/9200)\n",
            "Test Epoch: 149 | Loss: 0.316 | Acc: 92.011% (8557/9300)\n",
            "Test Epoch: 149 | Loss: 0.315 | Acc: 92.021% (8650/9400)\n",
            "Test Epoch: 149 | Loss: 0.314 | Acc: 92.000% (8740/9500)\n",
            "Test Epoch: 149 | Loss: 0.315 | Acc: 92.000% (8832/9600)\n",
            "Test Epoch: 149 | Loss: 0.313 | Acc: 92.021% (8926/9700)\n",
            "Test Epoch: 149 | Loss: 0.313 | Acc: 92.051% (9021/9800)\n",
            "Test Epoch: 149 | Loss: 0.315 | Acc: 91.980% (9106/9900)\n",
            "Test Epoch: 149 | Loss: 0.317 | Acc: 91.970% (9197/10000)\n"
          ]
        }
      ],
      "source": [
        "acc_list = []\n",
        "loss_list = []\n",
        "for epoch in range(start_epoch, start_epoch+150):\n",
        "    train(epoch)\n",
        "    loss, acc = test(epoch)\n",
        "    acc_list.append(acc)\n",
        "    loss_list.append(loss)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS-WMnGMEXh8",
        "outputId": "c7128441-9523-43cb-c5ba-e7e90ac4bc5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(acc_list)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"test_accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "WqrWPRm4nBpF",
        "outputId": "7a8e4943-3bef-4727-bd25-7b5d54e41299"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"epoch\") \n",
        "plt.ylabel(\"test_loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "cWgbPYfEnTvK",
        "outputId": "3a03a61a-8264-4f28-b7b2-f321883e0dc4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj_FyeZtx4p6",
        "outputId": "70784de9-e55f-4dd1-8196-4c0766378baf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (linear): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "net"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a575cbbaf1834c81af53830b0f9d4428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20c88bf5087e4b1a84f8cab8500fd690",
              "IPY_MODEL_6cdae7bf52ff4451ab0deee20f688c5a",
              "IPY_MODEL_f2370242142d4340a08a94f0047f4723"
            ],
            "layout": "IPY_MODEL_3f8a5996c5854f668fbafc516fa07a12"
          }
        },
        "20c88bf5087e4b1a84f8cab8500fd690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f77e07c63624bee9c8e2c11757d553d",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f1385595ce40409097a8f2d31b53c0",
            "value": "100%"
          }
        },
        "6cdae7bf52ff4451ab0deee20f688c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2627013bbea41e0b9cb74e9e7137788",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97ae87a2a80f4857835d85c228272199",
            "value": 170498071
          }
        },
        "f2370242142d4340a08a94f0047f4723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1989749d38d741d6abb60bc42adaa5ff",
            "placeholder": "​",
            "style": "IPY_MODEL_d1a3361f94f6432298424783d7efee19",
            "value": " 170498071/170498071 [00:01&lt;00:00, 104334970.13it/s]"
          }
        },
        "3f8a5996c5854f668fbafc516fa07a12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f77e07c63624bee9c8e2c11757d553d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f1385595ce40409097a8f2d31b53c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2627013bbea41e0b9cb74e9e7137788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ae87a2a80f4857835d85c228272199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1989749d38d741d6abb60bc42adaa5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a3361f94f6432298424783d7efee19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}